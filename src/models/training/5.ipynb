{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. トレーニング設定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 5. トレーニング設定\n",
    "### 5.1 トレーニング引数設定\n",
    "# Split dataset into training and evaluation sets\n",
    "dataset_size = len(tokenized_dataset)\n",
    "indices = np.random.permutation(dataset_size)\n",
    "split_idx = int(dataset_size * 0.8)\n",
    "train_dataset = tokenized_dataset.select(indices[:split_idx])\n",
    "# Limit evaluation dataset size\n",
    "eval_dataset = tokenized_dataset.select(indices[split_idx:split_idx+50])  # Maximum 50 samples\n",
    "\n",
    "logging.info(f\"Training dataset size: {len(train_dataset)}\")\n",
    "logging.info(f\"Evaluation dataset size: {len(eval_dataset)}\")\n",
    "\n",
    "# Disable wandb via environment variable\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "# Update training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=MODEL_OUTPUT_DIR,  \n",
    "    num_train_epochs=30,\n",
    "    learning_rate=8e-5,\n",
    "    weight_decay=0.06,\n",
    "    warmup_ratio=0.25,\n",
    "    lr_scheduler_type=\"cosine_with_restarts\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=20,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=20,\n",
    "    gradient_accumulation_steps=8,\n",
    "    max_steps=-1,\n",
    "    disable_tqdm=False,\n",
    "    logging_dir=LOG_OUTPUT_DIR,   \n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    no_cuda=False,\n",
    "    dataloader_num_workers=1,\n",
    "    report_to=[],\n",
    "    run_name=None,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_checkpointing=True,\n",
    "    max_grad_norm=0.5,\n",
    "    dataloader_pin_memory=True,\n",
    "    save_total_limit=2,\n",
    "    fp16=True,\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    eval_accumulation_steps=4,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"combined_score\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "このコードセクションについて、ソクラテス式チャットボットのトレーニングを例に説明していきます。\n",
    "\n",
    "### 1. データセットの分割\n",
    "\n",
    "```python\n",
    "dataset_size = len(tokenized_dataset)\n",
    "indices = np.random.permutation(dataset_size)\n",
    "split_idx = int(dataset_size * 0.8)\n",
    "train_dataset = tokenized_dataset.select(indices[:split_idx])\n",
    "eval_dataset = tokenized_dataset.select(indices[split_idx:split_idx+50])\n",
    "```\n",
    "\n",
    "これは、データセットを「トレーニング用」と「評価用」に分けている部分です。\n",
    "\n",
    "- 全体の80%をトレーニング用に使用\n",
    "- 残りの中から最大50個のサンプルを評価用に使用\n",
    "\n",
    "例えば、1000個の対話データがあった場合：\n",
    "- トレーニング用：800個の対話\n",
    "- 評価用：50個の対話\n",
    "を使用することになります。\n",
    "\n",
    "### 2. トレーニング設定（TrainingArguments）\n",
    "\n",
    "主要な設定を説明します：\n",
    "\n",
    "#### 基本設定\n",
    "```python\n",
    "num_train_epochs=30,  # 30回繰り返してトレーニング\n",
    "learning_rate=8e-5,   # 学習率（どのくらい大きく更新するか）\n",
    "```\n",
    "\n",
    "#### 学習の進め方\n",
    "```python\n",
    "warmup_ratio=0.25,  # 最初の25%は徐々に学習率を上げていく\n",
    "lr_scheduler_type=\"cosine_with_restarts\",  # 学習率を周期的に変化させる\n",
    "```\n",
    "\n",
    "これは、ソクラテス式の対話の特徴（質問の仕方、応答の仕方）を段階的に学習させるための設定です。\n",
    "\n",
    "#### 評価と保存\n",
    "```python\n",
    "evaluation_strategy=\"steps\",\n",
    "eval_steps=20,        # 20ステップごとに評価\n",
    "save_strategy=\"steps\",\n",
    "save_steps=20,        # 20ステップごとに保存\n",
    "```\n",
    "\n",
    "例えば：\n",
    "- 20回の対話トレーニングが終わるごとに\n",
    "- モデルが「なぜそう考えるのですか？」「それはどういう意味でしょうか？」といった\n",
    "  ソクラテス式の質問ができているかを評価\n",
    "\n",
    "#### バッチ処理設定\n",
    "```python\n",
    "per_device_train_batch_size=2,  # 一度に2つの対話をトレーニング\n",
    "gradient_accumulation_steps=8,   # 8回分まとめて更新\n",
    "```\n",
    "\n",
    "メモリの制約上、一度に処理できる対話数を制限しています。\n",
    "\n",
    "#### 評価指標\n",
    "```python\n",
    "metric_for_best_model=\"combined_score\",  # 総合評価スコアで判断\n",
    "load_best_model_at_end=True,            # 最も良い結果のモデルを保存\n",
    "```\n",
    "\n",
    "この「combined_score」には以下のような要素が含まれます：\n",
    "- 質問の適切さ（「なぜ」「どのように」などの問いかけ）\n",
    "- 対話の流れの自然さ\n",
    "- ソクラテス式の特徴的な言い回しの使用\n",
    "\n",
    "これらの設定により、ソクラテスのように：\n",
    "- 適切なタイミングで質問を投げかけ\n",
    "- 相手の考えを深めるような対話を行う\n",
    "能力を効率的に学習させることを目指しています。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "### 5.2 データローダーとコレータ設定\n",
    "# Modify data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    "    pad_to_multiple_of=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "データローダーとコレータ設定について、ソクラテス式チャットボットの例を用いて説明します。\n",
    "\n",
    "### データコレータ（Data Collator）とは？\n",
    "\n",
    "データコレータは、バラバラの対話データを機械学習用の形式に整形するツールです。例えるなら、様々な長さの対話を同じサイズの「お弁当箱」に詰めるようなものです。\n",
    "\n",
    "```python\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,    # テキストを数値に変換するツール\n",
    "    mlm=False,             # マスク言語モデリングを使用しない\n",
    "    pad_to_multiple_of=8   # データの長さを8の倍数に揃える\n",
    ")\n",
    "```\n",
    "\n",
    "### 具体例で説明\n",
    "\n",
    "例えば、以下のような異なる長さの対話があるとします：\n",
    "\n",
    "1. 短い対話：\n",
    "```\n",
    "ユーザー：「幸せとは何でしょうか？」\n",
    "モデル：「幸せとは何だとお考えですか？」\n",
    "```\n",
    "\n",
    "2. 長い対話：\n",
    "```\n",
    "ユーザー：「幸せとは何でしょうか？」\n",
    "モデル：「幸せとは何だとお考えですか？」\n",
    "ユーザー：「家族と過ごす時間だと思います」\n",
    "モデル：「なぜ家族と過ごす時間が幸せだとお感じになるのでしょうか？」\n",
    "```\n",
    "\n",
    "### データコレータの役割\n",
    "\n",
    "1. **長さの統一**\n",
    "   - `pad_to_multiple_of=8`は、すべての対話データの長さを8の倍数に調整します\n",
    "   - 短い対話には特殊なパディング（埋め合わせ）トークンを追加\n",
    "   - これにより、GPUでの計算効率が上がります\n",
    "\n",
    "2. **入力形式の統一**\n",
    "   - 対話をモデルが理解できる数値形式（トークン）に変換\n",
    "   - 例：「幸せ」→ [2345, 6789]（数値の例）\n",
    "\n",
    "3. **バッチ処理の準備**\n",
    "   - 複数の対話を一度に処理できるように整形\n",
    "   - メモリ使用を最適化\n",
    "\n",
    "### mlm=Falseの意味\n",
    "\n",
    "- `mlm=False`は、このモデルが「マスク言語モデリング」を使用しないことを示します\n",
    "- ソクラテス式チャットボットでは、文章の一部を隠して予測する必要はなく、\n",
    "  対話の流れを自然に学習させたいため、これをFalseに設定しています\n",
    "\n",
    "このように、データコレータは、様々な形式の対話データを、モデルが効率的に学習できる形に整形する重要な役割を果たしています。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "### 5.3 トレーニング実行と例外処理\n",
    "# Start training\n",
    "logging.info(\"Starting training...\")\n",
    "try:\n",
    "    checkpoint_dir = MODEL_OUTPUT_DIR  \n",
    "    resume_from_checkpoint = None\n",
    "    \n",
    "    # Check if running in Kaggle environment\n",
    "    is_kaggle = os.path.exists('/kaggle/working')\n",
    "    \n",
    "    # Checkpoint status and processing\n",
    "    if os.path.exists(checkpoint_dir):\n",
    "        print(\"\\nChecking checkpoint status...\")  \n",
    "        checkpoints = [f for f in os.listdir(checkpoint_dir) if f.startswith(\"checkpoint-\")]\n",
    "        if checkpoints:\n",
    "            # Get latest checkpoint\n",
    "            latest_checkpoint = max(checkpoints, key=lambda x: int(x.split(\"-\")[1]))\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, latest_checkpoint)\n",
    "            print(f\"Found latest checkpoint: {latest_checkpoint}\") \n",
    "            \n",
    "            # Check checkpoint status\n",
    "            state_path = os.path.join(checkpoint_path, \"trainer_state.json\")\n",
    "            if os.path.exists(state_path):\n",
    "                with open(state_path, 'r') as f:\n",
    "                    state = json.load(f)\n",
    "                current_epoch = state.get('epoch', 0)\n",
    "                print(f\"\\nCurrent training status:\")  \n",
    "                print(f\"Current epoch: {current_epoch}\")  \n",
    "                print(f\"Target epochs: {training_args.num_train_epochs}\")  \n",
    "                \n",
    "                # Exit safely if completed\n",
    "                if current_epoch >= training_args.num_train_epochs - 0.1:\n",
    "                    print(\"\\n\" + \"=\"*50)\n",
    "                    print(\"IMPORTANT NOTICE:\")\n",
    "                    print(f\"Training has already been completed at epoch {current_epoch}!\")\n",
    "                    print(f\"Target epochs was {training_args.num_train_epochs}\")  \n",
    "                    print(f\"Trained model is available at: {checkpoint_dir}\")\n",
    "                    print(\"=\"*50 + \"\\n\")\n",
    "                    logging.info(\"Training has already been completed. Exiting to protect existing model.\")\n",
    "                    logging.info(f\"Trained model is available at: {checkpoint_dir}\")\n",
    "                    exit(0)\n",
    "            else:\n",
    "                logging.warning(\"Invalid checkpoint state found. Please check manually.\")\n",
    "                logging.warning(f\"Checkpoint directory: {checkpoint_dir}\")\n",
    "                if not is_kaggle:  \n",
    "                    user_input = input(\"Do you want to continue and overwrite? (yes/no): \")\n",
    "                    if user_input.lower() != 'yes':\n",
    "                        logging.info(\"Aborting to protect existing data.\")\n",
    "                        exit(0)\n",
    "        else:\n",
    "            logging.warning(\"Checkpoint directory exists but no checkpoints found.\")\n",
    "            if not is_kaggle:  \n",
    "                user_input = input(\"Do you want to continue and overwrite the directory? (yes/no): \")\n",
    "                if user_input.lower() != 'yes':\n",
    "                    logging.info(\"Aborting to protect existing data.\")\n",
    "                    exit(0)\n",
    "\n",
    "    # Start training (or resume)\n",
    "    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
    "    logging.info(\"Training completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "\n",
    "\n",
    "トレーニング実行と例外処理の部分について、ソクラテス式チャットボットの例を用いて説明します。\n",
    "\n",
    "### 1. トレーニングの開始準備\n",
    "\n",
    "```python\n",
    "logging.info(\"Starting training...\")\n",
    "try:\n",
    "    checkpoint_dir = MODEL_OUTPUT_DIR  \n",
    "    resume_from_checkpoint = None\n",
    "```\n",
    "\n",
    "これは、トレーニングを始める準備をする部分です。チェックポイント（途中経過の保存）を保存するディレクトリを設定します。\n",
    "\n",
    "### 2. チェックポイントの確認システム\n",
    "\n",
    "チェックポイントとは、トレーニングの途中経過を保存したものです。例えば：\n",
    "- 10エポック目でのソクラテス式の質問の仕方\n",
    "- 20エポック目での対話の自然さ\n",
    "などの学習状態を保存しています。\n",
    "\n",
    "```python\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    print(\"\\nChecking checkpoint status...\")  \n",
    "    checkpoints = [f for f in os.listdir(checkpoint_dir) if f.startswith(\"checkpoint-\")]\n",
    "```\n",
    "\n",
    "### 3. 最新のチェックポイントの確認\n",
    "\n",
    "```python\n",
    "if checkpoints:\n",
    "    # 最新のチェックポイントを見つける\n",
    "    latest_checkpoint = max(checkpoints, key=lambda x: int(x.split(\"-\")[1]))\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, latest_checkpoint)\n",
    "```\n",
    "\n",
    "例えば：\n",
    "- checkpoint-1000（10エポック目）\n",
    "- checkpoint-2000（20エポック目）\n",
    "- checkpoint-3000（30エポック目）\n",
    "がある場合、checkpoint-3000を最新として選びます。\n",
    "\n",
    "### 4. トレーニング状態の確認\n",
    "\n",
    "```python\n",
    "state_path = os.path.join(checkpoint_path, \"trainer_state.json\")\n",
    "if os.path.exists(state_path):\n",
    "    with open(state_path, 'r') as f:\n",
    "        state = json.load(f)\n",
    "    current_epoch = state.get('epoch', 0)\n",
    "```\n",
    "\n",
    "これは、以下のような情報を確認します：\n",
    "- 現在のエポック数（何周目の学習か）\n",
    "- 目標のエポック数（何周学習する予定か）\n",
    "\n",
    "### 5. トレーニング完了チェック\n",
    "\n",
    "```python\n",
    "if current_epoch >= training_args.num_train_epochs - 0.1:\n",
    "    print(\"IMPORTANT NOTICE:\")\n",
    "    print(f\"Training has already been completed at epoch {current_epoch}!\")\n",
    "```\n",
    "\n",
    "例えば：\n",
    "- 目標が30エポックで\n",
    "- すでに30エポック完了している場合\n",
    "- 「トレーニング済み」として処理を終了\n",
    "\n",
    "### 6. 安全確認と再開の選択\n",
    "\n",
    "```python\n",
    "if not is_kaggle:  \n",
    "    user_input = input(\"Do you want to continue and overwrite? (yes/no): \")\n",
    "    if user_input.lower() != 'yes':\n",
    "        logging.info(\"Aborting to protect existing data.\")\n",
    "        exit(0)\n",
    "```\n",
    "\n",
    "これは、既存のトレーニング結果を誤って上書きしないための安全装置です。\n",
    "例えば：\n",
    "- すでに良い感じのソクラテス式の対話ができるモデルがある場合\n",
    "- 誤って上書きしないように確認を求めます\n",
    "\n",
    "### 7. トレーニングの実行\n",
    "\n",
    "```python\n",
    "trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
    "logging.info(\"Training completed successfully!\")\n",
    "```\n",
    "\n",
    "最後に、実際のトレーニングを開始します：\n",
    "- 新規トレーニングの場合：最初から開始\n",
    "- 途中再開の場合：チェックポイントから再開\n",
    "\n",
    "このように、このコードは：\n",
    "1. トレーニングの状態を確認\n",
    "2. 既存のモデルを保護\n",
    "3. 適切な開始位置を決定\n",
    "という重要な役割を果たしています。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "### 5.4 モデル保存と設定エクスポート\n",
    "    # Save settings (as JSON)\n",
    "    def convert_to_serializable(obj):\n",
    "        if isinstance(obj, set):\n",
    "            return list(obj)\n",
    "        elif isinstance(obj, dict):\n",
    "            return {k: convert_to_serializable(v) for k, v in obj.items()}\n",
    "        elif isinstance(obj, (list, tuple)):\n",
    "            return [convert_to_serializable(x) for x in obj]\n",
    "        return obj\n",
    "\n",
    "    # Convert each setting\n",
    "    training_args_dict = convert_to_serializable(training_args.to_dict())\n",
    "    lora_config_dict = convert_to_serializable(lora_config.to_dict())\n",
    "\n",
    "    config_dict = {\n",
    "        \"model_name\": model_name,\n",
    "        \"training_args\": training_args_dict,\n",
    "        \"lora_config\": lora_config_dict,\n",
    "        \"bnb_config\": {\n",
    "            \"load_in_4bit\": bnb_config.load_in_4bit,\n",
    "            \"bnb_4bit_use_double_quant\": bnb_config.bnb_4bit_use_double_quant,\n",
    "            \"bnb_4bit_quant_type\": bnb_config.bnb_4bit_quant_type,\n",
    "            \"bnb_4bit_compute_dtype\": str(bnb_config.bnb_4bit_compute_dtype),\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save configurations\n",
    "    with open(os.path.join(training_args.output_dir, \"training_config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(config_dict, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    # Save model and settings\n",
    "    trainer.save_model()\n",
    "    model.config.save_pretrained(training_args.output_dir)\n",
    "    tokenizer.save_pretrained(training_args.output_dir)\n",
    "    logging.info(\"Model and configuration saved successfully!\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"An error occurred: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "\n",
    "\n",
    "モデル保存と設定エクスポートの部分について、ソクラテス式チャットボットの例を用いて説明します。\n",
    "\n",
    "### 1. データ変換用の関数定義\n",
    "\n",
    "```python\n",
    "def convert_to_serializable(obj):\n",
    "    if isinstance(obj, set):\n",
    "        return list(obj)\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: convert_to_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, (list, tuple)):\n",
    "        return [convert_to_serializable(x) for x in obj]\n",
    "    return obj\n",
    "```\n",
    "\n",
    "これは、モデルの設定を保存可能な形式に変換する関数です。\n",
    "\n",
    "例えば：\n",
    "- 特殊な質問パターンのセット `{'なぜ', 'どのように', 'どう考えますか'}` を\n",
    "- 保存可能な配列 `['なぜ', 'どのように', 'どう考えますか']` に変換\n",
    "\n",
    "### 2. トレーニング設定の変換\n",
    "\n",
    "```python\n",
    "training_args_dict = convert_to_serializable(training_args.to_dict())\n",
    "lora_config_dict = convert_to_serializable(lora_config.to_dict())\n",
    "```\n",
    "\n",
    "トレーニングの設定を保存用に変換します。例えば：\n",
    "- 学習率\n",
    "- エポック数\n",
    "- バッチサイズ\n",
    "などの設定値を保存可能な形式に変換します。\n",
    "\n",
    "### 3. 設定情報の整理\n",
    "\n",
    "```python\n",
    "config_dict = {\n",
    "    \"model_name\": model_name,        # 使用したベースモデルの名前\n",
    "    \"training_args\": training_args_dict,  # トレーニング設定\n",
    "    \"lora_config\": lora_config_dict,      # LoRA（効率的な学習方法）の設定\n",
    "    \"bnb_config\": {                       # メモリ効率化の設定\n",
    "        \"load_in_4bit\": bnb_config.load_in_4bit,\n",
    "        \"bnb_4bit_use_double_quant\": bnb_config.bnb_4bit_use_double_quant,\n",
    "        \"bnb_4bit_quant_type\": bnb_config.bnb_4bit_quant_type,\n",
    "        \"bnb_4bit_compute_dtype\": str(bnb_config.bnb_4bit_compute_dtype),\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "これは、モデルの全設定をまとめる部分です。例えば：\n",
    "- ベースモデル：「gemma-2b-jpn-it」\n",
    "- トレーニング設定：30エポック、学習率8e-5など\n",
    "- LoRA設定：効率的な学習のための特別な設定\n",
    "- メモリ効率化設定：限られたGPUメモリでも動作するための設定\n",
    "\n",
    "### 4. 設定の保存\n",
    "\n",
    "```python\n",
    "with open(os.path.join(training_args.output_dir, \"training_config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(config_dict, f, indent=2, ensure_ascii=False)\n",
    "```\n",
    "\n",
    "設定をJSON形式で保存します。これにより：\n",
    "- 後で設定を確認できる\n",
    "- 同じ設定で再トレーニングできる\n",
    "- トラブル時の参照が可能\n",
    "\n",
    "### 5. モデルと関連ファイルの保存\n",
    "\n",
    "```python\n",
    "trainer.save_model()\n",
    "model.config.save_pretrained(training_args.output_dir)\n",
    "tokenizer.save_pretrained(training_args.output_dir)\n",
    "```\n",
    "\n",
    "これは実際のモデルと必要なファイルを保存します：\n",
    "- 学習済みモデル（ソクラテス式の対話ができるようになったモデル）\n",
    "- モデルの設定ファイル\n",
    "- トークナイザー（テキストを数値に変換するツール）\n",
    "\n",
    "### 6. エラー処理\n",
    "\n",
    "```python\n",
    "except Exception as e:\n",
    "    logging.error(f\"An error occurred: {str(e)}\")\n",
    "    raise\n",
    "```\n",
    "\n",
    "保存中にエラーが発生した場合の対策です。例えば：\n",
    "- ディスク容量不足\n",
    "- 保存先へのアクセス権限がない\n",
    "などの問題が発生した場合に、エラーメッセージを記録します。\n",
    "\n",
    "このように、この部分は学習したモデルと設定を安全に保存し、後で再利用できるようにする重要な役割を果たしています。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
