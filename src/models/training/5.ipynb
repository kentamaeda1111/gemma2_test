{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. トレーニング設定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "このセクションについて、ソクラテス式チャットボットの文脈で説明させていただきます。\n",
    "\n",
    "### 1. データセットの分割\n",
    "```python\n",
    "dataset_size = len(tokenized_dataset)\n",
    "indices = np.random.permutation(dataset_size)\n",
    "split_idx = int(dataset_size * 0.8)\n",
    "train_dataset = tokenized_dataset.select(indices[:split_idx])\n",
    "eval_dataset = tokenized_dataset.select(indices[split_idx:split_idx+50])\n",
    "```\n",
    "\n",
    "これは、対話データを「練習用」と「テスト用」に分けている部分です。例えると:\n",
    "- 全体の80%を「練習用」(train_dataset)として使用\n",
    "- 残りから50個の対話を「テスト用」(eval_dataset)として使用\n",
    "\n",
    "たとえば1000個の対話データがあった場合:\n",
    "- 800個を使って実際にソクラテス式の話し方を学習\n",
    "- 50個を使って「本当にソクラテス式の対話ができているか」をチェック\n",
    "\n",
    "### 2. トレーニング設定（TrainingArguments）\n",
    "主な設定を説明します：\n",
    "\n",
    "```python\n",
    "training_args = TrainingArguments(\n",
    "    num_train_epochs=30,  # 30回繰り返して学習\n",
    "    learning_rate=8e-5,   # 学習の速度\n",
    "    warmup_ratio=0.25,    # 準備運動の期間\n",
    "    eval_steps=20,        # 20ステップごとに評価\n",
    "    per_device_train_batch_size=2  # 一度に学習する対話の数\n",
    ")\n",
    "```\n",
    "\n",
    "これは「学習の進め方」を決める部分です。例えると:\n",
    "\n",
    "- `num_train_epochs=30`: \n",
    "  - 全ての対話データを30回繰り返して学習\n",
    "  - 人間で言えば、同じ教材を30回復習するようなもの\n",
    "\n",
    "- `learning_rate=8e-5`: \n",
    "  - 学習の速度を調整\n",
    "  - 小さい値なので、慎重に少しずつ学習する\n",
    "  - ソクラテスの話し方を急いで真似るのではなく、じっくり身につける\n",
    "\n",
    "- `warmup_ratio=0.25`:\n",
    "  - 学習の最初25%は準備運動期間\n",
    "  - 人間で言えば、いきなり本格的な対話をするのではなく、簡単な問答から始めるイメージ\n",
    "\n",
    "- `eval_steps=20`:\n",
    "  - 20ステップごとに「テスト」を実施\n",
    "  - 定期的に「本当にソクラテスらしい対話ができているか」をチェック\n",
    "\n",
    "### 3. データ整理（DataCollator）\n",
    "```python\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    "    pad_to_multiple_of=8\n",
    ")\n",
    "```\n",
    "\n",
    "これは、対話データを学習しやすい形に整理する部分です。例えると:\n",
    "- 様々な長さの対話を、コンピュータが扱いやすいように整えます\n",
    "- ソクラテスの対話集を、学習しやすいように教科書のような形式に整理するイメージです\n",
    "\n",
    "このセクション全体は、効率的かつ効果的に「ソクラテス式の対話の仕方」を学べるように、学習環境を整えている部分だと考えてください。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Split dataset into training and evaluation sets\n",
    "dataset_size = len(tokenized_dataset)\n",
    "indices = np.random.permutation(dataset_size)\n",
    "split_idx = int(dataset_size * 0.8)\n",
    "train_dataset = tokenized_dataset.select(indices[:split_idx])\n",
    "# Limit evaluation dataset size\n",
    "eval_dataset = tokenized_dataset.select(indices[split_idx:split_idx+50])  # Maximum 50 samples\n",
    "\n",
    "logging.info(f\"Training dataset size: {len(train_dataset)}\")\n",
    "logging.info(f\"Evaluation dataset size: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "このコードの部分について、ソクラテス式チャットボットのトレーニングを例に説明させていただきます。\n",
    "\n",
    "```python\n",
    "# データセット全体のサイズを取得\n",
    "dataset_size = len(tokenized_dataset)\n",
    "\n",
    "# データセットのインデックスをランダムに並び替え\n",
    "indices = np.random.permutation(dataset_size)\n",
    "\n",
    "# 全体の80%を訓練用に使うためのインデックスを計算\n",
    "split_idx = int(dataset_size * 0.8)\n",
    "\n",
    "# 訓練用データセットを作成（全体の80%）\n",
    "train_dataset = tokenized_dataset.select(indices[:split_idx])\n",
    "\n",
    "# 評価用データセットを作成（残りの20%から最大50サンプル）\n",
    "eval_dataset = tokenized_dataset.select(indices[split_idx:split_idx+50])\n",
    "```\n",
    "\n",
    "これを具体例で説明すると：\n",
    "\n",
    "1. **データセットの分割**\n",
    "   - 例えば、1000個のソクラテス式の対話データがあるとします\n",
    "   - この1000個のデータを「訓練用」と「評価用」に分けます\n",
    "   - 訓練用には800個（80%）を使います\n",
    "   - 評価用には残りの200個から50個だけを使います\n",
    "\n",
    "2. **なぜランダムに並び替えるのか？**\n",
    "   - 例えば、データが時系列順に並んでいると：\n",
    "     - 最初の方に「倫理についての対話」\n",
    "     - 真ん中に「知識についての対話」\n",
    "     - 後半に「美についての対話」\n",
    "   というように偏りがある可能性があります\n",
    "   - ランダムに並び替えることで、訓練データとテストデータの両方に様々な種類の対話が含まれるようになります\n",
    "\n",
    "3. **なぜ評価データを50サンプルに制限するのか？**\n",
    "   - 評価には時間がかかるため、全てのデータを使うと非効率です\n",
    "   - 例えば：\n",
    "     - 「正しい質問の仕方を学べているか？」\n",
    "     - 「ソクラテス式の対話の流れを維持できているか？」\n",
    "     - 「相手の考えを深める問いかけができているか？」\n",
    "   などを確認するのに、50個のサンプルで十分な評価ができます\n",
    "\n",
    "4. **ログ出力の意味**\n",
    "```python\n",
    "logging.info(f\"Training dataset size: {len(train_dataset)}\")\n",
    "logging.info(f\"Evaluation dataset size: {len(eval_dataset)}\")\n",
    "```\n",
    "   - これは、分割したデータの数を記録します\n",
    "   - 例：「訓練データ：800個、評価データ：50個」というように\n",
    "   - 後で「このモデルは何個のデータで訓練したんだっけ？」と確認できるようにするためです\n",
    "\n",
    "このように、データを適切に分割することで、モデルが「ソクラテスのように対話する」能力を正しく学習し、その成果を適切に評価できるようになります。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Disable wandb via environment variable (add before training_args)\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "はい、この部分について説明させていただきます。\n",
    "\n",
    "```python\n",
    "# Disable wandb via environment variable (add before training_args)\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "```\n",
    "\n",
    "これは「Weights & Biases (wandb)」という機械学習の実験管理ツールを無効にする設定です。以下に詳しく説明します：\n",
    "\n",
    "1. **Weights & Biases (wandb)とは？**\n",
    "   - 機械学習の訓練過程を記録・可視化するためのツール\n",
    "   - 例えば：\n",
    "     - ソクラテス式チャットボットの学習経過\n",
    "     - 損失値（モデルの誤差）の推移\n",
    "     - GPUの使用状況\n",
    "   などを綺麗なグラフで表示できます\n",
    "\n",
    "2. **なぜ無効にするのか？**\n",
    "   - 主な理由：\n",
    "     - Kaggle環境では外部サービスへの接続が制限される\n",
    "     - wandbを使用するにはインターネット接続とアカウント設定が必要\n",
    "     - 訓練に集中したい場合、余計な機能は省きたい\n",
    "   \n",
    "3. **どうやって無効にしているのか？**\n",
    "   ```python\n",
    "   import os  # OSの環境変数を操作するためのモジュール\n",
    "   os.environ[\"WANDB_DISABLED\"] = \"true\"  # wandbを無効化\n",
    "   ```\n",
    "   - 環境変数で「WANDB_DISABLED」を「true」に設定\n",
    "   - これにより、wandbの機能が完全に無効化される\n",
    "\n",
    "4. **なぜtraining_argsの前に設定するのか？**\n",
    "   - training_args（訓練の設定）を行う前に無効化しないと\n",
    "   - wandbが自動的に初期化されてしまう可能性がある\n",
    "   - そのため、先に無効化の設定を行う\n",
    "\n",
    "5. **代替の監視方法**\n",
    "   - wandbの代わりに、このコードでは：\n",
    "     - カスタムのログ機能\n",
    "     - TrainingMonitorCallback\n",
    "     - StyleCallback\n",
    "   などを使って、ソクラテス式チャットボットの訓練進捗を監視しています\n",
    "\n",
    "このように、wandbを無効化することで、Kaggle環境での安定した訓練実行が可能になり、かつ独自の監視機能で十分な訓練管理ができるようになっています。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Update training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=MODEL_OUTPUT_DIR,  \n",
    "    num_train_epochs=30,\n",
    "    learning_rate=8e-5,\n",
    "    weight_decay=0.06,\n",
    "    warmup_ratio=0.25,\n",
    "    lr_scheduler_type=\"cosine_with_restarts\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=20,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=20,\n",
    "    gradient_accumulation_steps=8,\n",
    "    max_steps=-1,\n",
    "    disable_tqdm=False,\n",
    "    logging_dir=LOG_OUTPUT_DIR,   \n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    no_cuda=False,\n",
    "    dataloader_num_workers=1,\n",
    "    report_to=[],\n",
    "    run_name=None,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_checkpointing=True,\n",
    "    max_grad_norm=0.5,\n",
    "    dataloader_pin_memory=True,\n",
    "    save_total_limit=2,\n",
    "    fp16=True,\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    eval_accumulation_steps=4,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"combined_score\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "\n",
    "\n",
    "TrainingArgumentsの設定について、ソクラテス式チャットボットの訓練を例に解説します。\n",
    "\n",
    "### 基本的な設定\n",
    "```python\n",
    "output_dir=MODEL_OUTPUT_DIR,  # モデルの保存先\n",
    "num_train_epochs=30,         # 訓練の繰り返し回数\n",
    "learning_rate=8e-5,         # 学習率（モデルの学習速度）\n",
    "```\n",
    "- 30エポック（全データを30回学習）\n",
    "- 学習率は小さめ（0.00008）に設定し、慎重に学習を進める\n",
    "- 例：ソクラテスの対話スタイルを急激な変更でなく、徐々に学習\n",
    "\n",
    "### 学習率の調整関連\n",
    "```python\n",
    "weight_decay=0.06,          # 重み減衰（過学習防止）\n",
    "warmup_ratio=0.25,          # 学習率のウォームアップ比率\n",
    "lr_scheduler_type=\"cosine_with_restarts\",  # 学習率のスケジュール\n",
    "```\n",
    "- 最初の25%は学習率を徐々に上げる（ウォームアップ）\n",
    "- その後、コサイン関数のように学習率を変動\n",
    "- 例：\n",
    "  - 最初は「簡単な問いかけ方」から学習\n",
    "  - 徐々に「深い思考を促す質問」の学習へ\n",
    "\n",
    "### 評価と保存の設定\n",
    "```python\n",
    "evaluation_strategy=\"steps\",  # ステップごとに評価\n",
    "eval_steps=20,               # 20ステップごとに評価\n",
    "save_strategy=\"steps\",       # ステップごとに保存\n",
    "save_steps=20,              # 20ステップごとに保存\n",
    "save_total_limit=2,         # 保存するチェックポイントの数を2つに制限\n",
    "```\n",
    "- 20ステップごとにモデルを評価・保存\n",
    "- 最新の2つのチェックポイントのみ保存（容量節約）\n",
    "- 例：「対話の自然さ」や「ソクラテス式の問いかけの質」を定期的にチェック\n",
    "\n",
    "### バッチ処理関連\n",
    "```python\n",
    "gradient_accumulation_steps=8,        # 勾配蓄積ステップ数\n",
    "per_device_train_batch_size=2,       # 訓練時のバッチサイズ\n",
    "per_device_eval_batch_size=1,        # 評価時のバッチサイズ\n",
    "```\n",
    "- 小さいバッチサイズ（2）で慎重に学習\n",
    "- 8ステップ分の勾配を蓄積して更新\n",
    "- 例：少数の対話例から慎重に学習し、急激な変化を避ける\n",
    "\n",
    "### 最適化と精度関連\n",
    "```python\n",
    "fp16=True,                    # 16ビット精度を使用\n",
    "optim=\"adamw_torch_fused\",    # 最適化アルゴリズム\n",
    "max_grad_norm=0.5,           # 勾配クリッピングの最大値\n",
    "```\n",
    "- 16ビット精度で計算（メモリ効率化）\n",
    "- AdamWオプティマイザーの最適化版を使用\n",
    "- 勾配の急激な変化を防ぐ（0.5で制限）\n",
    "\n",
    "### モニタリングと評価\n",
    "```python\n",
    "logging_strategy=\"steps\",     # ログ記録の戦略\n",
    "logging_steps=50,            # ログを記録するステップ間隔\n",
    "load_best_model_at_end=True,  # 最良モデルを最後に読み込む\n",
    "metric_for_best_model=\"combined_score\",  # モデル評価の指標\n",
    "```\n",
    "- 50ステップごとに進捗をログ記録\n",
    "- 「combined_score」（対話の質を総合評価）が最も良いモデルを採用\n",
    "- 例：「問いかけの適切さ」「対話の流れ」「ソクラテス式の特徴」を総合的に評価\n",
    "\n",
    "### その他の設定\n",
    "```python\n",
    "disable_tqdm=False,           # 進捗バーを表示\n",
    "gradient_checkpointing=True,  # メモリ効率化のための勾配チェックポイント\n",
    "dataloader_pin_memory=True,   # データローダーのメモリピン留め\n",
    "```\n",
    "- メモリ使用を最適化しつつ、訓練の進捗を可視化\n",
    "- 大規模なモデルでもメモリ効率よく訓練可能\n",
    "\n",
    "これらの設定により、ソクラテス式の対話スタイルを効率的かつ効果的に学習できる環境が整います。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Modify data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    "    pad_to_multiple_of=8\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "\n",
    "\n",
    "データコレーターの設定について、ソクラテス式チャットボットの文脈で説明させていただきます。\n",
    "\n",
    "```python\n",
    "# データコレーターの設定\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,     # 使用するトークナイザー\n",
    "    mlm=False,              # マスク言語モデリングを無効化\n",
    "    pad_to_multiple_of=8    # パディングを8の倍数に\n",
    ")\n",
    "```\n",
    "\n",
    "### データコレーターとは？\n",
    "- バッチ処理のために複数の対話データを適切にまとめる役割を持つ\n",
    "- いわば「対話データのパッケージング担当者」のようなもの\n",
    "\n",
    "### 具体的な設定の説明\n",
    "\n",
    "1. **tokenizer=tokenizer**\n",
    "   - 使用するトークナイザーを指定\n",
    "   - 例：\n",
    "     ```\n",
    "     ユーザー: 「正義とは何だと思いますか？」\n",
    "     アシスタント: 「その質問は興味深いですね。あなたにとって正義とは何でしょうか？」\n",
    "     ```\n",
    "   - このような対話をトークン（単語や文字の単位）に分割\n",
    "\n",
    "2. **mlm=False**\n",
    "   - MLM（Masked Language Modeling）を無効化\n",
    "   - なぜ無効なのか：\n",
    "     - MLMは文章の一部を隠して予測する事前学習手法\n",
    "     - チャットボットの場合は、対話の流れを順番に学習したい\n",
    "     - 例：\n",
    "       - ×：「[MASK]とは何だと思いますか？」を予測\n",
    "       - ○：「質問に対して、さらに問いかけを返す」を学習\n",
    "\n",
    "3. **pad_to_multiple_of=8**\n",
    "   - データの長さを8の倍数に調整\n",
    "   - なぜ8の倍数か：\n",
    "     - GPU処理の効率化のため\n",
    "     - メモリアクセスの最適化\n",
    "   - 例：\n",
    "     ```\n",
    "     短い対話：「なぜですか？」（5トークン）\n",
    "     → 8トークンに調整（3トークン分パディング）\n",
    "     \n",
    "     長い対話：「その考えの根拠は...」（22トークン）\n",
    "     → 24トークンに調整（2トークン分パディング）\n",
    "     ```\n",
    "\n",
    "### 実際の処理例\n",
    "\n",
    "```\n",
    "入力対話：\n",
    "ユーザー: 「知識とは何でしょうか？」\n",
    "アシスタント: 「面白い質問ですね。まず、あなたは知識をどのように定義しますか？」\n",
    "\n",
    "処理手順：\n",
    "1. トークン化\n",
    "2. 必要に応じて8の倍数になるようパディング\n",
    "3. バッチにまとめる\n",
    "```\n",
    "\n",
    "### メリット\n",
    "- GPU処理の効率化\n",
    "- メモリ使用の最適化\n",
    "- 安定した学習の実現\n",
    "\n",
    "このデータコレーターの設定により、ソクラテス式の対話データを効率的に処理し、スムーズな学習が可能になります。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
