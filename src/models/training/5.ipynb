{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. トレーニング実行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "このコードの部分について、ソクラテス式チャットボットの学習を例に説明させていただきます。\n",
    "\n",
    "### 1. チェックポイント管理 (5.1の前半部分)\n",
    "\n",
    "チェックポイントとは、モデルの学習途中の状態を保存したものです。例えば：\n",
    "\n",
    "- ソクラテスボットが100個の対話を学習した時点\n",
    "- 200個の対話を学習した時点\n",
    "- 300個の対話を学習した時点\n",
    "\n",
    "というように、定期的に学習状態を保存します。これは以下のような理由で重要です：\n",
    "\n",
    "1. **学習の中断と再開**：\n",
    "   - 例：深夜にサーバーがメンテナンスで停止しても、最後のチェックポイントから再開できる\n",
    "   - 例：学習中にエラーが発生しても、最後の安定した状態から再開できる\n",
    "\n",
    "2. **進捗確認**：\n",
    "   - どれだけの対話を学習したか\n",
    "   - ソクラテス式の問答がどの程度上手くなってきているか\n",
    "   を確認できます\n",
    "\n",
    "コードでは、チェックポイントのディレクトリを確認し、以前の学習データがあれば、その状態を確認します。もし学習が既に完了していれば（設定したエポック数に達していれば）、誤って再学習することを防ぐために処理を終了します。\n",
    "\n",
    "### 2. モデルの学習実行 (5.1の後半部分)\n",
    "\n",
    "`trainer.train()`で実際の学習を開始します。この過程で：\n",
    "- ソクラテス式の対話データを少しずつモデルに学習させる\n",
    "- 定期的に学習の進捗を確認する\n",
    "- 必要に応じてチェックポイントを保存する\n",
    "\n",
    "といった処理が行われます。\n",
    "\n",
    "### 3. ベストモデルと設定の保存 (5.2)\n",
    "\n",
    "学習が完了したら、以下の情報を保存します：\n",
    "\n",
    "1. **学習設定の保存**：\n",
    "   - どのような設定で学習を行ったか（学習率、バッチサイズなど）\n",
    "   - どのようなモデルを使用したか\n",
    "   などの情報を`training_config.json`として保存\n",
    "\n",
    "2. **最良のモデルの保存**：\n",
    "   - 学習中で最も性能の良かったモデル（最も上手くソクラテス式の対話ができるようになった状態）を保存\n",
    "   - このモデルの性能指標（どれくらい上手く対話ができるか）も一緒に保存\n",
    "\n",
    "3. **トークナイザーの保存**：\n",
    "   - モデルが文章を理解するために必要な単語分割の規則も保存\n",
    "\n",
    "これらの情報は、後で：\n",
    "- モデルを使って実際にチャットボットを動かす時\n",
    "- 学習設定を改善する時\n",
    "- 追加の学習を行う時\n",
    "などに必要となります。\n",
    "\n",
    "このように、このセクションは「学習の実行」「進捗管理」「結果の保存」という、モデル学習の重要な工程を管理している部分だと言えます。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "### 5.1 チェックポイント管理とトレーニング実行\n",
    "\n",
    "# Start training\n",
    "logging.info(\"Starting training...\")\n",
    "try:\n",
    "    checkpoint_dir = MODEL_OUTPUT_DIR  \n",
    "    resume_from_checkpoint = None\n",
    "    \n",
    "    # Check if running in Kaggle environment\n",
    "    is_kaggle = os.path.exists('/kaggle/working')\n",
    "    \n",
    "    # Checkpoint status and processing\n",
    "    if os.path.exists(checkpoint_dir):\n",
    "        print(\"\\nChecking checkpoint status...\")  \n",
    "        checkpoints = [f for f in os.listdir(checkpoint_dir) if f.startswith(\"checkpoint-\")]\n",
    "        if checkpoints:\n",
    "            # Get latest checkpoint\n",
    "            latest_checkpoint = max(checkpoints, key=lambda x: int(x.split(\"-\")[1]))\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, latest_checkpoint)\n",
    "            print(f\"Found latest checkpoint: {latest_checkpoint}\") \n",
    "            \n",
    "            # Check checkpoint status\n",
    "            state_path = os.path.join(checkpoint_path, \"trainer_state.json\")\n",
    "            if os.path.exists(state_path):\n",
    "                with open(state_path, 'r') as f:\n",
    "                    state = json.load(f)\n",
    "                current_epoch = state.get('epoch', 0)\n",
    "                print(f\"\\nCurrent training status:\")  \n",
    "                print(f\"Current epoch: {current_epoch}\")  \n",
    "                print(f\"Target epochs: {training_args.num_train_epochs}\")  \n",
    "                \n",
    "                # Exit safely if completed\n",
    "                if current_epoch >= training_args.num_train_epochs - 0.1:\n",
    "                    print(\"\\n\" + \"=\"*50)\n",
    "                    print(\"IMPORTANT NOTICE:\")\n",
    "                    print(f\"Training has already been completed at epoch {current_epoch}!\")\n",
    "                    print(f\"Target epochs was {training_args.num_train_epochs}\")  \n",
    "                    print(f\"Trained model is available at: {checkpoint_dir}\")\n",
    "                    print(\"=\"*50 + \"\\n\")\n",
    "                    logging.info(\"Training has already been completed. Exiting to protect existing model.\")\n",
    "                    logging.info(f\"Trained model is available at: {checkpoint_dir}\")\n",
    "                    exit(0)\n",
    "            else:\n",
    "                logging.warning(\"Invalid checkpoint state found. Please check manually.\")\n",
    "                logging.warning(f\"Checkpoint directory: {checkpoint_dir}\")\n",
    "        else:\n",
    "            logging.warning(\"Checkpoint directory exists but no checkpoints found.\")\n",
    "            logging.info(\"Continuing with training...\")  # 追加: 自動的に続行\n",
    "\n",
    "    # Start training (or resume)\n",
    "    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
    "    logging.info(\"Training completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "このコードについて、ソクラテス式チャットボットのトレーニングを例に説明させていただきます。\n",
    "\n",
    "### チェックポイントとは？\n",
    "チェックポイントとは、モデルの学習途中の状態を保存したものです。例えば、ソクラテス式チャットボットの学習において、「問答形式での対話の仕方」を学習している途中で、何らかの理由で学習が中断された場合に、最初からやり直すのではなく、途中から再開できるようにするための仕組みです。\n",
    "\n",
    "### コードの主な機能\n",
    "\n",
    "1. **チェックポイントの確認**\n",
    "```python\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    print(\"\\nChecking checkpoint status...\")  \n",
    "    checkpoints = [f for f in os.listdir(checkpoint_dir) if f.startswith(\"checkpoint-\")]\n",
    "```\n",
    "- まず、以前の学習データ（チェックポイント）が存在するかを確認します\n",
    "- 例：前回の学習で「ソクラテスのような質問の仕方」を5時間学習した後に中断した場合、その続きから始められるようにします\n",
    "\n",
    "2. **最新のチェックポイントを見つける**\n",
    "```python\n",
    "if checkpoints:\n",
    "    latest_checkpoint = max(checkpoints, key=lambda x: int(x.split(\"-\")[1]))\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, latest_checkpoint)\n",
    "```\n",
    "- 複数のチェックポイントがある場合、最新のものを探します\n",
    "- 例：「checkpoint-1000」（1000ステップ目）、「checkpoint-2000」（2000ステップ目）があった場合、2000ステップ目の方を選びます\n",
    "\n",
    "3. **学習の進捗確認**\n",
    "```python\n",
    "current_epoch = state.get('epoch', 0)\n",
    "print(f\"Current epoch: {current_epoch}\")  \n",
    "print(f\"Target epochs: {training_args.num_train_epochs}\")  \n",
    "```\n",
    "- 現在の学習回数（エポック）と目標の学習回数を確認します\n",
    "- 例：30エポックの学習を予定していて、既に25エポック終わっていれば、残り5エポックだと分かります\n",
    "\n",
    "4. **学習完了チェック**\n",
    "```python\n",
    "if current_epoch >= training_args.num_train_epochs - 0.1:\n",
    "    print(\"Training has already been completed!\")\n",
    "    exit(0)\n",
    "```\n",
    "- 既に学習が完了している場合は、誤って再学習することを防ぎます\n",
    "- 例：ソクラテス式の対話の仕方を既に十分学習済みの場合、不要な追加学習を防ぎます\n",
    "\n",
    "5. **学習の開始または再開**\n",
    "```python\n",
    "trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
    "```\n",
    "- チェックポイントがある場合はそこから再開し、ない場合は新規に学習を開始します\n",
    "- 例：\n",
    "  - 新規の場合：ソクラテス式の対話の学習を一から開始\n",
    "  - 再開の場合：前回の「なぜそう考えるのですか？」といった質問の仕方を学んだところから継続\n",
    "\n",
    "このシステムにより、長時間かかる学習プロセスを安全に管理し、途中で中断しても再開できる仕組みを実現しています。まるで、本を読むときのしおりのような役割を果たしているのです。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "    ### 5.2 ベストモデルと設定の保存\n",
    "    # Save settings (as JSON)\n",
    "    def convert_to_serializable(obj):\n",
    "        if isinstance(obj, set):\n",
    "            return list(obj)\n",
    "        elif isinstance(obj, dict):\n",
    "            return {k: convert_to_serializable(v) for k, v in obj.items()}\n",
    "        elif isinstance(obj, (list, tuple)):\n",
    "            return [convert_to_serializable(x) for x in obj]\n",
    "        return obj\n",
    "\n",
    "    # Convert each setting\n",
    "    training_args_dict = convert_to_serializable(training_args.to_dict())\n",
    "    lora_config_dict = convert_to_serializable(lora_config.to_dict())\n",
    "\n",
    "    config_dict = {\n",
    "        \"model_name\": model_name,\n",
    "        \"training_args\": training_args_dict,\n",
    "        \"lora_config\": lora_config_dict,\n",
    "        \"bnb_config\": {\n",
    "            \"load_in_4bit\": bnb_config.load_in_4bit,\n",
    "            \"bnb_4bit_use_double_quant\": bnb_config.bnb_4bit_use_double_quant,\n",
    "            \"bnb_4bit_quant_type\": bnb_config.bnb_4bit_quant_type,\n",
    "            \"bnb_4bit_compute_dtype\": str(bnb_config.bnb_4bit_compute_dtype),\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save configurations\n",
    "    with open(os.path.join(training_args.output_dir, \"training_config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(config_dict, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    # トレーナーが保持している最良のモデルを保存\n",
    "    # load_best_model_at_end=Trueにより、この時点で既にbestモデルがロードされている\n",
    "    best_model_path = os.path.join(training_args.output_dir, \"best_model\")\n",
    "    os.makedirs(best_model_path, exist_ok=True)\n",
    "    \n",
    "    # Save best model and its configuration\n",
    "    trainer.model.save_pretrained(best_model_path)\n",
    "    model.config.save_pretrained(best_model_path)\n",
    "    tokenizer.save_pretrained(best_model_path)\n",
    "    \n",
    "    # Save a marker file indicating this is the best model\n",
    "    with open(os.path.join(best_model_path, \"best_model_info.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        best_metrics = {\n",
    "            \"best_metric\": trainer.state.best_metric,\n",
    "            \"best_model_checkpoint\": trainer.state.best_model_checkpoint,\n",
    "            \"best_perplexity\": trainer.state.best_metric\n",
    "        }\n",
    "        json.dump(best_metrics, f, indent=2)\n",
    "    \n",
    "    logging.info(f\"Best model saved to {best_model_path}\")\n",
    "    logging.info(f\"Best perplexity: {trainer.state.best_metric}\")\n",
    "    logging.info(\"Model and configuration saved successfully!\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"An error occurred: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "このコードについて、ソクラテス式チャットボットの例を用いて説明させていただきます。\n",
    "\n",
    "### 1. データの保存形式の準備\n",
    "```python\n",
    "def convert_to_serializable(obj):\n",
    "    if isinstance(obj, set):\n",
    "        return list(obj)\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: convert_to_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, (list, tuple)):\n",
    "        return [convert_to_serializable(x) for x in obj]\n",
    "    return obj\n",
    "```\n",
    "これは、複雑なデータ構造をJSON形式で保存できるように変換する関数です。\n",
    "例：ソクラテス式の対話パターンのセット（集合）を保存可能なリスト形式に変換します。\n",
    "\n",
    "### 2. 設定情報の保存\n",
    "```python\n",
    "config_dict = {\n",
    "    \"model_name\": model_name,\n",
    "    \"training_args\": training_args_dict,\n",
    "    \"lora_config\": lora_config_dict,\n",
    "    \"bnb_config\": {\n",
    "        \"load_in_4bit\": bnb_config.load_in_4bit,\n",
    "        ...\n",
    "    }\n",
    "}\n",
    "```\n",
    "モデルの設定情報をまとめて保存します。例えば：\n",
    "- モデル名：「google/gemma-2-2b-jpn-it」\n",
    "- 学習設定：エポック数30回、学習率2e-4など\n",
    "- LoRA設定：質問の仕方を効率的に学習するための設定\n",
    "- 量子化設定：モデルを軽量化するための設定\n",
    "\n",
    "### 3. ベストモデルの保存\n",
    "```python\n",
    "best_model_path = os.path.join(training_args.output_dir, \"best_model\")\n",
    "trainer.model.save_pretrained(best_model_path)\n",
    "model.config.save_pretrained(best_model_path)\n",
    "tokenizer.save_pretrained(best_model_path)\n",
    "```\n",
    "最も性能の良かったモデルを保存します。例えば：\n",
    "- 30エポックの学習の中で、最も「ソクラテス式の対話」が上手くできたモデルを保存\n",
    "- モデルの設定と、単語を処理するためのトークナイザーも一緒に保存\n",
    "\n",
    "### 4. ベストモデルの性能情報の保存\n",
    "```python\n",
    "best_metrics = {\n",
    "    \"best_metric\": trainer.state.best_metric,\n",
    "    \"best_model_checkpoint\": trainer.state.best_model_checkpoint,\n",
    "    \"best_perplexity\": trainer.state.best_metric\n",
    "}\n",
    "```\n",
    "モデルの性能指標を保存します。例：\n",
    "- パープレキシティ（文章の自然さを示す指標）\n",
    "- どのチェックポイントが最も良かったか\n",
    "- いつの時点で最高の性能を記録したか\n",
    "\n",
    "### 5. エラー処理\n",
    "```python\n",
    "except Exception as e:\n",
    "    logging.error(f\"An error occurred: {str(e)}\")\n",
    "    raise\n",
    "```\n",
    "保存処理中に問題が発生した場合（ディスク容量不足など）、エラーログを記録して通知します。\n",
    "\n",
    "このコードは、まるでソクラテス式チャットボットの「完成品」と「作り方のレシピ」の両方を保存するようなものです。後で同じような対話モデルを作りたい場合や、モデルを改良したい場合に、これらの情報が重要な参考資料となります。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
