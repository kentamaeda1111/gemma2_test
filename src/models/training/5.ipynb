{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Execution and Model Management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "このコードの部分について、ソクラテス式チャットボットの開発を例に説明させていただきます。\n",
    "\n",
    "この部分は大きく分けて3つの重要な処理を行っています：\n",
    "\n",
    "### 1. データの分割 (5.1 Data Set Split and Validation)\n",
    "```python\n",
    "dataset_size = len(tokenized_dataset)\n",
    "indices = np.random.permutation(dataset_size)\n",
    "split_idx = int(dataset_size * 0.8)\n",
    "```\n",
    "これは、用意した会話データを「学習用」と「評価用」に分ける作業です。\n",
    "\n",
    "例えば、1000個の会話データがあった場合：\n",
    "- 800個（80%）を学習用データとして使用\n",
    "- 残りの中から最大100個を評価用データとして使用\n",
    "\n",
    "評価用データは、モデルが本当にソクラテス風の話し方を身につけているか確認するために使います。\n",
    "\n",
    "### 2. トレーニングの準備と実行 (5.2 Training Execution Control)\n",
    "```python\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    ...\n",
    ")\n",
    "```\n",
    "これは、実際の学習を行うための準備です。例えると、以下のような設定を行っています：\n",
    "- モデル（生徒）に「かね？」という語尾の使い方を教える\n",
    "- 質問で返すパターンを学ばせる\n",
    "- 「では、そこで君に問おう」といった老練な言い回しを覚えさせる\n",
    "\n",
    "### 3. チェックポイントと保存の管理\n",
    "```python\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    logging.info(\"\\nChecking checkpoint status...\")\n",
    "    checkpoints = [f for f in os.listdir(checkpoint_dir) if f.startswith(\"checkpoint-\")]\n",
    "```\n",
    "これは、学習の途中経過を保存する仕組みです。\n",
    "\n",
    "例えば：\n",
    "- 10時間の学習予定で、2時間ごとに進捗を保存\n",
    "- 途中で停電などが起きても、最後に保存した地点から再開可能\n",
    "- 学習が完了したら、ソクラテス風の話し方を習得したモデルを保存\n",
    "\n",
    "実際の例で言うと：\n",
    "```\n",
    "モデル「なるほど、君は研究での成功を喜びと感じるようだね」\n",
    "↓ （学習中に停電）\n",
    "↓ （前回の保存地点から再開）\n",
    "↓ （学習完了）\n",
    "モデル「では、その喜びと幸せは、何か違いがあるのかね？」\n",
    "```\n",
    "\n",
    "このように、長時間の学習プロセスを安全に管理しながら、目指すソクラテス風の話し方を習得させていく仕組みになっています。\n",
    "\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1 Data Set Split and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 5.1 Data Set Split and Validation\n",
    "# Data set split\n",
    "dataset_size = len(tokenized_dataset)\n",
    "indices = np.random.permutation(dataset_size)\n",
    "split_idx = int(dataset_size * 0.8)\n",
    "\n",
    "# Create training and test datasets\n",
    "train_dataset = tokenized_dataset.select(indices[:split_idx])\n",
    "eval_dataset = tokenized_dataset.select(indices[split_idx:split_idx+100])  # Maximum 100 samples\n",
    "\n",
    "# Record dataset size\n",
    "logging.info(f\"Training dataset size: {len(train_dataset)}\")\n",
    "logging.info(f\"Evaluation dataset size: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "はい、この部分について、ソクラテス式チャットボットの開発に即して、より詳しく説明させていただきます。\n",
    "\n",
    "### コードの詳細説明\n",
    "\n",
    "```python\n",
    "# データセットの総数を取得\n",
    "dataset_size = len(tokenized_dataset)\n",
    "\n",
    "# データをランダムに並び替えるための準備\n",
    "indices = np.random.permutation(dataset_size)\n",
    "\n",
    "# 80%のデータを学習用とするための分割点を計算\n",
    "split_idx = int(dataset_size * 0.8)\n",
    "```\n",
    "\n",
    "これは、会話データを「学習用」と「評価用」に分ける処理です。具体的な例を使って説明します：\n",
    "\n",
    "例えば、以下のような10,000個の会話データがあるとします：\n",
    "\n",
    "```json\n",
    "[\n",
    "  {\n",
    "    \"messages\": [\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"幸せとは何だと思いますか？\"\n",
    "      },\n",
    "      {\n",
    "        \"role\": \"model\",\n",
    "        \"content\": \"君にとって幸せとは何かを、まず考えてみてはどうかね？\"\n",
    "      }\n",
    "    ]\n",
    "  },\n",
    "  // ... 他の会話データ ...\n",
    "]\n",
    "```\n",
    "\n",
    "このデータを以下のように分割します：\n",
    "\n",
    "```python\n",
    "# 学習用と評価用のデータセットを作成\n",
    "train_dataset = tokenized_dataset.select(indices[:split_idx])\n",
    "eval_dataset = tokenized_dataset.select(indices[split_idx:split_idx+100])\n",
    "```\n",
    "\n",
    "1. **学習用データ（train_dataset）**\n",
    "   - 全体の80%（この例では8,000個）の会話を使用\n",
    "   - モデルが「ソクラテス風の話し方」を学ぶための教材\n",
    "   - 例：\n",
    "     - 「〜かね？」という語尾の使い方\n",
    "     - 質問で返す話し方\n",
    "     - 「では、君はどう考えるのかね？」といった問いかけ方\n",
    "\n",
    "2. **評価用データ（eval_dataset）**\n",
    "   - 残りのデータから最大100個を使用\n",
    "   - モデルが正しく学習できているか確認するためのテスト問題\n",
    "   - 例：\n",
    "     - ユーザー：「研究が思うように進まなくて悩んでいます」\n",
    "     - 期待する応答：「なるほど。では君は、その研究の何が思うように進まないと感じているのかね？」\n",
    "\n",
    "```python\n",
    "# データセットのサイズを記録\n",
    "logging.info(f\"Training dataset size: {len(train_dataset)}\")\n",
    "logging.info(f\"Evaluation dataset size: {len(eval_dataset)}\")\n",
    "```\n",
    "\n",
    "この部分は、分割したデータの数を記録します。これにより：\n",
    "- 実際に何個のデータで学習するのか\n",
    "- 何個のデータでテストするのか\n",
    "が明確になり、後で学習結果を評価する際の参考になります。\n",
    "\n",
    "このような分割が必要な理由は、「暗記」と「理解」を区別するためです。学習用データだけで評価すると、モデルが単に会話を「暗記」しただけなのか、本当に「ソクラテス風の話し方」を「理解」したのか区別できません。評価用の新しいデータで試すことで、モデルが本当にソクラテス風の対話の特徴を学習できているか確認できるのです。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2 Training Execution Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 5.2 Training Execution Control\n",
    "# Trainer initialization\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[StyleCallback(), TrainingMonitorCallback()],\n",
    ")\n",
    "\n",
    "# Check memory state\n",
    "log_memory_usage()\n",
    "\n",
    "# Training execution\n",
    "logging.info(\"Starting training...\")\n",
    "try:\n",
    "    checkpoint_dir = \"./model\"\n",
    "    resume_from_checkpoint = None\n",
    "    \n",
    "    # Checkpoint status and processing modification\n",
    "    if os.path.exists(checkpoint_dir):\n",
    "        logging.info(\"\\nChecking checkpoint status...\")\n",
    "        checkpoints = [f for f in os.listdir(checkpoint_dir) if f.startswith(\"checkpoint-\")]\n",
    "        if checkpoints:\n",
    "            # Get latest checkpoint\n",
    "            latest_checkpoint = max(checkpoints, key=lambda x: int(x.split(\"-\")[1]))\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, latest_checkpoint)\n",
    "            logging.info(f\"Found latest checkpoint: {latest_checkpoint}\")\n",
    "            \n",
    "            # Check checkpoint status\n",
    "            state_path = os.path.join(checkpoint_path, \"trainer_state.json\")\n",
    "            if os.path.exists(state_path):\n",
    "                with open(state_path, 'r') as f:\n",
    "                    state = json.load(f)\n",
    "                current_epoch = state.get('epoch', 0)\n",
    "                logging.info(f\"\\nCurrent training status:\")\n",
    "                logging.info(f\"Current epoch: {current_epoch}\")\n",
    "                logging.info(f\"Target epochs: {training_args.num_train_epochs}\")\n",
    "                \n",
    "                # Exit safely if completed\n",
    "                if current_epoch >= training_args.num_train_epochs - 0.1:\n",
    "                    logging.info(\"\\n\" + \"=\"*50)\n",
    "                    logging.info(\"IMPORTANT NOTICE:\")\n",
    "                    logging.info(f\"Training has already been completed at epoch {current_epoch}!\")\n",
    "                    logging.info(f\"Target epochs was {training_args.num_train_epochs}\")\n",
    "                    logging.info(f\"Trained model is available at: {checkpoint_dir}\")\n",
    "                    logging.info(\"=\"*50 + \"\\n\")\n",
    "                    exit(0)\n",
    "            else:\n",
    "                logging.warning(\"Invalid checkpoint state found. Proceeding with training...\")\n",
    "                logging.warning(f\"Checkpoint directory: {checkpoint_dir}\")\n",
    "        else:\n",
    "            logging.warning(\"Checkpoint directory exists but no checkpoints found. Proceeding with training...\")\n",
    "\n",
    "    # Start learning (or resume)\n",
    "    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
    "    logging.info(\"Training completed successfully!\")\n",
    "    \n",
    "    # Save settings (as JSON)\n",
    "    import json\n",
    "\n",
    "    def convert_to_serializable(obj):\n",
    "        if isinstance(obj, set):\n",
    "            return list(obj)\n",
    "        elif isinstance(obj, dict):\n",
    "            return {k: convert_to_serializable(v) for k, v in obj.items()}\n",
    "        elif isinstance(obj, (list, tuple)):\n",
    "            return [convert_to_serializable(x) for x in obj]\n",
    "        return obj\n",
    "\n",
    "    # Convert all settings\n",
    "    training_args_dict = convert_to_serializable(training_args.to_dict())\n",
    "    lora_config_dict = convert_to_serializable(lora_config.to_dict())\n",
    "\n",
    "    config_dict = {\n",
    "        \"model_name\": model_name,\n",
    "        \"training_args\": training_args_dict,\n",
    "        \"lora_config\": lora_config_dict,\n",
    "        \"bnb_config\": {\n",
    "            \"load_in_4bit\": bnb_config.load_in_4bit,\n",
    "            \"bnb_4bit_use_double_quant\": bnb_config.bnb_4bit_use_double_quant,\n",
    "            \"bnb_4bit_quant_type\": bnb_config.bnb_4bit_quant_type,\n",
    "            \"bnb_4bit_compute_dtype\": str(bnb_config.bnb_4bit_compute_dtype),\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(training_args.output_dir, \"training_config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(config_dict, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    # Save model\n",
    "    trainer.save_model()\n",
    "    # Save settings\n",
    "    model.config.save_pretrained(training_args.output_dir)\n",
    "    tokenizer.save_pretrained(training_args.output_dir)\n",
    "    logging.info(\"Model and configuration saved successfully!\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"An error occurred: {str(e)}\")\n",
    "    # Checkpoint is also kept even on error\n",
    "    raise \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "\n",
    "\n",
    "この部分について、ソクラテス式チャットボットの開発プロジェクトに即して説明させていただきます。大きく分けて4つの重要な部分があります：\n",
    "\n",
    "### 1. トレーナーの初期化\n",
    "```python\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[StyleCallback(), TrainingMonitorCallback()],\n",
    ")\n",
    "```\n",
    "これは学習の「先生役」を設定する部分です。具体的には：\n",
    "- `StyleCallback`: ソクラテス風の話し方（「〜かね？」「では、君は〜」など）が properly できているか監視\n",
    "- `TrainingMonitorCallback`: 学習の進み具合を監視し、グラフなどで可視化\n",
    "\n",
    "### 2. チェックポイントの管理\n",
    "```python\n",
    "checkpoint_dir = \"./model\"\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    logging.info(\"\\nChecking checkpoint status...\")\n",
    "    checkpoints = [f for f in os.listdir(checkpoint_dir) if f.startswith(\"checkpoint-\")]\n",
    "```\n",
    "これは学習の途中経過を保存・管理する部分です。例えば：\n",
    "\n",
    "- 1000回の対話練習のうち、100回ごとに進捗を保存\n",
    "- もし200回目で停電が起きても、100回目の状態から再開可能\n",
    "- 既に完了している場合（例：1000回終わっている）は、再度学習を開始しない\n",
    "\n",
    "### 3. 学習の実行\n",
    "```python\n",
    "trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
    "logging.info(\"Training completed successfully!\")\n",
    "```\n",
    "実際の学習を行う部分です。例えば：\n",
    "```\n",
    "ユーザー: 「幸せとは何でしょうか？」\n",
    "モデル: 「そうだね...」（← まだソクラテス風ではない）\n",
    "↓ （学習を重ねる）\n",
    "モデル: 「君にとって幸せとは何かを、まず考えてみてはどうかね？」（← ソクラテス風に進化）\n",
    "```\n",
    "\n",
    "### 4. 設定と成果物の保存\n",
    "```python\n",
    "# Save settings (as JSON)\n",
    "config_dict = {\n",
    "    \"model_name\": model_name,\n",
    "    \"training_args\": training_args_dict,\n",
    "    \"lora_config\": lora_config_dict,\n",
    "    ...\n",
    "}\n",
    "```\n",
    "学習が終わった後の処理です：\n",
    "- 学習したモデルを保存（後で使えるように）\n",
    "- 学習設定を保存（どういう設定で学習したか記録）\n",
    "- トークナイザーを保存（言葉の解釈方法を保存）\n",
    "\n",
    "例えると、以下のようなものを保存：\n",
    "- ソクラテス風の話し方のパターン\n",
    "- 「〜かね？」という語尾の使い方\n",
    "- 質問で返すタイミング\n",
    "- 「では、そこで君に問おう」といった特徴的なフレーズの使い方\n",
    "\n",
    "このように、学習全体のプロセスを管理し、途中で問題が起きても安全に再開でき、最終的な成果物をきちんと保存する仕組みになっています。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
