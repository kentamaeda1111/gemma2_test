{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "このコードの「Model Architecture」の部分について、プロジェクトの文脈に沿って分かりやすく説明させていただきます。\n",
    "\n",
    "この部分は大きく4つのステップに分かれており、ソクラテス風チャットボットを作るための「モデルの準備」を行っています：\n",
    "\n",
    "### 1. メモリ効率化の設定 (3.1 Quantization Setup)\n",
    "```python\n",
    "bnb_config = BitsAndBytesConfig(...)\n",
    "```\n",
    "これは、大きな言語モデルを普通のパソコンでも動かせるようにする工夫です。\n",
    "\n",
    "例えば、通常のモデルはとても大きなメモリを必要としますが、この設定により数値の精度を少し落とす代わりに、メモリ使用量を大幅に削減します。具体的には32ビットの数値を4ビットに圧縮するような形です。\n",
    "\n",
    "### 2. ベースモデルの読み込み (3.2 Basic Model Initialization)\n",
    "```python\n",
    "model = AutoModelForCausalLM.from_pretrained(...)\n",
    "```\n",
    "ここでは、基礎となる「gemma-2-2b-jpn-it」というモデルを読み込んでいます。これは日本語が使える比較的小さな言語モデルで、これをベースにソクラテス風の話し方を学習させていきます。\n",
    "\n",
    "### 3. 効率的な学習の準備 (3.3 LoRA Setup and Application)\n",
    "```python\n",
    "lora_config = LoraConfig(...)\n",
    "model = get_peft_model(model, lora_config)\n",
    "```\n",
    "LoRAという特殊な学習方法を設定しています。これは、モデル全体を変更するのではなく、「口調」に関係する部分だけを効率的に学習させる方法です。\n",
    "\n",
    "たとえば、「ですます調」を「〜かね？」という口調に変えるような学習を、モデルの一部分だけで効率的に行えるようにします。これにより：\n",
    "- 学習が速くなる\n",
    "- 必要なメモリが少なくて済む\n",
    "- 元のモデルの基本的な能力（日本語理解など）を保持したまま、口調だけを変更できる\n",
    "\n",
    "### 4. データ処理の準備 (3.4 Model Optimization Setup)\n",
    "```python\n",
    "data_collator = DataCollatorForLanguageModeling(...)\n",
    "```\n",
    "学習データ（ソクラテス風の対話例）をモデルに効率よく供給するための準備をしています。例えば：\n",
    "\n",
    "```\n",
    "User: 幸せとは何だと思いますか？\n",
    "Model: 君にとって幸せとは何かね？その定義から考えてみようではないか。\n",
    "```\n",
    "\n",
    "このような対話データを、モデルが理解しやすい形に整形して供給する役割を果たします。\n",
    "\n",
    "これらの設定により、通常のパソコンでも効率的にソクラテス風の話し方を学習させることができる環境が整います。\n",
    "\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Quantization Setup (BitsAndBytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 3.1 Quantization Setup (BitsAndBytes)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_storage=torch.uint8,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "はい、Quantization（量子化）の設定について、より詳しく説明させていただきます。\n",
    "\n",
    "### 基本的な考え方\n",
    "\n",
    "まず、言語モデル（今回の場合はGemma-2b）の中身は、大量の数値（パラメータ）の集まりです。これらの数値が「言葉の理解」や「ソクラテス風の返答」を可能にしています。\n",
    "\n",
    "通常、これらの数値は32ビット（または16ビット）の精度で保存されています。例えば：\n",
    "- 「〜かね？」という口調の特徴を表す数値：1.2847629375...\n",
    "- 「問いかけ」のパターンを表す数値：0.9374628461...\n",
    "\n",
    "### Quantizationで何が変わるのか\n",
    "\n",
    "このコードでは、4ビットの精度に圧縮しています：\n",
    "\n",
    "```python\n",
    "load_in_4bit=True  # 4ビットでの読み込みを有効化\n",
    "```\n",
    "\n",
    "32ビット → 4ビットへの圧縮により：\n",
    "- 元の数値：1.2847629375...\n",
    "- 圧縮後：1.3（近似値）\n",
    "\n",
    "これにより、メモリ使用量を約8分の1に削減できます。\n",
    "\n",
    "### 精度を保つための工夫\n",
    "\n",
    "```python\n",
    "bnb_4bit_use_double_quant=True  # 二重量子化を有効化\n",
    "bnb_4bit_quant_type=\"nf4\"       # 特殊な4ビット形式を使用\n",
    "```\n",
    "\n",
    "単純に4ビットに圧縮すると精度が落ちすぎる可能性があるため、以下の工夫をしています：\n",
    "\n",
    "1. `double_quant=True`：\n",
    "   - 二段階の圧縮を行い、重要な情報をより正確に保持\n",
    "   - 例：「〜かね？」という特徴的な口調の情報は、より正確に保持\n",
    "\n",
    "2. `quant_type=\"nf4\"`：\n",
    "   - 言語モデル用に最適化された4ビット形式を使用\n",
    "   - 通常の4ビット形式より、言語理解に重要な数値の精度を保持\n",
    "\n",
    "### 計算時の設定\n",
    "\n",
    "```python\n",
    "bnb_4bit_compute_dtype=torch.float16  # 計算時は16ビットを使用\n",
    "bnb_4bit_quant_storage=torch.uint8    # 保存時は8ビット単位で格納\n",
    "```\n",
    "\n",
    "- 保存時は4ビットで圧縮\n",
    "- 実際の計算時は16ビットで処理（精度と速度のバランス）\n",
    "- データはメモリ上で8ビット単位で整理（コンピュータが扱いやすい形式）\n",
    "\n",
    "### なぜこれが重要か？\n",
    "\n",
    "例えば、ソクラテス風の対話モデルを作る際：\n",
    "\n",
    "1. 元のモデル（圧縮なし）：\n",
    "   - 必要メモリ：約4GB\n",
    "   - 一般的なPCでは扱いが困難\n",
    "\n",
    "2. 圧縮後のモデル：\n",
    "   - 必要メモリ：約0.5GB\n",
    "   - 一般的なPCでも十分動作可能\n",
    "   - ソクラテス風の口調（「〜かね？」など）の学習に必要な精度は維持\n",
    "\n",
    "このように、モデルの本質的な能力を保ちながら、より少ないリソースで動作させることができます。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Basic Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 3.2 Basic Model Initialization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation='eager',\n",
    "    token=huggingface_token  # API token added\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "\n",
    "\n",
    "はい、Basic Model Initializationの部分について説明させていただきます。\n",
    "\n",
    "この部分は、ベースとなる言語モデル（Gemma-2b）を読み込んで初期設定を行う重要な箇所です。一行ずつ見ていきましょう：\n",
    "\n",
    "### 1. モデルの基本読み込み\n",
    "```python\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,  # \"google/gemma-2-2b-jpn-it\"\n",
    "```\n",
    "- `AutoModelForCausalLM`は「次の言葉を予測できる言語モデル」を意味します\n",
    "- 例えば：\n",
    "  ```\n",
    "  ユーザー: 正義とは何でしょうか？\n",
    "  モデル: 君は正義とは何だと考えるかね？\n",
    "  ```\n",
    "  このような対話形式の予測ができるモデルです\n",
    "\n",
    "### 2. 省メモリ設定の適用\n",
    "```python\n",
    "    quantization_config=bnb_config,\n",
    "```\n",
    "- 先ほど説明した4ビット圧縮の設定を適用します\n",
    "- これにより、8GBのGPUでも動作可能になります\n",
    "\n",
    "### 3. デバイスの自動選択\n",
    "```python\n",
    "    device_map=\"auto\",\n",
    "```\n",
    "- モデルをGPUとCPUのどちらで動かすかを自動で決定します\n",
    "- GPUが利用可能な場合はGPUを、なければCPUを使用\n",
    "- 複数のGPUがある場合は、最適な配分を自動で行います\n",
    "\n",
    "### 4. 数値形式の指定\n",
    "```python\n",
    "    torch_dtype=torch.bfloat16,\n",
    "```\n",
    "- `bfloat16`は、通常の16ビット形式の特殊版です\n",
    "- 特に言語モデルの学習に適しています\n",
    "- 例：「〜かね？」という口調を学習する際の数値の扱いが安定します\n",
    "\n",
    "### 5. 注意機構の実装方式\n",
    "```python\n",
    "    attn_implementation='eager',\n",
    "```\n",
    "- `eager`は「素直な実装」を意味します\n",
    "- より複雑な実装方式もありますが、安定性を重視してこちらを選択\n",
    "- ソクラテス風の対話学習には、この安定した実装が適しています\n",
    "\n",
    "### 6. 認証トークン\n",
    "```python\n",
    "    token=huggingface_token  # API token added\n",
    "```\n",
    "- Hugging Faceというサービスからモデルをダウンロードするための認証情報\n",
    "- これがないとモデルをダウンロードできません\n",
    "\n",
    "### なぜこの設定が重要か？\n",
    "\n",
    "例えば、以下のような対話を学習させる際：\n",
    "```\n",
    "ユーザー: 幸せについて考えています\n",
    "モデル: 君は幸せとは何だと思うかね？\n",
    "```\n",
    "\n",
    "この設定により：\n",
    "1. 必要最小限のメモリで動作\n",
    "2. 利用可能な最適なハードウェアを自動選択\n",
    "3. 学習時の数値計算が安定\n",
    "4. セキュアにモデルをダウンロード\n",
    "\n",
    "これらが全て自動的に最適な形で設定され、ソクラテス風の対話モデルの学習が可能になります。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 LoRA Setup and Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 3.3 LoRA Setup and Application\n",
    "# LoRA parameter setup\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Create and initialize LoRA model\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "\n",
    "\n",
    "LoRA (Low-Rank Adaptation) の設定について、プロジェクトの文脈に沿って説明させていただきます。\n",
    "\n",
    "### LoRAとは？\n",
    "まず、LoRAは「大きな言語モデルの一部分だけを効率的に学習させる」ための手法です。\n",
    "\n",
    "通常の学習では、モデル全体を更新しますが、LoRAでは「差分」だけを学習します：\n",
    "```\n",
    "元のGemma → LoRAで学習 → ソクラテス風Gemma\n",
    "```\n",
    "\n",
    "### パラメータの詳細説明\n",
    "\n",
    "```python\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # ランク（学習の複雑さ）\n",
    "```\n",
    "- `r=16`は学習の「細かさ」を指定\n",
    "- 大きいほど細かい特徴まで学習できる\n",
    "- 例：\n",
    "  - 低い値：「〜かね？」という基本的な口調の変更\n",
    "  - 高い値：「では、その考えについて掘り下げてみようかね？」といった複雑な表現も学習\n",
    "\n",
    "```python\n",
    "    lora_alpha=32,  # スケーリング係数\n",
    "```\n",
    "- 学習の強さを調整\n",
    "- `r`との比率（alpha/r = 2）で、学習の影響力を決定\n",
    "- 今回は適度な強さで、元の日本語能力を保ちながらソクラテス風の特徴を学習\n",
    "\n",
    "```python\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "```\n",
    "- 学習対象となるモジュール（モデルの部品）を指定\n",
    "- これらは「注意機構」と呼ばれる、文脈理解に重要な部分\n",
    "- 例：\n",
    "  ```\n",
    "  ユーザー: 正義について考えています\n",
    "  モデル: 君にとって正義とは何かね？\n",
    "  ```\n",
    "  この対話で、「正義」というキーワードに対して「問いかけ」で返すパターンを学習\n",
    "\n",
    "```python\n",
    "    lora_dropout=0.1,  # ドロップアウト率\n",
    "```\n",
    "- 過学習（暗記学習）を防ぐための設定\n",
    "- 10%の確率で一時的に学習をスキップ\n",
    "- これにより、より汎用的なソクラテス風の対話が可能に\n",
    "\n",
    "```python\n",
    "    bias=\"none\",  # バイアスの学習設定\n",
    "    task_type=\"CAUSAL_LM\",  # タスクタイプ\n",
    "```\n",
    "- 基本的な学習設定\n",
    "- `CAUSAL_LM`は対話形式の学習に適した設定\n",
    "\n",
    "### モデルの初期化\n",
    "```python\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "```\n",
    "1. `prepare_model_for_kbit_training`: \n",
    "   - 4ビット量子化したモデルをLoRA学習用に準備\n",
    "\n",
    "2. `get_peft_model`: \n",
    "   - LoRAの設定を適用し、学習可能な状態にする\n",
    "\n",
    "### なぜこの方法が効果的か？\n",
    "\n",
    "従来の方法との比較：\n",
    "1. 通常の学習\n",
    "   - モデル全体（約2GB）を更新\n",
    "   - 大量のメモリと時間が必要\n",
    "   - 元の日本語能力が損なわれるリスク\n",
    "\n",
    "2. LoRAでの学習\n",
    "   - 差分のみ（数MB）を更新\n",
    "   - 少ないメモリで高速学習\n",
    "   - 元の日本語能力を保ちながら、ソクラテス風の特徴だけを効率的に学習\n",
    "\n",
    "これにより、効率的かつ効果的にソクラテス風チャットボットを作成することができます。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4 Model Optimization Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 3.4 Model Optimization Setup\n",
    "# Optimize cache setup\n",
    "model.config.use_cache = False\n",
    "\n",
    "# Data collator setup\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    "    pad_to_multiple_of=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "\n",
    "\n",
    "モデルの最適化設定について、プロジェクトの文脈に沿って説明させていただきます。\n",
    "\n",
    "### 1. キャッシュの無効化\n",
    "```python\n",
    "model.config.use_cache = False\n",
    "```\n",
    "\n",
    "これは「記憶の使い方」の設定です。\n",
    "\n",
    "- キャッシュとは？\n",
    "  - モデルの「一時的なメモ」のようなもの\n",
    "  - 計算結果を一時的に保存する場所\n",
    "\n",
    "- なぜ無効にするのか？\n",
    "  ```\n",
    "  ユーザー: 正義について考えています\n",
    "  モデル: 君は正義とは何だと思うかね？\n",
    "  ユーザー: 正義とは、、、\n",
    "  ```\n",
    "  このような対話を学習する際：\n",
    "  1. キャッシュありの場合：\n",
    "     - 前の応答を「覚えすぎて」しまい\n",
    "     - 同じような返答ばかりする可能性がある\n",
    "  \n",
    "  2. キャッシュなしの場合：\n",
    "     - 毎回新鮮な目で文脈を理解\n",
    "     - より柔軟なソクラテス風の応答が可能\n",
    "\n",
    "### 2. データコレーターの設定\n",
    "```python\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,  # トークン化ツール\n",
    "    mlm=False,           # マスク言語モデルの無効化\n",
    "    pad_to_multiple_of=8 # データの長さ調整\n",
    ")\n",
    "```\n",
    "\n",
    "これは「学習データの整理係」のような役割です：\n",
    "\n",
    "1. `tokenizer=tokenizer`\n",
    "   - 日本語の文章をモデルが理解できる数値に変換\n",
    "   - 例：\n",
    "     ```\n",
    "     「君は正義とは何だと思うかね？」\n",
    "     ↓\n",
    "     [1045, 2358, 7890, ...]  # 数値の例\n",
    "     ```\n",
    "\n",
    "2. `mlm=False`\n",
    "   - `False`は「次の言葉を予測する」モード\n",
    "   - 対話形式の学習に適している\n",
    "   - 例：\n",
    "     ```\n",
    "     入力: 「君は正義とは何だと」\n",
    "     予測: 「思うかね？」\n",
    "     ```\n",
    "\n",
    "3. `pad_to_multiple_of=8`\n",
    "   - データの長さを8の倍数に調整\n",
    "   - コンピュータの計算効率を上げる\n",
    "   - 例：\n",
    "     ```\n",
    "     元のデータ長: 29トークン\n",
    "     調整後: 32トークン（8の倍数）\n",
    "     ```\n",
    "\n",
    "### なぜこの設定が重要か？\n",
    "\n",
    "ソクラテス風の対話を学習させる際：\n",
    "\n",
    "1. キャッシュなし\n",
    "   - より自然な問答が可能\n",
    "   - 「〜かね？」という口調を機械的に繰り返すことを防ぐ\n",
    "\n",
    "2. データコレーター\n",
    "   - 対話データを効率的に処理\n",
    "   - メモリ使用を最適化\n",
    "   - 学習速度を向上\n",
    "\n",
    "これらの設定により、より自然で効率的なソクラテス風対話の学習が可能になります。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
