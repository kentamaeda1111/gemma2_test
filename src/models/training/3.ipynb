{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 3.モデルとトークナイザーの設定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "このコードセクションについて、ソクラテス式チャットボットの文脈で説明させていただきます。\n",
    "\n",
    "### 1. モデルとトークナイザーの準備\n",
    "\n",
    "```python\n",
    "model_name = \"google/gemma-2-2b-jpn-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(...)\n",
    "```\n",
    "\n",
    "これは、ベースとなる言語モデル（Gemma）を選択し、その「言葉の理解者」（トークナイザー）を準備している部分です。\n",
    "\n",
    "例えると、ソクラテスが使う日本語の辞書を用意しているようなものです。この辞書は「なるほど」「だろうか？」といった言葉をコンピュータが理解できる数値（トークン）に変換します。\n",
    "\n",
    "### 2. 特殊トークンの追加\n",
    "\n",
    "```python\n",
    "tokenizer.add_special_tokens({\n",
    "    'additional_special_tokens': ['。', '、', '！', '？']\n",
    "})\n",
    "```\n",
    "\n",
    "ソクラテス式の対話で重要な句読点（「。」「、」）や感情を表す記号（「！」「？」）を特別な意味を持つ記号として追加しています。これにより、モデルは「ではないでしょうか？」という問いかけの「？」の重要性をより理解できるようになります。\n",
    "\n",
    "### 3. メモリ効率化の設定\n",
    "\n",
    "```python\n",
    "bnb_config = BitsAndBytesConfig(...)\n",
    "```\n",
    "\n",
    "これは、大きな言語モデルをより少ないメモリで動かすための設定です。4ビット量子化という技術を使って、モデルを圧縮しています。\n",
    "\n",
    "例えると、ソクラテスの知識を詰め込んだ分厚い本を、要点だけを残して薄い本にまとめるようなものです。\n",
    "\n",
    "### 4. モデルのロードと設定\n",
    "\n",
    "```python\n",
    "model = AutoModelForCausalLM.from_pretrained(...)\n",
    "```\n",
    "\n",
    "圧縮設定を適用しながら、実際のモデルをメモリに読み込みます。GPUメモリの使用量も細かく制御しています。\n",
    "\n",
    "### 5. LoRA（効率的な学習方法）の設定\n",
    "\n",
    "```python\n",
    "lora_config = LoraConfig(...)\n",
    "model = get_peft_model(model, lora_config)\n",
    "```\n",
    "\n",
    "これは、効率的な学習方法であるLoRAの設定です。モデル全体を更新するのではなく、重要な部分だけを調整します。\n",
    "\n",
    "例えると、ソクラテスの基本的な対話術は保持したまま、特定の話題（例：「なるほど、それは興味深い観点ですね」といった返答パターン）についての応答の仕方だけを調整するようなものです。\n",
    "\n",
    "このように、このセクションでは：\n",
    "1. 基本となる日本語対話モデルの選択\n",
    "2. 対話に重要な記号の追加\n",
    "3. メモリ効率化の設定\n",
    "4. モデルの読み込みと最適化\n",
    "5. 効率的な学習方法の設定\n",
    "\n",
    "を行っています。これらの設定により、限られたコンピュータリソースで効率的にソクラテス式の対話スタイルを学習できる環境が整います。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Model and tokenizer preparation\n",
    "model_name = \"google/gemma-2-2b-jpn-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    token=os.environ[\"HUGGINGFACE_TOKEN\"],  \n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "モデルとトークナイザーの設定部分について、わかりやすく説明させていただきます。\n",
    "\n",
    "```python\n",
    "# Model and tokenizer preparation\n",
    "model_name = \"google/gemma-2-2b-jpn-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    token=os.environ[\"HUGGINGFACE_TOKEN\"],  \n",
    "    trust_remote_code=True\n",
    ")\n",
    "```\n",
    "\n",
    "### 1. モデルの選択 (`model_name = \"google/gemma-2-2b-jpn-it\"`)\n",
    "\n",
    "これは、使用する基本のAIモデルを指定している部分です。今回選んでいるのは:\n",
    "- `google/gemma-2b-jpn-it`: Googleが開発した日本語対応の言語モデル\n",
    "- `2b`: 2billionのパラメータ（学習済みの知識量）を持つモデル\n",
    "- `jpn-it`: 日本語とIT分野に特化したバージョン\n",
    "\n",
    "例えば、ソクラテス式の対話を行う際に:\n",
    "```\n",
    "ユーザー: プログラミングについて教えてください\n",
    "AI: プログラミングとは何だと思いますか？まずはあなたの考えを聞かせてください\n",
    "```\n",
    "\n",
    "このような日本語での対話的な応答が可能になります。\n",
    "\n",
    "### 2. トークナイザーの設定\n",
    "\n",
    "```python\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    token=os.environ[\"HUGGINGFACE_TOKEN\"],  \n",
    "    trust_remote_code=True\n",
    ")\n",
    "```\n",
    "\n",
    "トークナイザーとは、テキストを機械が理解できる数値（トークン）に変換するツールです。\n",
    "\n",
    "例えば：\n",
    "```\n",
    "「なぜプログラミングを学びたいのですか？」\n",
    "↓\n",
    "[1245, 789, 4567, 2341, 9012]  # 実際の数値は異なります\n",
    "```\n",
    "\n",
    "重要なパラメータ:\n",
    "- `token=os.environ[\"HUGGINGFACE_TOKEN\"]`: モデルをダウンロードするための認証キー\n",
    "- `trust_remote_code=True`: Googleが提供する特別な処理コードを信頼して実行\n",
    "\n",
    "このトークナイザーの特徴:\n",
    "1. 日本語に最適化されている\n",
    "2. ソクラテス式の質問（「なぜ」「どのように」など）を適切に処理できる\n",
    "3. 文末表現（「〜ですか？」「〜でしょうか」）を正確に認識できる\n",
    "\n",
    "実際の使用例:\n",
    "```python\n",
    "# 入力テキストをトークン化\n",
    "text = \"プログラミングについて、あなたはどう考えていますか？\"\n",
    "tokens = tokenizer.encode(text)\n",
    "\n",
    "# トークンを再びテキストに戻す\n",
    "decoded_text = tokenizer.decode(tokens)\n",
    "```\n",
    "\n",
    "このように、ユーザーとAIの対話をスムーズに処理できる環境を整えています。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Add special tokens to tokenizer\n",
    "tokenizer.add_special_tokens({\n",
    "    'additional_special_tokens': [\n",
    "        '。', '、', '！', '？',  # Punctuation marks\n",
    "    ]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "はい、この部分について説明させていただきます。\n",
    "\n",
    "```python\n",
    "# Add special tokens to tokenizer\n",
    "tokenizer.add_special_tokens({\n",
    "    'additional_special_tokens': [\n",
    "        '。', '、', '！', '？',  # Punctuation marks\n",
    "    ]\n",
    "})\n",
    "```\n",
    "\n",
    "### 特殊トークンの追加とは？\n",
    "\n",
    "これは、日本語の重要な句読点（「。」「、」「！」「？」）を特別な意味を持つトークンとして登録する処理です。\n",
    "\n",
    "### なぜ必要なのか？\n",
    "\n",
    "1. **文の区切りの認識**\n",
    "   - 「。」：文の終わりを明確に認識\n",
    "   - 「、」：文の中での区切りを認識\n",
    "   \n",
    "2. **感情や意図の理解**\n",
    "   - 「！」：強い主張や感嘆\n",
    "   - 「？」：質問や疑問の表現\n",
    "\n",
    "### ソクラテス式対話での具体例：\n",
    "\n",
    "```\n",
    "【特殊トークンなしの場合】\n",
    "AI: プログラミングについて興味があるようですねどのような目的で学びたいと考えていますか\n",
    "\n",
    "【特殊トークンありの場合】\n",
    "AI: プログラミングについて興味があるようですね。どのような目的で学びたいと考えていますか？\n",
    "```\n",
    "\n",
    "### 効果：\n",
    "\n",
    "1. **読みやすい文章生成**\n",
    "   ```\n",
    "   悪い例：あなたはそう考えるのですかなぜそう思うのですか\n",
    "   良い例：あなたはそう考えるのですか？なぜそう思うのですか？\n",
    "   ```\n",
    "\n",
    "2. **ソクラテス式の対話の特徴を強調**\n",
    "   ```\n",
    "   質問の明確化：\n",
    "   「なぜそう考えるのですか？」\n",
    "   「その根拠は何でしょうか？」\n",
    "   \n",
    "   思考の促し：\n",
    "   「それについて、もう少し詳しく説明できますか？」\n",
    "   「その考えの背景には、どのような経験があるのでしょうか？」\n",
    "   ```\n",
    "\n",
    "3. **文の区切りの適切な制御**\n",
    "   ```\n",
    "   複数の問いかけ：\n",
    "   「その考えは興味深いですね。では、具体的な例を挙げていただけますか？」\n",
    "   \n",
    "   理由の掘り下げ：\n",
    "   「なるほど、そのように考えたのですね。その理由について、もう少し詳しく聞かせていただけますか？」\n",
    "   ```\n",
    "\n",
    "このように、特殊トークンを追加することで、より自然で読みやすい、ソクラテス式の対話に適した文章生成が可能になります。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Optimize BitsAndBytesConfig settings\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_storage=torch.uint8,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "\n",
    "\n",
    "BitsAndBytesConfigの設定について、わかりやすく説明させていただきます。\n",
    "\n",
    "```python\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_storage=torch.uint8,\n",
    ")\n",
    "```\n",
    "\n",
    "### これは何のための設定？\n",
    "\n",
    "これは**メモリ使用量を削減しながら、モデルの性能を維持するための最適化設定**です。\n",
    "\n",
    "### 主要な設定項目の説明：\n",
    "\n",
    "1. **`load_in_4bit=True`**\n",
    "   - 通常32ビットのモデルを4ビットに圧縮\n",
    "   - 例えば：\n",
    "     ```\n",
    "     【32ビット】8GB必要なモデル\n",
    "     ↓\n",
    "     【4ビット】約1GB程度まで削減\n",
    "     ```\n",
    "\n",
    "2. **`bnb_4bit_use_double_quant=True`**\n",
    "   - さらなるメモリ削減のための二重量子化を有効化\n",
    "   - メモリ使用量を約10-15%追加削減\n",
    "\n",
    "3. **`bnb_4bit_quant_type=\"nf4\"`**\n",
    "   - 4ビット量子化の種類を指定\n",
    "   - \"nf4\"は「Normal Float 4」の略で、通常の数値をより正確に表現\n",
    "\n",
    "4. **`bnb_4bit_compute_dtype=torch.float16`**\n",
    "   - 計算時の精度を16ビットに設定\n",
    "   - 例：ソクラテス式の返答を生成する際の計算精度\n",
    "\n",
    "5. **`bnb_4bit_quant_storage=torch.uint8`**\n",
    "   - データの保存形式を8ビット符号なし整数に設定\n",
    "\n",
    "### なぜこの設定が重要か？\n",
    "\n",
    "1. **メモリ効率**\n",
    "   ```\n",
    "   【従来】\n",
    "   ソクラテス式チャットボット: 8GB必要\n",
    "   ↓\n",
    "   【最適化後】\n",
    "   ソクラテス式チャットボット: 2GB程度で動作可能\n",
    "   ```\n",
    "\n",
    "2. **応答速度の維持**\n",
    "   ```\n",
    "   ユーザー: プログラミングについて教えてください\n",
    "   AI: （0.5秒程度で応答）なぜプログラミングに興味を持ったのですか？\n",
    "   ```\n",
    "\n",
    "3. **質の維持**\n",
    "   ```\n",
    "   【最適化前と同じ品質を維持】\n",
    "   - 適切な問いかけ\n",
    "   - 論理的な会話の展開\n",
    "   - 丁寧な言葉遣い\n",
    "   ```\n",
    "\n",
    "### 実際の効果：\n",
    "\n",
    "```\n",
    "【メモリ使用量】\n",
    "最適化前: 8GB\n",
    "最適化後: 2GB\n",
    "\n",
    "【応答時間】\n",
    "最適化前: 0.5秒\n",
    "最適化後: 0.5-0.6秒\n",
    "（ほぼ同じ速度を維持）\n",
    "\n",
    "【応答品質】\n",
    "ユーザー: プログラミングを学ぶべきでしょうか？\n",
    "\n",
    "AI: その質問について、まずはあなたご自身の考えをお聞かせいただけますか？\n",
    "プログラミングを学ぶことで、どのような目標を達成したいとお考えですか？\n",
    "```\n",
    "\n",
    "このように、メモリ使用量を大幅に削減しながらも、ソクラテス式の対話に必要な質と速度を維持することができます。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load model with modifications\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    token=os.environ[\"HUGGINGFACE_TOKEN\"],  \n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"balanced\",\n",
    "    torch_dtype=torch.float16,\n",
    "    attn_implementation='sdpa',\n",
    "    max_memory={0: \"4GiB\", 1: \"4GiB\", \"cpu\": \"24GB\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "\n",
    "\n",
    "モデルのロードと設定について、わかりやすく説明させていただきます。\n",
    "\n",
    "```python\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    token=os.environ[\"HUGGINGFACE_TOKEN\"],  \n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"balanced\",\n",
    "    torch_dtype=torch.float16,\n",
    "    attn_implementation='sdpa',\n",
    "    max_memory={0: \"4GiB\", 1: \"4GiB\", \"cpu\": \"24GB\"}\n",
    ")\n",
    "```\n",
    "\n",
    "### 1. 基本的な役割\n",
    "これは、ソクラテス式対話を行うAIモデルを実際にメモリにロードし、使用できる状態にする設定です。\n",
    "\n",
    "### 2. 各設定項目の説明\n",
    "\n",
    "#### ① モデルの取得\n",
    "```python\n",
    "model_name = \"google/gemma-2-2b-jpn-it\"\n",
    "token=os.environ[\"HUGGINGFACE_TOKEN\"]\n",
    "```\n",
    "- Hugging Faceからモデルをダウンロード\n",
    "- トークンを使って認証を行う\n",
    "\n",
    "#### ② メモリ最適化設定\n",
    "```python\n",
    "quantization_config=bnb_config\n",
    "torch_dtype=torch.float16\n",
    "```\n",
    "例えば：\n",
    "```\n",
    "【通常の場合】\n",
    "質問: プログラミングについて教えてください\n",
    "→ 8GBのメモリを使用\n",
    "\n",
    "【最適化後】\n",
    "質問: プログラミングについて教えてください\n",
    "→ 2GBのメモリで同じ質の応答が可能\n",
    "```\n",
    "\n",
    "#### ③ GPU使用の設定\n",
    "```python\n",
    "device_map=\"balanced\"\n",
    "max_memory={0: \"4GiB\", 1: \"4GiB\", \"cpu\": \"24GB\"}\n",
    "```\n",
    "- `device_map=\"balanced\"`: 2つのGPUに負荷を均等に分散\n",
    "- メモリ割り当て:\n",
    "  - GPU0: 4GB\n",
    "  - GPU1: 4GB\n",
    "  - CPU: 24GB\n",
    "\n",
    "実際の動作例：\n",
    "```\n",
    "【ユーザーからの質問】\n",
    "「プログラミングは難しいですか？」\n",
    "\n",
    "【処理の流れ】\n",
    "1. GPU0: 質問の理解と文脈の処理\n",
    "2. GPU1: ソクラテス式の応答生成\n",
    "3. CPU: バックアップとデータ保持\n",
    "\n",
    "【生成される応答】\n",
    "「『難しい』というのは、どのような意味でそう感じていますか？\n",
    "具体的に、どのような部分に不安を感じていますか？」\n",
    "```\n",
    "\n",
    "#### ④ 注意機構の実装\n",
    "```python\n",
    "attn_implementation='sdpa'\n",
    "```\n",
    "- SDPA (Scaled Dot-Product Attention)を使用\n",
    "- より効率的な注意機構の実装\n",
    "\n",
    "効果の例：\n",
    "```\n",
    "ユーザー: Pythonについて教えてください\n",
    "\n",
    "AI: その質問について掘り下げていきましょう。\n",
    "    ↓\n",
    "    [注意機構が働く]\n",
    "    ↓\n",
    "まず、Pythonについてどのような印象をお持ちですか？\n",
    "具体的に、どのような目的でPythonを学びたいとお考えですか？\n",
    "```\n",
    "\n",
    "### 3. この設定の利点\n",
    "\n",
    "1. **安定した対話性能**\n",
    "```\n",
    "ユーザー: プログラミングは役に立ちますか？\n",
    "\n",
    "AI: 「役に立つ」というのは、あなたにとってどのような意味を持ちますか？\n",
    "    具体的にどのような場面で活用したいとお考えですか？\n",
    "```\n",
    "\n",
    "2. **効率的なメモリ使用**\n",
    "```\n",
    "2つのGPUで分散処理\n",
    "↓\n",
    "より長い対話でも安定した応答が可能\n",
    "```\n",
    "\n",
    "3. **高速な応答生成**\n",
    "```\n",
    "質問への応答時間: 0.5-1秒程度\n",
    "対話の文脈を保持しながらの継続的な会話が可能\n",
    "```\n",
    "\n",
    "このように、限られたハードウェアリソースで最大限の性能を引き出す設定となっています。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Prepare model for LoRA and disable cache\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "\n",
    "\n",
    "LoRAの準備設定について、わかりやすく説明させていただきます。\n",
    "\n",
    "```python\n",
    "# Prepare model for LoRA and disable cache\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model.config.use_cache = False\n",
    "```\n",
    "\n",
    "### 1. LoRAとは？\n",
    "LoRA (Low-Rank Adaptation) は、大規模な言語モデルを効率的に微調整（ファインチューニング）するための手法です。\n",
    "\n",
    "例えるなら：\n",
    "```\n",
    "【通常の学習】\n",
    "本全体を書き換える\n",
    "\n",
    "【LoRA】\n",
    "付箋を貼って修正する\n",
    "```\n",
    "\n",
    "### 2. 設定の詳細説明\n",
    "\n",
    "#### ① モデルのLoRA準備\n",
    "```python\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "```\n",
    "\n",
    "これは何をしているか：\n",
    "- モデルをLoRA学習用に最適化\n",
    "- 4ビット量子化されたモデルで学習できるように調整\n",
    "\n",
    "実際の効果：\n",
    "```\n",
    "【変更前のモデル】\n",
    "一般的な対話：\n",
    "ユーザー: プログラミングについて教えて\n",
    "AI: プログラミングは○○です。△△が特徴です。\n",
    "\n",
    "【LoRA適用後】\n",
    "ソクラテス式の対話：\n",
    "ユーザー: プログラミングについて教えて\n",
    "AI: プログラミングについて、あなたはどのような印象をお持ちですか？\n",
    "    具体的に知りたい部分はどのような点でしょうか？\n",
    "```\n",
    "\n",
    "#### ② キャッシュの無効化\n",
    "```python\n",
    "model.config.use_cache = False\n",
    "```\n",
    "\n",
    "これが必要な理由：\n",
    "1. **メモリの効率的使用**\n",
    "   ```\n",
    "   キャッシュあり：\n",
    "   - メモリ使用量が増加\n",
    "   - 学習が遅くなる可能性\n",
    "\n",
    "   キャッシュなし：\n",
    "   - メモリ使用を最適化\n",
    "   - 学習が安定する\n",
    "   ```\n",
    "\n",
    "2. **より正確な学習**\n",
    "   ```\n",
    "   【キャッシュありの場合】\n",
    "   過去の応答パターンに引きずられる可能性\n",
    "\n",
    "   【キャッシュなしの場合】\n",
    "   毎回新鮮な目で応答を生成\n",
    "   ```\n",
    "\n",
    "### 3. 実際の効果例\n",
    "\n",
    "```\n",
    "【学習の進行による変化】\n",
    "\n",
    "初期段階：\n",
    "ユーザー: プログラミングは難しいですか？\n",
    "AI: はい、プログラミングは難しいです。\n",
    "\n",
    "↓ LoRA学習後 ↓\n",
    "\n",
    "完成段階：\n",
    "ユーザー: プログラミングは難しいですか？\n",
    "AI: 「難しい」と感じる理由について、もう少し具体的に教えていただけますか？\n",
    "    これまでのプログラミングの経験について、お聞かせいただけますでしょうか？\n",
    "```\n",
    "\n",
    "### 4. メリット\n",
    "\n",
    "1. **効率的な学習**\n",
    "   - 必要なパラメータのみを調整\n",
    "   - メモリ使用量を抑制\n",
    "\n",
    "2. **高速な適応**\n",
    "   ```\n",
    "   通常の学習: 数日〜数週間\n",
    "   LoRA学習: 数時間〜1日程度\n",
    "   ```\n",
    "\n",
    "3. **柔軟な調整**\n",
    "   ```\n",
    "   対話スタイルの微調整が容易\n",
    "   例：\n",
    "   - より丁寧な問いかけ\n",
    "   - より深い思考を促す質問\n",
    "   - より適切なフォローアップ\n",
    "   ```\n",
    "\n",
    "このように、LoRAを使用することで、効率的かつ効果的にソクラテス式対話モデルを作り上げることができます。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Adjust LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "LoRAの詳細設定について、わかりやすく説明させていただきます。\n",
    "\n",
    "```python\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                # ランクを指定\n",
    "    lora_alpha=32,       # スケーリング係数\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # 対象モジュール\n",
    "    lora_dropout=0.1,    # ドロップアウト率\n",
    "    bias=\"none\",         # バイアスの設定\n",
    "    task_type=\"CAUSAL_LM\",  # タスクタイプ\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "### 1. ランク設定 (`r=16`)\n",
    "```python\n",
    "r=16\n",
    "```\n",
    "\n",
    "これは学習の「細かさ」を決める値です。\n",
    "\n",
    "例えるなら：\n",
    "```\n",
    "r=4  : 太い筆で描く（大まかな学習）\n",
    "r=16 : 細い筆で描く（より繊細な学習）\n",
    "r=32 : より細い筆（さらに詳細な学習だが、メモリを多く使用）\n",
    "```\n",
    "\n",
    "実際の効果：\n",
    "```\n",
    "【r=4の場合】\n",
    "ユーザー: プログラミングについて教えて\n",
    "AI: なぜプログラミングを学びたいですか？\n",
    "\n",
    "【r=16の場合】\n",
    "ユーザー: プログラミングについて教えて\n",
    "AI: プログラミングについて興味を持たれた理由を、もう少し詳しくお聞かせいただけますでしょうか？\n",
    "    具体的にどのような目標をお持ちでしょうか？\n",
    "```\n",
    "\n",
    "\n",
    "### 2. スケーリング係数 (`lora_alpha=32`)\n",
    "```python\n",
    "lora_alpha=32\n",
    "```\n",
    "\n",
    "これは学習の「強さ」を調整する値です。\n",
    "\n",
    "- 小さい値：穏やかな学習\n",
    "- 大きい値：積極的な学習\n",
    "\n",
    "効果の例：\n",
    "```\n",
    "【alpha=16の場合】\n",
    "AI: プログラミングについて、どう思いますか？\n",
    "\n",
    "【alpha=32の場合】\n",
    "AI: プログラミングについて、あなたのお考えをお聞かせいただけますでしょうか？\n",
    "    特に、どのような点に関心をお持ちでしょうか？\n",
    "```\n",
    "\n",
    "\n",
    "### 3. 対象モジュール\n",
    "```python\n",
    "target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "```\n",
    "\n",
    "これは、モデルのどの部分を学習させるかを指定します。\n",
    "\n",
    "- `q_proj`: 質問の理解\n",
    "- `k_proj`: キーポイントの把握\n",
    "- `v_proj`: 価値判断\n",
    "- `o_proj`: 出力の生成\n",
    "\n",
    "実際の効果：\n",
    "```\n",
    "【学習前】\n",
    "ユーザー: Pythonは何に使えますか？\n",
    "AI: Pythonは様々な用途に使えます。\n",
    "\n",
    "【学習後】\n",
    "ユーザー: Pythonは何に使えますか？\n",
    "AI: その質問は興味深いですね。\n",
    "    まず、Pythonでどのようなことをしたいとお考えですか？\n",
    "    具体的な目標や興味のある分野はありますでしょうか？\n",
    "```\n",
    "\n",
    "\n",
    "### 4. ドロップアウト設定 (`lora_dropout=0.1`)\n",
    "```python\n",
    "lora_dropout=0.1\n",
    "```\n",
    "\n",
    "これは過学習を防ぐための設定です。\n",
    "\n",
    "例えるなら：\n",
    "```\n",
    "時々目をつぶって学習することで、\n",
    "より汎用的な理解力を身につける\n",
    "```\n",
    "\n",
    "効果：\n",
    "```\n",
    "【ドロップアウトなし】\n",
    "特定のフレーズに固執した応答\n",
    "\n",
    "【ドロップアウトあり】\n",
    "より柔軟で自然な対話が可能\n",
    "```\n",
    "\n",
    "\n",
    "### 5. タスクタイプ\n",
    "```python\n",
    "task_type=\"CAUSAL_LM\"\n",
    "```\n",
    "\n",
    "これは、モデルの使用目的を指定します。\n",
    "- `CAUSAL_LM`: 対話型の言語モデル\n",
    "\n",
    "実際の使用例：\n",
    "```\n",
    "ユーザー: プログラミングは難しいですか？\n",
    "\n",
    "AI: 「難しい」という感覚は人それぞれ異なりますね。\n",
    "    あなたが「難しい」と感じる具体的な部分について、\n",
    "    もう少し詳しくお聞かせいただけますでしょうか？\n",
    "    \n",
    "    例えば：\n",
    "    - 特定の概念の理解\n",
    "    - 文法の習得\n",
    "    - エラーの対処\n",
    "    \n",
    "    これらの中で、特に気になる点はありますか？\n",
    "```\n",
    "\n",
    "\n",
    "このように、LoRAの設定を適切に行うことで、ソクラテス式の対話スタイルを効果的に学習させることができます。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create LoRA model\n",
    "model = get_peft_model(model, lora_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "LoRAモデルの作成部分について、わかりやすく説明させていただきます。\n",
    "\n",
    "```python\n",
    "# Create LoRA model\n",
    "model = get_peft_model(model, lora_config)\n",
    "```\n",
    "\n",
    "\n",
    "### 1. この処理の意味\n",
    "\n",
    "これは、先ほど設定したLoRA設定を実際にモデルに適用する処理です。\n",
    "\n",
    "例えるなら：\n",
    "```\n",
    "【通常の本】\n",
    "↓\n",
    "【付箋を貼れるように加工された本】\n",
    "```\n",
    "\n",
    "\n",
    "### 2. 具体的な変化\n",
    "\n",
    "変換前と後の違いを見てみましょう：\n",
    "\n",
    "```\n",
    "【変換前のモデル】\n",
    "- 全パラメータの変更が必要\n",
    "- 大きなメモリが必要\n",
    "- 学習に時間がかかる\n",
    "\n",
    "【変換後のモデル（LoRA適用後）】\n",
    "- 必要な部分だけ調整可能\n",
    "- メモリ効率が良い\n",
    "- 短時間で学習可能\n",
    "```\n",
    "\n",
    "\n",
    "### 3. 実際の効果例\n",
    "\n",
    "```\n",
    "【学習前の応答】\n",
    "ユーザー: プログラミングを始めたいのですが...\n",
    "AI: プログラミングを始めるならPythonがおすすめです。\n",
    "\n",
    "【LoRA学習後の応答】\n",
    "ユーザー: プログラミングを始めたいのですが...\n",
    "AI: プログラミングに興味を持たれた理由について、\n",
    "    お聞かせいただけますでしょうか？\n",
    "\n",
    "    例えば：\n",
    "    - 特定の目標がありますか？\n",
    "    - どのような成果を期待されていますか？\n",
    "    - なぜ今、始めようと思われたのでしょうか？\n",
    "```\n",
    "\n",
    "\n",
    "### 4. 主なメリット\n",
    "\n",
    "1. **メモリ効率**\n",
    "   ```\n",
    "   【通常の学習】\n",
    "   必要メモリ: 8GB以上\n",
    "   \n",
    "   【LoRA適用後】\n",
    "   必要メモリ: 2GB程度\n",
    "   ```\n",
    "\n",
    "2. **学習時間の短縮**\n",
    "   ```\n",
    "   【通常の学習】\n",
    "   学習時間: 数日〜数週間\n",
    "   \n",
    "   【LoRA適用後】\n",
    "   学習時間: 数時間程度\n",
    "   ```\n",
    "\n",
    "3. **柔軟な調整**\n",
    "   ```\n",
    "   例：ソクラテス式の対話スタイルの強化\n",
    "   \n",
    "   【調整前】\n",
    "   AI: それは良い考えですね。\n",
    "   \n",
    "   【調整後】\n",
    "   AI: その考えに至った過程について、\n",
    "       もう少し詳しくお聞かせいただけますでしょうか？\n",
    "   ```\n",
    "\n",
    "\n",
    "### 5. 実際の学習例\n",
    "\n",
    "```\n",
    "【段階的な学習の様子】\n",
    "\n",
    "1. 基本的な質問\n",
    "ユーザー: Pythonとは何ですか？\n",
    "AI: Pythonについて、どのようなことをご存知ですか？\n",
    "\n",
    "2. 掘り下げる質問\n",
    "ユーザー: プログラミングを学びたいです\n",
    "AI: 具体的に、どのような目的でプログラミングを\n",
    "    学びたいとお考えですか？\n",
    "\n",
    "3. 思考を促す質問\n",
    "ユーザー: エラーが出て困っています\n",
    "AI: そのエラーについて、どのように解釈されていますか？\n",
    "    エラーメッセージからどのような情報が読み取れるでしょうか？\n",
    "```\n",
    "\n",
    "\n",
    "このように、LoRAモデルの作成により、効率的かつ効果的なソクラテス式対話の学習が可能になります。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
