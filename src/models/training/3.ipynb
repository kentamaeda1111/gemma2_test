{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. モデル設定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "このコードセクションについて、わかりやすく説明させていただきます。\n",
    "\n",
    "### 1. 量子化設定 (3.1)\n",
    "まず、モデルの「量子化」という処理を行っています。これは大きな言語モデルを、通常のパソコンでも扱えるサイズに圧縮する技術です。\n",
    "\n",
    "例えば：\n",
    "- 通常の言語モデルは高精度の数値（32ビット）を使用しますが\n",
    "- この設定では4ビットに圧縮して、メモリ使用量を大幅に削減します\n",
    "- ちょうど、高解像度の画像を圧縮して軽くするようなイメージです\n",
    "\n",
    "### 2. モデルのロードと設定 (3.2)\n",
    "次に、ソクラテス式対話を行うためのベースとなるモデル（Gemma）をロードし、学習できるように設定しています：\n",
    "\n",
    "- `model = AutoModelForCausalLM.from_pretrained()`: 事前学習済みのGemmaモデルを読み込みます\n",
    "- `prepare_model_for_kbit_training()`: 圧縮したモデルを学習可能な状態にします\n",
    "- `gradient_checkpointing_enable()`: メモリを節約しながら学習を行う設定を有効にします\n",
    "- `check_requires_grad()`: すべてのパラメータが正しく学習できる状態かチェックします\n",
    "\n",
    "### 3. LoRA設定 (3.3)\n",
    "最後に、LoRA（Low-Rank Adaptation）という効率的な学習方法を設定しています：\n",
    "\n",
    "簡単に言うと：\n",
    "- 元のGemmaモデル全体を変更するのではなく\n",
    "- ソクラテス式の対話に必要な部分だけを効率的に調整できるようにする設定です\n",
    "\n",
    "具体例：\n",
    "```python\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                # 学習の細かさを調整（小さいほどメモリ効率が良い）\n",
    "    lora_alpha=16,      # 学習の強さを調整\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # 対話の質問応答に関係する部分を指定\n",
    "    lora_dropout=0.05,  # 過学習を防ぐための設定\n",
    ")\n",
    "```\n",
    "\n",
    "この設定により：\n",
    "1. 少ないメモリでも学習が可能になり\n",
    "2. 効率的にソクラテス式の対話スタイルを学習でき\n",
    "3. 元のモデルの基本的な言語能力を保ちながら\n",
    "4. 対話の特徴だけを効果的に調整することができます\n",
    "\n",
    "これは、ちょうど経験豊富な教師が「ソクラテス式問答法」というスキルを新しく学ぶようなイメージです。基本的な教授能力はそのままに、新しい教え方のスタイルだけを効率的に身につけていくような感じです。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "### 3.1 量子化設定（BitsAndBytes）\n",
    "# Optimize BitsAndBytesConfig settings\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_storage=torch.uint8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "この量子化設定について、ソクラテス式チャットボットの文脈で分かりやすく説明させていただきます。\n",
    "\n",
    "# 量子化設定の基本的な説明\n",
    "\n",
    "量子化設定とは、大きな言語モデルをより効率的に動かすための「圧縮技術」のようなものです。例えるなら、高解像度の写真を少し画質を落として容量を小さくするようなものです。\n",
    "\n",
    "```python\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # モデルを4ビットで読み込む\n",
    "    bnb_4bit_use_double_quant=True,  # 二重量子化を使用\n",
    "    bnb_4bit_quant_type=\"nf4\",  # 量子化タイプ\n",
    "    bnb_4bit_compute_dtype=torch.float16,  # 計算時の精度\n",
    "    bnb_4bit_quant_storage=torch.uint8,  # データの保存形式\n",
    ")\n",
    "```\n",
    "\n",
    "## 各設定項目の詳細説明\n",
    "\n",
    "1. `load_in_4bit=True`\n",
    "   - 通常、モデルのパラメータは32ビットや16ビットで保存されています\n",
    "   - これを4ビットに圧縮することで、メモリ使用量を大幅に削減します\n",
    "   - 例：ソクラテスボットの「質問の仕方」に関する知識を、少し精度を落としても十分使える形で保存\n",
    "\n",
    "2. `bnb_4bit_use_double_quant=True`\n",
    "   - さらなるメモリ節約のための二重圧縮を行います\n",
    "   - モデルの品質をなるべく保ちながら、より効率的にメモリを使用します\n",
    "   - 例：ソクラテスの「問答法」の微妙なニュアンスを保持しながら、メモリ使用量を抑える\n",
    "\n",
    "3. `bnb_4bit_quant_type=\"nf4\"`\n",
    "   - 特殊な4ビット量子化方式を使用します\n",
    "   - 言語モデルに特化した圧縮方式で、文章生成の品質を保つのに適しています\n",
    "   - 例：「なぜそう考えるのですか？」といった質問の微妙なニュアンスを保持する\n",
    "\n",
    "4. `bnb_4bit_compute_dtype=torch.float16`\n",
    "   - 実際の計算時は16ビットの精度で行います\n",
    "   - 圧縮しつつも、計算時は適度な精度を確保します\n",
    "   - 例：ユーザーの回答を分析する際の思考プロセスを正確に行える精度を確保\n",
    "\n",
    "5. `bnb_4bit_quant_storage=torch.uint8`\n",
    "   - データをメモリに保存する際の形式を指定します\n",
    "   - 符号なし8ビット整数として保存することで、効率的なメモリ使用を実現\n",
    "   - 例：ソクラテスボットの知識をコンパクトに保存\n",
    "\n",
    "## メリット\n",
    "\n",
    "1. メモリ使用量の大幅削減\n",
    "   - 通常の1/8程度のメモリで動作可能\n",
    "   - より多くのユーザーに同時にサービスを提供できる\n",
    "\n",
    "2. 処理速度の向上\n",
    "   - データ量が少なくなるため、応答が速くなる\n",
    "   - ソクラテスボットがより自然な会話のテンポで対話可能に\n",
    "\n",
    "3. コスト効率の向上\n",
    "   - より少ないコンピューティングリソースで運用可能\n",
    "   - サービスの運用コストを抑えられる\n",
    "\n",
    "このように、量子化設定は「高品質なソクラテス式対話」を「効率的に」実現するための重要な技術的基盤となっています。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "### 3.2 モデルロードと学習設定\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    token=os.environ[\"HUGGINGFACE_TOKEN\"],  \n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation='eager'\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model.config.use_cache = False\n",
    "model.gradient_checkpointing_enable()\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "# パラメータの勾配計算が有効になっているか確認\n",
    "def check_requires_grad(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if not param.requires_grad:\n",
    "            logging.warning(f\"Parameter {name} does not require gradients\")\n",
    "\n",
    "check_requires_grad(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "\n",
    "\n",
    "このモデルロードと学習設定について、ソクラテス式チャットボットの文脈で分かりやすく説明させていただきます。\n",
    "\n",
    "# モデルロードと学習設定の説明\n",
    "\n",
    "## 1. モデルの読み込み\n",
    "```python\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,  # \"google/gemma-2-2b-jpn-it\" を使用\n",
    "    token=os.environ[\"HUGGINGFACE_TOKEN\"],  # HuggingFaceのアクセストークン\n",
    "    quantization_config=bnb_config,  # 先ほどの量子化設定を適用\n",
    "    device_map=\"auto\",  # GPU/CPUの自動割り当て\n",
    "    torch_dtype=torch.bfloat16,  # 16ビットの特殊な形式で計算\n",
    "    attn_implementation='eager'  # 注意機構の実装方式\n",
    ")\n",
    "```\n",
    "\n",
    "これは、ベースとなる言語モデルを読み込む部分です。例えるなら：\n",
    "- 「新入社員のソクラテス」に、基本的な日本語能力と対話能力を持たせる段階\n",
    "- `model_name`で指定したモデルが「素のソクラテス」で、これから「問答法の達人」に育てていく\n",
    "\n",
    "## 2. 学習準備\n",
    "```python\n",
    "model = prepare_model_for_kbit_training(model)  # 量子化モデルの学習準備\n",
    "model.config.use_cache = False  # キャッシュを無効化\n",
    "model.gradient_checkpointing_enable()  # メモリ効率を改善\n",
    "model.enable_input_require_grads()  # 勾配計算を有効化\n",
    "```\n",
    "\n",
    "これらの設定は、モデルを効率的に学習させるための準備です：\n",
    "\n",
    "1. `prepare_model_for_kbit_training`\n",
    "   - 量子化したモデルを学習できるように準備\n",
    "   - 例：ソクラテスに「新しい問答法を学ぶ能力」を与える\n",
    "\n",
    "2. `use_cache = False`\n",
    "   - 学習中のメモリ使用を抑える\n",
    "   - 例：ソクラテスが新しい対話テクニックを学ぶときに、余計な情報を一時的に忘れる\n",
    "\n",
    "3. `gradient_checkpointing_enable()`\n",
    "   - メモリを節約しながら学習を行う技術\n",
    "   - 例：ソクラテスが大量の対話例から学ぶとき、少しずつ消化して学習する\n",
    "\n",
    "4. `enable_input_require_grads()`\n",
    "   - モデルが学習できる状態にする\n",
    "   - 例：ソクラテスに「学習する準備ができました」と宣言させる\n",
    "\n",
    "## 3. 学習状態の確認\n",
    "```python\n",
    "def check_requires_grad(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if not param.requires_grad:\n",
    "            logging.warning(f\"Parameter {name} does not require gradients\")\n",
    "\n",
    "check_requires_grad(model)\n",
    "```\n",
    "\n",
    "これは学習の準備が正しくできているか確認する部分です：\n",
    "- 全てのパラメータが学習可能な状態になっているか確認\n",
    "- 例えるなら、ソクラテスの「学習意欲」をチェックするようなもの\n",
    "- もし学習できないパラメータがあれば警告を出す\n",
    "\n",
    "## この設定の重要性\n",
    "\n",
    "1. **効率的な学習**\n",
    "   - 限られたコンピュータリソースで効率的に学習\n",
    "   - より多くの対話例から学べる\n",
    "\n",
    "2. **安定性の確保**\n",
    "   - メモリ管理を最適化\n",
    "   - クラッシュを防ぎながら学習を進める\n",
    "\n",
    "3. **品質の維持**\n",
    "   - 16ビットの精度で計算することで、\n",
    "   - ソクラテスの対話の質を保ちながら学習できる\n",
    "\n",
    "このように、これらの設定は「効率的」かつ「安定的」に、ソクラテス式の対話能力を向上させるための重要な基盤となっています。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "### 3.3 LoRA設定とモデル変換\n",
    "# Adjust LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                # 16から8に減少\n",
    "    lora_alpha=16,      # 32から16に減少\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,  # 0.1から0.05に減少\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Create LoRA model\n",
    "model = get_peft_model(model, lora_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "\n",
    "\n",
    "LoRA（Low-Rank Adaptation）設定について、ソクラテス式チャットボットの文脈で分かりやすく説明させていただきます。\n",
    "\n",
    "# LoRA設定の基本的な説明\n",
    "\n",
    "LoRAとは、大きな言語モデルを効率的に微調整（ファインチューニング）するための技術です。例えるなら：\n",
    "- 「ベースとなるソクラテス」に対して\n",
    "- 「問答法の専門家」としての特徴を\n",
    "- 少ないパラメータで効率的に追加する方法\n",
    "\n",
    "```python\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                # ランク（学習パラメータの数）\n",
    "    lora_alpha=16,      # スケーリング係数\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # 調整対象\n",
    "    lora_dropout=0.05,  # ドロップアウト率\n",
    "    bias=\"none\",        # バイアスの扱い\n",
    "    task_type=\"CAUSAL_LM\",  # タスクタイプ\n",
    ")\n",
    "```\n",
    "\n",
    "## 各パラメータの詳細説明\n",
    "\n",
    "1. `r=8`（ランク）\n",
    "   - 以前は16だったものを8に減少\n",
    "   - より少ないパラメータで学習を行う\n",
    "   - 例：ソクラテスが新しい問答テクニックを学ぶときの「メモの取り方」を簡略化\n",
    "   - メリット：学習が速くなり、メモリ使用量も減少\n",
    "   - デメリット：あまり小さくしすぎると学習能力が低下\n",
    "\n",
    "2. `lora_alpha=16`（スケーリング係数）\n",
    "   - 以前は32だったものを16に減少\n",
    "   - 学習時の変更の強さを調整\n",
    "   - 例：ソクラテスが新しい知識を取り入れる際の「積極性」\n",
    "   - 大きいほど：大胆に新しい対話スタイルを学ぶ\n",
    "   - 小さいほど：慎重に既存の知識を活かす\n",
    "\n",
    "3. `target_modules`（調整対象）\n",
    "   ```python\n",
    "   target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "   ```\n",
    "   - モデルの注意機構（Attention）の主要な部分を調整\n",
    "   - 例：ソクラテスの「質問力」に関わる部分\n",
    "     - `q_proj`: 質問の仕方\n",
    "     - `k_proj`: 重要ポイントの把握\n",
    "     - `v_proj`: 回答の解釈\n",
    "     - `o_proj`: 次の質問の組み立て\n",
    "\n",
    "4. `lora_dropout=0.05`（ドロップアウト率）\n",
    "   - 以前は0.1だったものを0.05に減少\n",
    "   - 過学習を防ぐための設定\n",
    "   - 例：ソクラテスが「時々意図的に情報を忘れる」ことで\n",
    "   - 特定の対話パターンに過度に固執するのを防ぐ\n",
    "\n",
    "5. その他の設定\n",
    "   ```python\n",
    "   bias=\"none\"        # バイアスパラメータは更新しない\n",
    "   task_type=\"CAUSAL_LM\"  # 文章生成タスクであることを指定\n",
    "   ```\n",
    "\n",
    "## LoRAモデルの作成\n",
    "```python\n",
    "model = get_peft_model(model, lora_config)\n",
    "```\n",
    "- 設定した内容でモデルを変換\n",
    "- 例：ベースのソクラテスに「問答法の専門家」としての機能を追加\n",
    "\n",
    "## この設定の利点\n",
    "\n",
    "1. **効率的な学習**\n",
    "   - 元のモデルの大部分はそのまま保持\n",
    "   - 必要な部分だけを効率的に調整\n",
    "   - 結果：学習時間とメモリ使用量を大幅に削減\n",
    "\n",
    "2. **柔軟な調整**\n",
    "   - パラメータを変更することで学習の特性を制御可能\n",
    "   - 例：より丁寧な質問、より深い掘り下げなど\n",
    "\n",
    "3. **安定性の向上**\n",
    "   - 以前の設定（r=16, alpha=32, dropout=0.1）から\n",
    "   - より控えめな値（r=8, alpha=16, dropout=0.05）に調整\n",
    "   - 結果：より安定した学習が期待できる\n",
    "\n",
    "このように、LoRA設定は「効率的」かつ「効果的」に、ソクラテス式チャットボットの特徴を強化するための重要な技術となっています。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
