{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. モデル設定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "このセクションについて、ソクラテス式チャットボットの文脈で分かりやすく説明します。\n",
    "\n",
    "### 1. 量子化設定 (3.1)\n",
    "これは大きな言語モデルを小さく圧縮して、一般的なPCでも動かせるようにする設定です。\n",
    "\n",
    "例えば：\n",
    "- 元のモデルは「人間の知識とは何か？」という質問に対して複雑な応答を生成できますが、そのままだとメモリを大量に使います\n",
    "- 量子化により、応答の品質をほぼ維持したまま、メモリ使用量を約1/4に削減できます\n",
    "- `load_in_4bit=True`は、32ビットの精度を4ビットに圧縮することを意味します\n",
    "\n",
    "### 2. モデルのロードと初期化 (3.2)\n",
    "ベースとなる言語モデル（この場合はGemma）をロードし、複数のGPUに効率的に配置する設定です。\n",
    "\n",
    "例えば：\n",
    "- `device_map=\"balanced\"`は、モデルを2つのGPUに均等に分散させます\n",
    "- `max_memory`で各GPUに4GB、CPUに24GBのメモリを割り当てています\n",
    "- これにより、「なぜそう考えるのですか？」「その根拠は何でしょうか？」といったソクラテス式の対話を、限られたリソースで実現できます\n",
    "\n",
    "### 3. LoRA設定 (3.3)\n",
    "効率的な学習のための設定です。モデル全体ではなく、重要な部分だけを調整することで、少ないリソースで効果的な学習を可能にします。\n",
    "\n",
    "具体例：\n",
    "- ベースモデルは一般的な対話ができますが、ソクラテス式の対話スタイルは身についていません\n",
    "- LoRAを使うと、以下のような特徴を効率的に学習できます：\n",
    "  - 「なるほど、それは興味深い視点ですね」\n",
    "  - 「その考えをもう少し掘り下げてみましょうか？」\n",
    "  - 「その結論に至った過程を説明していただけますか？」\n",
    "- `target_modules`は、質問の生成や応答の形成に関わる重要な部分を指定しています\n",
    "- `r=16`と`lora_alpha=32`は、どの程度の細かさで対話スタイルを調整するかを決める値です\n",
    "\n",
    "このように、限られたコンピュータリソースで、効率的にソクラテス式の対話スタイルを学習できる環境を整えているのです。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "### 3.1 量子化設定（BitsAndBytes）\n",
    "# Optimize BitsAndBytesConfig settings\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_storage=torch.uint8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "はい、量子化設定について分かりやすく説明させていただきます。\n",
    "\n",
    "# 量子化設定（BitsAndBytes）の解説\n",
    "\n",
    "## 基本的な概念\n",
    "\n",
    "量子化とは、大きな言語モデルをより少ないメモリで動かすための技術です。例えば、通常32ビットで表現される数値を4ビットで表現することで、メモリ使用量を大幅に削減できます。\n",
    "\n",
    "これは、例えるなら以下のようなものです：\n",
    "- 32ビット：0から42億までの数値を表現できる（精密な表現）\n",
    "- 4ビット：0から15までの数値しか表現できない（簡略化された表現）\n",
    "\n",
    "## 設定の詳細解説\n",
    "\n",
    "```python\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # 4ビット量子化を使用\n",
    "    bnb_4bit_use_double_quant=True,  # 二重量子化を有効化\n",
    "    bnb_4bit_quant_type=\"nf4\",  # 量子化タイプをnf4に設定\n",
    "    bnb_4bit_compute_dtype=torch.float16,  # 計算時は16ビットを使用\n",
    "    bnb_4bit_quant_storage=torch.uint8,  # データ保存は8ビットで\n",
    ")\n",
    "```\n",
    "\n",
    "### 具体例で理解する\n",
    "\n",
    "ソクラテス式チャットボットの場合を例に説明します：\n",
    "\n",
    "1. `load_in_4bit=True`\n",
    "   - 効果：モデルのメモリ使用量を約8分の1に削減\n",
    "   - 例：「人生の意味とは何でしょうか？」という質問に対する回答を生成する際、より少ないメモリで処理できる\n",
    "\n",
    "2. `bnb_4bit_use_double_quant=True`\n",
    "   - 効果：さらにメモリ使用量を削減（約10-15%追加削減）\n",
    "   - 例：複数の哲学的な質問に同時に対応する際のメモリ効率が向上\n",
    "\n",
    "3. `bnb_4bit_quant_type=\"nf4\"`\n",
    "   - 効果：特に言語モデル用に最適化された4ビット形式を使用\n",
    "   - 例：「なぜそう考えるのですか？」といった問いかけの微妙なニュアンスをより正確に保持\n",
    "\n",
    "4. `bnb_4bit_compute_dtype=torch.float16`\n",
    "   - 効果：計算時は16ビットを使用してより正確な演算を実現\n",
    "   - 例：対話の文脈を理解し、適切な反応を生成する際の精度を確保\n",
    "\n",
    "5. `bnb_4bit_quant_storage=torch.uint8`\n",
    "   - 効果：モデルのパラメータを8ビットで保存\n",
    "   - 例：学習したソクラテス式の対話パターンをより効率的に保存\n",
    "\n",
    "## メリット\n",
    "\n",
    "1. メモリ効率：\n",
    "   - 通常なら24GBのGPUメモリが必要なモデルを4GB程度で動作可能に\n",
    "   - より多くの対話履歴を保持できる\n",
    "\n",
    "2. 処理速度：\n",
    "   - より軽量になることで応答速度が向上\n",
    "   - ソクラテス式の問答をよりスムーズに行える\n",
    "\n",
    "3. コスト効率：\n",
    "   - より少ないコンピューティングリソースで動作\n",
    "   - 学習や推論のコストを削減\n",
    "\n",
    "このように、量子化設定によって、高性能な言語モデルを効率的に動作させることができ、ソクラテス式チャットボットのような複雑な対話システムを、より少ないリソースで実現することが可能になります。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "### 3.2 モデルロードと初期化\n",
    "# Load model with modifications\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    token=os.environ[\"HUGGINGFACE_TOKEN\"],  \n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"balanced\",\n",
    "    torch_dtype=torch.float16,\n",
    "    attn_implementation='sdpa',\n",
    "    max_memory={0: \"4GiB\", 1: \"4GiB\", \"cpu\": \"24GB\"}\n",
    ")\n",
    "\n",
    "# Prepare model for LoRA and disable cache\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "はい、モデルのロードと初期化部分について説明させていただきます。\n",
    "\n",
    "# モデルロードと初期化の解説\n",
    "\n",
    "## 基本的なモデルのロード\n",
    "```python\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,  # \"google/gemma-2-2b-jpn-it\" を使用\n",
    "    token=os.environ[\"HUGGINGFACE_TOKEN\"],  # Hugging Faceのアクセストークン\n",
    "    quantization_config=bnb_config,  # 先ほどの量子化設定を適用\n",
    "    device_map=\"balanced\",  # GPU間でバランスよく配置\n",
    "    torch_dtype=torch.float16,  # 16ビットの浮動小数点を使用\n",
    "    attn_implementation='sdpa',  # 注意機構の実装方式\n",
    "    max_memory={0: \"4GiB\", 1: \"4GiB\", \"cpu\": \"24GB\"}  # メモリ制限の設定\n",
    ")\n",
    "```\n",
    "\n",
    "### パラメータの詳細説明\n",
    "\n",
    "1. `model_name`\n",
    "   - 使用するモデル：Google の Gemma 2.0（日本語・IT特化版）\n",
    "   - ソクラテス式対話に適した基礎モデルとして選択\n",
    "\n",
    "2. `token`\n",
    "   - Hugging Face の認証トークン\n",
    "   - モデルをダウンロードする権限の確認用\n",
    "\n",
    "3. `device_map=\"balanced\"`\n",
    "   - 複数のGPUにモデルを分散配置\n",
    "   - 例：ソクラテス式の対話生成時の処理負荷を2つのGPUで分散\n",
    "\n",
    "4. `torch_dtype=torch.float16`\n",
    "   - 16ビットの精度でモデルを動作\n",
    "   - メモリ使用量を抑えつつ、対話の質を維持\n",
    "\n",
    "5. `attn_implementation='sdpa'`\n",
    "   - Scaled Dot-Product Attention の実装を使用\n",
    "   - より効率的な注意機構の計算が可能\n",
    "\n",
    "6. `max_memory`\n",
    "   - メモリ使用量の制限設定\n",
    "   ```python\n",
    "   {\n",
    "       0: \"4GiB\",     # 1つ目のGPUに4GB\n",
    "       1: \"4GiB\",     # 2つ目のGPUに4GB\n",
    "       \"cpu\": \"24GB\"  # CPUに24GB\n",
    "   }\n",
    "   ```\n",
    "\n",
    "## モデルの前処理\n",
    "```python\n",
    "# LoRA学習の準備\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "# キャッシュを無効化\n",
    "model.config.use_cache = False\n",
    "```\n",
    "\n",
    "### 前処理の説明\n",
    "\n",
    "1. `prepare_model_for_kbit_training()`\n",
    "   - 量子化されたモデルをLoRA（Low-Rank Adaptation）学習用に準備\n",
    "   - ソクラテス式の対話スタイルを効率的に学習できるよう調整\n",
    "\n",
    "2. `use_cache = False`\n",
    "   - モデルのキャッシュ機能を無効化\n",
    "   - メモリを節約し、より多くの対話データを処理可能に\n",
    "\n",
    "## この設定の利点\n",
    "\n",
    "1. **効率的なリソース使用**\n",
    "   - 2つのGPUでバランスよく処理を分散\n",
    "   - メモリ使用量を適切に制限\n",
    "\n",
    "2. **学習の最適化**\n",
    "   - LoRA学習に適した状態にモデルを準備\n",
    "   - ソクラテス式の対話パターンを効率的に学習可能\n",
    "\n",
    "3. **安定性の向上**\n",
    "   - キャッシュを無効化することで安定した学習を実現\n",
    "   - メモリ不足によるクラッシュを防止\n",
    "\n",
    "このように、モデルのロードと初期化の設定は、ソクラテス式チャットボットの学習を効率的かつ安定的に行うための重要な基盤となっています。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "### 3.3 LoRA設定\n",
    "# Adjust LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Create LoRA model\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "\n",
    "\n",
    "はい、LoRA（Low-Rank Adaptation）設定について説明させていただきます。\n",
    "\n",
    "# LoRA設定の解説\n",
    "\n",
    "## LoRAとは？\n",
    "LoRAは、大規模な言語モデルを効率的に微調整（ファインチューニング）するための手法です。モデル全体ではなく、重要な部分だけを調整することで、少ないメモリと計算資源で効果的な学習が可能になります。\n",
    "\n",
    "## 設定の詳細\n",
    "```python\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                    # ランク分解の次元数\n",
    "    lora_alpha=32,           # スケーリングパラメータ\n",
    "    target_modules=[         # 調整対象のモジュール\n",
    "        \"q_proj\",           # Query投影\n",
    "        \"k_proj\",           # Key投影\n",
    "        \"v_proj\",           # Value投影\n",
    "        \"o_proj\"            # 出力投影\n",
    "    ],\n",
    "    lora_dropout=0.1,        # ドロップアウト率\n",
    "    bias=\"none\",             # バイアスの調整方法\n",
    "    task_type=\"CAUSAL_LM\",   # タスクタイプ\n",
    ")\n",
    "```\n",
    "\n",
    "### パラメータの詳細説明\n",
    "\n",
    "1. `r=16`\n",
    "   - ランク分解の次元数を16に設定\n",
    "   - 例：ソクラテス式の「問いかけ」パターンを16次元の特徴空間で表現\n",
    "   - 大きすぎると学習が重くなり、小さすぎると表現力が不足\n",
    "\n",
    "2. `lora_alpha=32`\n",
    "   - スケーリング係数\n",
    "   - 例：「なぜそう考えるのですか？」といった質問の強さを調整\n",
    "   - 値が大きいほど元のモデルからの変更が大きくなる\n",
    "\n",
    "3. `target_modules`\n",
    "   - 調整対象となる注意機構の層\n",
    "   ```python\n",
    "   [\n",
    "       \"q_proj\",  # 質問の仕方を学習\n",
    "       \"k_proj\",  # 重要なキーワードの認識\n",
    "       \"v_proj\",  # 文脈の理解\n",
    "       \"o_proj\"   # 最終的な出力の調整\n",
    "   ]\n",
    "   ```\n",
    "\n",
    "4. `lora_dropout=0.1`\n",
    "   - 10%のニューロンをランダムに無効化\n",
    "   - 過学習を防ぎ、より汎用的なソクラテス式対話を実現\n",
    "\n",
    "5. `bias=\"none\"`\n",
    "   - バイアス項は調整しない設定\n",
    "   - モデルの基本的な応答傾向を維持\n",
    "\n",
    "6. `task_type=\"CAUSAL_LM\"`\n",
    "   - 因果的言語モデルとして設定\n",
    "   - 対話の文脈を考慮した応答生成が可能\n",
    "\n",
    "## LoRAモデルの作成\n",
    "```python\n",
    "model = get_peft_model(model, lora_config)\n",
    "```\n",
    "- 設定したLoRA構成をモデルに適用\n",
    "- 効率的な学習が可能な状態に変換\n",
    "\n",
    "## この設定のメリット\n",
    "\n",
    "1. **メモリ効率**\n",
    "   - 通常の微調整と比べて約95%のメモリ削減\n",
    "   - 例：32GBのモデルを2GB程度のメモリで調整可能\n",
    "\n",
    "2. **学習速度**\n",
    "   - 限られたパラメータのみを調整\n",
    "   - ソクラテス式の対話スタイルを素早く学習\n",
    "\n",
    "3. **柔軟性**\n",
    "   - 元のモデルの知識を保持しつつ\n",
    "   - ソクラテス式の対話パターンを追加学習\n",
    "\n",
    "4. **安定性**\n",
    "   - ドロップアウトによる過学習防止\n",
    "   - 一貫した対話スタイルの維持\n",
    "\n",
    "このように、LoRA設定により、効率的かつ効果的にソクラテス式チャットボットの対話スタイルを学習することができます。元のモデルの基本的な言語理解力を保持しながら、特定の対話パターンを追加できる点が大きな特徴です。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
