{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. カスタムトレーナーとトレーニング実行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "このコードセクションについて、ソクラテス式チャットボットの学習を例に説明します。\n",
    "\n",
    "### 1. カスタムトレーナーの定義\n",
    "\n",
    "```python\n",
    "class CustomTrainer(Trainer):\n",
    "    def training_step(self, *args, **kwargs):\n",
    "        loss = super().training_step(*args, **kwargs)\n",
    "        if self.state.global_step % 50 == 0:\n",
    "            clear_memory()\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "        return loss\n",
    "```\n",
    "\n",
    "これは「先生」（モデル）の学習過程を管理する特別なクラスです。50回の学習ステップごとにメモリを掃除して、コンピュータのリソースを効率的に使えるようにします。\n",
    "\n",
    "例えば：\n",
    "- モデルが「なぜそう考えるのですか？」という問いかけ方を50回練習したら\n",
    "- 使い終わったメモリを掃除して、次の50回の練習に備えます\n",
    "\n",
    "### 2. 評価用のカスタムトレーナー\n",
    "\n",
    "```python\n",
    "class CustomTrainer(Trainer):\n",
    "    def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix=\"eval\"):\n",
    "        eval_dataset = eval_dataset if eval_dataset is not None else self.eval_dataset\n",
    "        if eval_dataset is not None:\n",
    "            eval_dataset = eval_dataset.select(range(min(100, len(eval_dataset))))\n",
    "        return super().evaluate(eval_dataset, ignore_keys, metric_key_prefix)\n",
    "```\n",
    "\n",
    "これは「先生」の成長具合を確認するためのクラスです。効率化のため、最大100個の会話例で評価を行います。\n",
    "\n",
    "例えば：\n",
    "- 全部で1000個の会話例があっても\n",
    "- 最初の100個だけを使って「そうですね、では次の質問について考えてみましょう」といった返答が適切にできているか確認します\n",
    "\n",
    "### 3. トレーニングの実行と管理\n",
    "\n",
    "```python\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[StyleCallback(), TrainingMonitorCallback()],\n",
    ")\n",
    "```\n",
    "\n",
    "これは実際の学習プロセスを開始する部分です。必要な要素をすべて組み合わせて：\n",
    "- モデル（「先生」の基本的な能力）\n",
    "- 学習データ（ソクラテス式の会話例）\n",
    "- 評価方法（どれだけソクラテス式の対話ができているか）\n",
    "- 進捗監視（学習の進み具合をチェック）\n",
    "\n",
    "### 4. チェックポイントの管理\n",
    "\n",
    "コードの残りの部分は、学習の途中経過を保存・管理する仕組みです：\n",
    "- 途中で中断しても再開できるように定期的に保存\n",
    "- すでに完了している場合は誤って上書きしないように保護\n",
    "- 学習設定や結果を記録して後で確認できるように保存\n",
    "\n",
    "例えば：\n",
    "- 10時間の学習予定で5時間経過した時点で保存\n",
    "- 停電などで中断しても、5時間目から再開可能\n",
    "- 完了後は「なぜですか？」「それはどういう意味でしょうか？」といった問いかけ方を学んだモデルを安全に保存\n",
    "\n",
    "これらの機能により、長時間かかる学習プロセスを安全かつ効率的に管理できます。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def training_step(self, *args, **kwargs):\n",
    "        loss = super().training_step(*args, **kwargs)\n",
    "        if self.state.global_step % 50 == 0:\n",
    "            clear_memory()\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "このコードについて、ソクラテス式チャットボットのコンテキストで説明させていただきます。\n",
    "\n",
    "```python\n",
    "class CustomTrainer(Trainer):\n",
    "    def training_step(self, *args, **kwargs):\n",
    "        loss = super().training_step(*args, **kwargs)\n",
    "        if self.state.global_step % 50 == 0:\n",
    "            clear_memory()\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "        return loss\n",
    "```\n",
    "\n",
    "このコードは、モデルのトレーニングプロセスをカスタマイズするためのものです。以下のポイントで説明します：\n",
    "\n",
    "1. **基本的な役割**:\n",
    "   - このカスタムトレーナーは、通常のトレーニング処理に「メモリ管理」という機能を追加しています\n",
    "   - これは特に、大量のデータでソクラテス式の対話を学習させる際に重要です\n",
    "\n",
    "2. **メモリ管理が必要な理由**:\n",
    "   - ソクラテス式の対話は長文になりがちです（例：「なぜそう考えるのですか？」「その考えの根拠は何でしょうか？」といった問いかけの連続）\n",
    "   - このような長い対話をトレーニングすると、GPUメモリを大量に使用します\n",
    "   - メモリが溢れると学習が途中で止まってしまう可能性があります\n",
    "\n",
    "3. **具体的な動作**:\n",
    "   ```python\n",
    "   if self.state.global_step % 50 == 0:\n",
    "   ```\n",
    "   - 50ステップごとに（つまり50回の学習更新ごとに）メモリクリーンアップを実行します\n",
    "   - 例えば：\n",
    "     - 1-49ステップ目：通常通り学習を進める\n",
    "     - 50ステップ目：メモリクリーンアップを実行\n",
    "     - 51-99ステップ目：通常通り学習を進める\n",
    "     - 100ステップ目：再びメモリクリーンアップを実行\n",
    "     - という具合です\n",
    "\n",
    "4. **メモリクリーンアップの方法**:\n",
    "   ```python\n",
    "   clear_memory()\n",
    "   gc.collect()\n",
    "   torch.cuda.empty_cache()\n",
    "   ```\n",
    "   - `clear_memory()`：不要なメモリを解放\n",
    "   - `gc.collect()`：Pythonのガベージコレクションを実行\n",
    "   - `torch.cuda.empty_cache()`：GPUの一時メモリをクリア\n",
    "\n",
    "5. **実際の効果の例**:\n",
    "   - メモリクリーンアップなし：\n",
    "     ```\n",
    "     「哲学とは何でしょうか？」\n",
    "     「その定義の根拠は？」\n",
    "     「なるほど、では具体例を挙げてみましょうか」\n",
    "     ※ここでメモリ不足でエラー\n",
    "     ```\n",
    "\n",
    "   - メモリクリーンアップあり：\n",
    "     ```\n",
    "     「哲学とは何でしょうか？」\n",
    "     「その定義の根拠は？」\n",
    "     「なるほど、では具体例を挙げてみましょうか」\n",
    "     「その例から何が言えるでしょうか？」\n",
    "     「さらに深く考えてみましょう」\n",
    "     ※安定して長い対話の学習が可能\n",
    "     ```\n",
    "\n",
    "このカスタマイズにより、長時間の学習を安定して行うことができ、ソクラテス式の対話パターンをより効果的に学習させることが可能になります。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create custom Trainer class for evaluation\n",
    "class CustomTrainer(Trainer):\n",
    "    def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix=\"eval\"):\n",
    "        eval_dataset = eval_dataset if eval_dataset is not None else self.eval_dataset\n",
    "        if eval_dataset is not None:\n",
    "            # Limit evaluation dataset to 100 samples\n",
    "            eval_dataset = eval_dataset.select(range(min(100, len(eval_dataset))))\n",
    "        return super().evaluate(eval_dataset, ignore_keys, metric_key_prefix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "このコードについて、ソクラテス式チャットボットのコンテキストで説明させていただきます。\n",
    "\n",
    "```python\n",
    "class CustomTrainer(Trainer):\n",
    "    def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix=\"eval\"):\n",
    "        eval_dataset = eval_dataset if eval_dataset is not None else self.eval_dataset\n",
    "        if eval_dataset is not None:\n",
    "            # Limit evaluation dataset to 100 samples\n",
    "            eval_dataset = eval_dataset.select(range(min(100, len(eval_dataset))))\n",
    "        return super().evaluate(eval_dataset, ignore_keys, metric_key_prefix)\n",
    "```\n",
    "\n",
    "このコードは、モデルの評価（検証）プロセスをカスタマイズするためのものです。以下のポイントで説明します：\n",
    "\n",
    "1. **基本的な役割**:\n",
    "   - このカスタムトレーナーは、評価時のデータセットサイズを制限する機能を追加しています\n",
    "   - 評価用データを最大100サンプルに制限することで、評価プロセスを効率化します\n",
    "\n",
    "2. **なぜ評価データを制限するのか**:\n",
    "   例えば以下のような対話データがあるとします：\n",
    "   ```\n",
    "   ユーザー: 「正義とは何でしょうか？」\n",
    "   モデル: 「興味深い質問ですね。あなたにとって正義とは何でしょうか？」\n",
    "   ユーザー: 「弱者を守ることだと思います」\n",
    "   モデル: 「なぜそのように考えるのでしょうか？具体例を挙げて説明していただけますか？」\n",
    "   ```\n",
    "   - このような対話を数千件評価するのは時間がかかります\n",
    "   - 実際には100件程度でも十分な評価が可能です\n",
    "   - 評価時間を短縮することで、より頻繁に評価を行えます\n",
    "\n",
    "3. **コードの詳細説明**:\n",
    "   ```python\n",
    "   eval_dataset = eval_dataset if eval_dataset is not None else self.eval_dataset\n",
    "   ```\n",
    "   - 評価用データセットが指定されていない場合は、トレーナーが持っているデータセットを使用します\n",
    "\n",
    "   ```python\n",
    "   if eval_dataset is not None:\n",
    "       eval_dataset = eval_dataset.select(range(min(100, len(eval_dataset))))\n",
    "   ```\n",
    "   - データセットが100件より多い場合、最初の100件だけを選択します\n",
    "   - データセットが100件未満の場合は、全件を使用します\n",
    "\n",
    "4. **実際の効果の例**:\n",
    "   - 制限なしの場合：\n",
    "     ```\n",
    "     評価データ: 1000件の対話\n",
    "     評価時間: 約60分\n",
    "     メモリ使用量: 大\n",
    "     ```\n",
    "\n",
    "   - 制限ありの場合：\n",
    "     ```\n",
    "     評価データ: 100件の対話\n",
    "     評価時間: 約6分\n",
    "     メモリ使用量: 適度\n",
    "     ```\n",
    "\n",
    "5. **具体的なメリット**:\n",
    "   - より頻繁な評価が可能になります\n",
    "     ```\n",
    "     例：ソクラテス式の質問の質を評価\n",
    "     - 「なぜそう考えるのですか？」\n",
    "     - 「その考えの根拠は何でしょうか？」\n",
    "     - 「具体例を挙げて説明できますか？」\n",
    "     このような質問パターンが適切に学習できているか、\n",
    "     短時間で確認できます\n",
    "     ```\n",
    "\n",
    "   - リソースの効率的な使用\n",
    "     - 評価に使用するGPUメモリを節約できます\n",
    "     - 他の処理（学習など）により多くのリソースを割り当てられます\n",
    "\n",
    "このカスタマイズにより、モデルの評価プロセスが効率化され、より頻繁にモデルの質を確認できるようになります。これは特に、ソクラテス式の対話の質を維持・向上させる上で重要です。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Update trainer settings\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[StyleCallback(), TrainingMonitorCallback()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "\n",
    "\n",
    "このコードについて、ソクラテス式チャットボットのコンテキストで説明させていただきます。\n",
    "\n",
    "```python\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[StyleCallback(), TrainingMonitorCallback()],\n",
    ")\n",
    "```\n",
    "\n",
    "これは、トレーニングの実行に必要な全ての要素を設定するコードです。各パラメータを詳しく説明します：\n",
    "\n",
    "1. **model=model**:\n",
    "   - 学習対象のモデル（この場合はGemma-2b-jpn）を指定します\n",
    "   - 例：ソクラテス式の対話を学習させる基本となるモデル\n",
    "   ```python\n",
    "   # モデルの例\n",
    "   ユーザー: 「幸せとは何でしょうか？」\n",
    "   モデル: 「その質問について、あなたはどのようにお考えですか？」\n",
    "   ```\n",
    "\n",
    "2. **args=training_args**:\n",
    "   - 学習の設定パラメータを指定します\n",
    "   - 主な設定例：\n",
    "     ```python\n",
    "     - 学習回数: 30エポック\n",
    "     - 学習率: 8e-5\n",
    "     - バッチサイズ: 2\n",
    "     - 評価頻度: 20ステップごと\n",
    "     ```\n",
    "\n",
    "3. **train_dataset=train_dataset**:\n",
    "   - 学習用のデータセット\n",
    "   - 例：ソクラテス式の対話データ（全体の80%）\n",
    "   ```python\n",
    "   {\n",
    "       \"messages\": [\n",
    "           {\"role\": \"user\", \"content\": \"正義とは何でしょうか？\"},\n",
    "           {\"role\": \"model\", \"content\": \"興味深い質問ですね。あなたは正義とは何だとお考えですか？\"},\n",
    "           {\"role\": \"user\", \"content\": \"弱者を守ることだと思います\"},\n",
    "           {\"role\": \"model\", \"content\": \"なぜそのようにお考えなのでしょうか？具体例を挙げて説明していただけますか？\"}\n",
    "       ]\n",
    "   }\n",
    "   ```\n",
    "\n",
    "4. **eval_dataset=eval_dataset**:\n",
    "   - 評価用のデータセット（全体の20%）\n",
    "   - モデルの性能を定期的に評価するために使用\n",
    "\n",
    "5. **data_collator=data_collator**:\n",
    "   - データの前処理を行うツール\n",
    "   - 例：対話データをモデルが理解できる形式に変換\n",
    "   ```python\n",
    "   入力: 「なぜそう考えるのですか？」\n",
    "   変換後: [1234, 5678, 9012, ...]  # トークン化されたデータ\n",
    "   ```\n",
    "\n",
    "6. **compute_metrics=compute_metrics**:\n",
    "   - モデルの評価指標を計算する関数\n",
    "   - ソクラテス式対話の評価例：\n",
    "     ```python\n",
    "     - スタイル一貫性スコア（問いかけの適切さ）\n",
    "     - 対話の流れスコア（論理的な展開）\n",
    "     - 総合スコア\n",
    "     ```\n",
    "\n",
    "7. **callbacks=[StyleCallback(), TrainingMonitorCallback()]**:\n",
    "   - トレーニング中の監視と記録を行う機能\n",
    "   - StyleCallback:\n",
    "     ```python\n",
    "     # 対話スタイルの評価例\n",
    "     - 「なぜ」「どのように」などの問いかけの使用頻度\n",
    "     - 文末表現の一貫性（「〜でしょうか？」「〜と考えられますか？」）\n",
    "     ```\n",
    "   - TrainingMonitorCallback:\n",
    "     ```python\n",
    "     # モニタリング項目例\n",
    "     - GPUメモリ使用量\n",
    "     - 学習の進捗状況\n",
    "     - エラーの検知と記録\n",
    "     ```\n",
    "\n",
    "このトレーナー設定により：\n",
    "1. モデルは効率的にソクラテス式の対話パターンを学習できます\n",
    "2. 学習の進捗を詳細に監視できます\n",
    "3. 対話の質を定量的に評価できます\n",
    "\n",
    "例えば、以下のような学習の進捗が確認できます：\n",
    "```\n",
    "ステップ 100:\n",
    "- 損失値: 2.34\n",
    "- スタイル一貫性: 0.85\n",
    "- 対話の流れ: 0.78\n",
    "- GPUメモリ使用率: 65%\n",
    "\n",
    "ステップ 200:\n",
    "- 損失値: 1.98\n",
    "- スタイル一貫性: 0.89\n",
    "- 対話の流れ: 0.82\n",
    "- GPUメモリ使用率: 67%\n",
    "```\n",
    "\n",
    "これにより、モデルが適切にソクラテス式の対話スタイルを学習できているかを継続的に確認できます。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Start training\n",
    "logging.info(\"Starting training...\")\n",
    "try:\n",
    "    checkpoint_dir = MODEL_OUTPUT_DIR  \n",
    "    resume_from_checkpoint = None\n",
    "    \n",
    "    # Check if running in Kaggle environment\n",
    "    is_kaggle = os.path.exists('/kaggle/working')\n",
    "    \n",
    "    # Checkpoint status and processing\n",
    "    if os.path.exists(checkpoint_dir):\n",
    "        print(\"\\nChecking checkpoint status...\")  \n",
    "        checkpoints = [f for f in os.listdir(checkpoint_dir) if f.startswith(\"checkpoint-\")]\n",
    "        if checkpoints:\n",
    "            # Get latest checkpoint\n",
    "            latest_checkpoint = max(checkpoints, key=lambda x: int(x.split(\"-\")[1]))\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, latest_checkpoint)\n",
    "            print(f\"Found latest checkpoint: {latest_checkpoint}\") \n",
    "            \n",
    "            # Check checkpoint status\n",
    "            state_path = os.path.join(checkpoint_path, \"trainer_state.json\")\n",
    "            if os.path.exists(state_path):\n",
    "                with open(state_path, 'r') as f:\n",
    "                    state = json.load(f)\n",
    "                current_epoch = state.get('epoch', 0)\n",
    "                print(f\"\\nCurrent training status:\")  \n",
    "                print(f\"Current epoch: {current_epoch}\")  \n",
    "                print(f\"Target epochs: {training_args.num_train_epochs}\")  \n",
    "                \n",
    "                # Exit safely if completed\n",
    "                if current_epoch >= training_args.num_train_epochs - 0.1:\n",
    "                    print(\"\\n\" + \"=\"*50)\n",
    "                    print(\"IMPORTANT NOTICE:\")\n",
    "                    print(f\"Training has already been completed at epoch {current_epoch}!\")\n",
    "                    print(f\"Target epochs was {training_args.num_train_epochs}\")  \n",
    "                    print(f\"Trained model is available at: {checkpoint_dir}\")\n",
    "                    print(\"=\"*50 + \"\\n\")\n",
    "                    logging.info(\"Training has already been completed. Exiting to protect existing model.\")\n",
    "                    logging.info(f\"Trained model is available at: {checkpoint_dir}\")\n",
    "                    exit(0)\n",
    "            else:\n",
    "                logging.warning(\"Invalid checkpoint state found. Please check manually.\")\n",
    "                logging.warning(f\"Checkpoint directory: {checkpoint_dir}\")\n",
    "                if not is_kaggle:  \n",
    "                    user_input = input(\"Do you want to continue and overwrite? (yes/no): \")\n",
    "                    if user_input.lower() != 'yes':\n",
    "                        logging.info(\"Aborting to protect existing data.\")\n",
    "                        exit(0)\n",
    "        else:\n",
    "            logging.warning(\"Checkpoint directory exists but no checkpoints found.\")\n",
    "            if not is_kaggle:  \n",
    "                user_input = input(\"Do you want to continue and overwrite the directory? (yes/no): \")\n",
    "                if user_input.lower() != 'yes':\n",
    "                    logging.info(\"Aborting to protect existing data.\")\n",
    "                    exit(0)\n",
    "\n",
    "    # Start training (or resume)\n",
    "    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
    "    logging.info(\"Training completed successfully!\")\n",
    "    \n",
    "    # Save settings (as JSON)\n",
    "    import json\n",
    "\n",
    "    def convert_to_serializable(obj):\n",
    "        if isinstance(obj, set):\n",
    "            return list(obj)\n",
    "        elif isinstance(obj, dict):\n",
    "            return {k: convert_to_serializable(v) for k, v in obj.items()}\n",
    "        elif isinstance(obj, (list, tuple)):\n",
    "            return [convert_to_serializable(x) for x in obj]\n",
    "        return obj\n",
    "\n",
    "    # Convert each setting\n",
    "    training_args_dict = convert_to_serializable(training_args.to_dict())\n",
    "    lora_config_dict = convert_to_serializable(lora_config.to_dict())\n",
    "\n",
    "    config_dict = {\n",
    "        \"model_name\": model_name,\n",
    "        \"training_args\": training_args_dict,\n",
    "        \"lora_config\": lora_config_dict,\n",
    "        \"bnb_config\": {\n",
    "            \"load_in_4bit\": bnb_config.load_in_4bit,\n",
    "            \"bnb_4bit_use_double_quant\": bnb_config.bnb_4bit_use_double_quant,\n",
    "            \"bnb_4bit_quant_type\": bnb_config.bnb_4bit_quant_type,\n",
    "            \"bnb_4bit_compute_dtype\": str(bnb_config.bnb_4bit_compute_dtype),\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(training_args.output_dir, \"training_config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(config_dict, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    # Save model\n",
    "    trainer.save_model()\n",
    "    # Save settings\n",
    "    model.config.save_pretrained(training_args.output_dir)\n",
    "    tokenizer.save_pretrained(training_args.output_dir)\n",
    "    logging.info(\"Model and configuration saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "\n",
    "\n",
    "このコードについて、ソクラテス式チャットボットのコンテキストで説明させていただきます。大きく分けて3つの主要部分があります：\n",
    "\n",
    "### 1. チェックポイントの確認と管理\n",
    "```python\n",
    "checkpoint_dir = MODEL_OUTPUT_DIR  \n",
    "resume_from_checkpoint = None\n",
    "is_kaggle = os.path.exists('/kaggle/working')\n",
    "```\n",
    "これは学習の再開ポイントを管理する部分です。例えば：\n",
    "- 15エポック目で停止した学習を16エポック目から再開できます\n",
    "- 停電や事故で中断した場合でも、最後の保存点から再開できます\n",
    "\n",
    "具体例：\n",
    "```python\n",
    "# 学習が中断された場合のチェックポイント例\n",
    "checkpoint-1000/\n",
    "    ├── 学習済みモデル（1000ステップ目まで）\n",
    "    └── trainer_state.json\n",
    "    # 内容例：\n",
    "    # {\n",
    "    #   \"epoch\": 15,\n",
    "    #   \"学習済み対話例\": \"なぜそう考えるのですか？\"\n",
    "    # }\n",
    "```\n",
    "\n",
    "### 2. 学習状態の確認と保護\n",
    "```python\n",
    "if os.path.exists(state_path):\n",
    "    with open(state_path, 'r') as f:\n",
    "        state = json.load(f)\n",
    "    current_epoch = state.get('epoch', 0)\n",
    "```\n",
    "これは既存の学習結果を保護する機能です：\n",
    "- 既に完了している学習を誤って上書きすることを防ぎます\n",
    "- 学習の進捗状況を確認できます\n",
    "\n",
    "例えば：\n",
    "```\n",
    "===== 学習状態の確認 =====\n",
    "現在のエポック: 28\n",
    "目標エポック: 30\n",
    "場所: models/kaggle_model_ver2/\n",
    "\n",
    "警告: 学習がほぼ完了しています！\n",
    "上書きしますか？(yes/no): no\n",
    "→ 安全に終了します\n",
    "```\n",
    "\n",
    "### 3. 設定と結果の保存\n",
    "```python\n",
    "# 設定の保存\n",
    "config_dict = {\n",
    "    \"model_name\": model_name,\n",
    "    \"training_args\": training_args_dict,\n",
    "    \"lora_config\": lora_config_dict,\n",
    "    \"bnb_config\": {\n",
    "        \"load_in_4bit\": bnb_config.load_in_4bit,\n",
    "        ...\n",
    "    }\n",
    "}\n",
    "```\n",
    "これは学習設定と結果を保存する部分です：\n",
    "\n",
    "1. **設定の保存例**:\n",
    "```json\n",
    "{\n",
    "    \"model_name\": \"google/gemma-2b-jpn-it\",\n",
    "    \"training_args\": {\n",
    "        \"epochs\": 30,\n",
    "        \"learning_rate\": 8e-5,\n",
    "        \"batch_size\": 2\n",
    "    },\n",
    "    \"lora_config\": {\n",
    "        \"r\": 16,\n",
    "        \"lora_alpha\": 32\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "2. **モデルの保存**:\n",
    "```python\n",
    "trainer.save_model()\n",
    "model.config.save_pretrained(training_args.output_dir)\n",
    "tokenizer.save_pretrained(training_args.output_dir)\n",
    "```\n",
    "保存される内容の例：\n",
    "```\n",
    "models/kaggle_model_ver2/\n",
    "├── config.json        # モデル設定\n",
    "├── tokenizer.json     # トークナイザー設定\n",
    "├── model.safetensors  # 学習済みモデル\n",
    "└── training_log.txt   # 学習ログ\n",
    "    # ログ例：\n",
    "    # \"対話例1: なぜそう考えるのですか？\"\n",
    "    # \"対話例2: その根拠を説明していただけますか？\"\n",
    "```\n",
    "\n",
    "### 重要なポイント：\n",
    "\n",
    "1. **安全性の確保**:\n",
    "   - 既存の学習結果を誤って上書きしない\n",
    "   - Kaggle環境とローカル環境で異なる処理を行う\n",
    "\n",
    "2. **再現性の確保**:\n",
    "   - 全ての設定をJSONとして保存\n",
    "   - 後で同じ条件で再学習できる\n",
    "\n",
    "3. **進捗の管理**:\n",
    "   - チェックポイントで学習状態を保存\n",
    "   - 中断しても再開可能\n",
    "\n",
    "このコードにより、ソクラテス式チャットボットの学習プロセスを安全に管理し、必要な場合は再現することができます。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "except Exception as e:\n",
    "    logging.error(f\"An error occurred: {str(e)}\")\n",
    "    # Checkpoints are preserved even if an error occurs\n",
    "    raise \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "\n",
    "このコードについて、ソクラテス式チャットボットのコンテキストで説明させていただきます。\n",
    "\n",
    "```python\n",
    "except Exception as e:\n",
    "    logging.error(f\"An error occurred: {str(e)}\")\n",
    "    # Checkpoints are preserved even if an error occurs\n",
    "    raise \n",
    "```\n",
    "\n",
    "これは学習中にエラーが発生した場合の対処を行うコードです。以下に詳しく説明します：\n",
    "\n",
    "### 1. エラーの捕捉と記録\n",
    "```python\n",
    "except Exception as e:\n",
    "    logging.error(f\"An error occurred: {str(e)}\")\n",
    "```\n",
    "\n",
    "これは様々なエラーを捕捉して記録します。例えば：\n",
    "\n",
    "```python\n",
    "# エラーの例1：メモリ不足\n",
    "\"CUDA out of memory. Tried to allocate 2.20 GB.\"\n",
    "# ログ出力：\n",
    "\"ERROR: An error occurred: CUDA out of memory. Tried to allocate 2.20 GB.\"\n",
    "\n",
    "# エラーの例2：データの形式エラー\n",
    "\"Invalid dialogue format in batch 23\"\n",
    "# ログ出力：\n",
    "\"ERROR: An error occurred: Invalid dialogue format in batch 23\"\n",
    "\n",
    "# 実際のソクラテス式対話での例：\n",
    "try:\n",
    "    # 長い対話の処理\n",
    "    dialogue = [\n",
    "        \"なぜ哲学は重要だと考えますか？\",\n",
    "        \"その考えの根拠を説明していただけますか？\",\n",
    "        \"具体例を挙げて詳しく説明していただけますか？\",\n",
    "        # ... 非常に長い対話が続く ...\n",
    "    ]\n",
    "except Exception as e:\n",
    "    # エラーログの出力\n",
    "    # \"ERROR: Memory exceeded while processing philosophical dialogue\"\n",
    "```\n",
    "\n",
    "### 2. チェックポイントの保護\n",
    "```python\n",
    "# Checkpoints are preserved even if an error occurs\n",
    "raise\n",
    "```\n",
    "\n",
    "このコードには2つの重要な役割があります：\n",
    "\n",
    "1. **チェックポイントの保持**:\n",
    "   ```python\n",
    "   # 例：15エポック目でエラーが発生した場合\n",
    "   models/kaggle_model_ver2/\n",
    "   ├── checkpoint-1000/    # 保持される\n",
    "   ├── checkpoint-2000/    # 保持される\n",
    "   └── training_log.txt    # エラーが記録される\n",
    "   ```\n",
    "\n",
    "2. **エラーの再送出**:\n",
    "   - エラーを上位に伝えることで、プログラムを適切に終了させます\n",
    "   - デバッグ情報を失わずに保持できます\n",
    "\n",
    "### 実際の使用例：\n",
    "\n",
    "```python\n",
    "# 学習中のエラー発生シナリオ\n",
    "try:\n",
    "    # 学習処理\n",
    "    # エポック10: 「なぜそう考えますか？」の学習 → 成功\n",
    "    # エポック11: 「その根拠を説明できますか？」の学習 → 成功\n",
    "    # エポック12: 「具体例を挙げていただけますか？」の学習 → エラー発生！\n",
    "\n",
    "except Exception as e:\n",
    "    # エラーログの記録\n",
    "    logging.error(\"学習中にエラーが発生: メモリ不足\")\n",
    "    \n",
    "    # この時点で保存されているもの：\n",
    "    # - エポック10までの学習結果\n",
    "    # - エポック11までの学習結果\n",
    "    # - エラーログ\n",
    "    \n",
    "    raise  # エラーを再送出\n",
    "```\n",
    "\n",
    "### このエラー処理の利点：\n",
    "\n",
    "1. **データの保護**:\n",
    "   - エラーが発生しても、それまでの学習成果が失われません\n",
    "   - 後で学習を再開できます\n",
    "\n",
    "2. **デバッグの容易さ**:\n",
    "   ```python\n",
    "   # ログファイルの例\n",
    "   2024-03-20 15:30:12 - ERROR - CUDA out of memory\n",
    "   2024-03-20 15:30:12 - INFO - Last successful dialogue:\n",
    "   User: \"正義とは何でしょうか？\"\n",
    "   Model: \"その質問について、あなたはどうお考えですか？\"\n",
    "   ```\n",
    "\n",
    "3. **トレーサビリティ**:\n",
    "   - エラーの発生時刻\n",
    "   - エラーの種類\n",
    "   - エラー発生時の状況\n",
    "   が全て記録されます\n",
    "\n",
    "このエラー処理により、学習プロセスの信頼性が向上し、問題が発生しても適切に対処できます。\n",
    "\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
