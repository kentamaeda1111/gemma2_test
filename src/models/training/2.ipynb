{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. データパイプライン"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "データパイプラインについて、ソクラテス式チャットボットの文脈で説明させていただきます。\n",
    "\n",
    "### 1. トークナイザー設定 (2.1)\n",
    "これは人間の言葉をAIが理解できる数値（トークン）に変換するための準備です。\n",
    "\n",
    "例えば：\n",
    "```\n",
    "「なぜそう考えるのだろうか？」\n",
    "→ [1045, 2367, 8901, 4532, 7890]\n",
    "```\n",
    "\n",
    "特に今回は、ソクラテス式の対話に重要な句読点（「。」「、」「！」「？」）を特別なトークンとして追加しています。これにより、文の区切りや問いかけの調子をAIがより正確に理解できるようになります。\n",
    "\n",
    "### 2. データセット準備 (2.2)\n",
    "教師データ（対話例）を読み込んで整理する工程です。\n",
    "\n",
    "例えば以下のような対話データを処理します：\n",
    "```json\n",
    "{\n",
    "  \"messages\": [\n",
    "    {\"role\": \"user\", \"content\": \"幸せとは何だと思いますか？\"},\n",
    "    {\"role\": \"model\", \"content\": \"幸せとは何でしょうか。まずあなたはどのように考えますか？\"},\n",
    "    {\"role\": \"user\", \"content\": \"家族と過ごす時間だと思います\"},\n",
    "    {\"role\": \"model\", \"content\": \"なるほど、家族との時間を大切にされているのですね。では、なぜ家族と過ごす時間が幸せだと感じるのでしょうか？\"}\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "このデータに対して：\n",
    "1. 形式が正しいか確認（validate_message_format）\n",
    "2. ユーザーとモデルの発言が交互になっているか確認\n",
    "3. 対話の長さが適切か確認（256トークン以内）\n",
    "4. Gemmaモデル用の形式に変換\n",
    "\n",
    "### 3. データ前処理 (2.3)\n",
    "ソクラテス式の対話の特徴的な表現に注目するよう設定します：\n",
    "\n",
    "1. 文末表現の強調：\n",
    "   - 「だろうか」「ではないか」などの問いかけ\n",
    "   - 「なるほど」「興味深い」などの相槌\n",
    "   \n",
    "2. 接続詞の強調：\n",
    "   - 「しかし」「だから」などの論理展開\n",
    "   - 「たとえば」「つまり」などの説明\n",
    "\n",
    "3. 句読点の強調：\n",
    "   - 「、」で区切られる思考の流れ\n",
    "   - 「。」で終わる文の完結\n",
    "   - 「？」による問いかけ\n",
    "\n",
    "これらの要素に対して注意度（attention）を高く設定することで、ソクラテス式の対話らしい応答ができるよう学習を導きます。\n",
    "\n",
    "### 4. データセット最適化 (2.4)\n",
    "準備したデータを効率的に学習できるよう最終調整を行います：\n",
    "\n",
    "1. データの数値化（tokenize_function）\n",
    "2. データの検証（validate_dataset）\n",
    "3. 注意度マスクの適用（preprocess_function）\n",
    "\n",
    "これにより、例えば：\n",
    "```\n",
    "入力: 「幸せとは何でしょうか」\n",
    "↓\n",
    "数値化: [1234, 5678, 9012]\n",
    "↓\n",
    "注意度: [0.8, 1.0, 1.0]  # 「何でしょうか」の部分に高い注意度\n",
    "```\n",
    "\n",
    "このように処理されたデータを使って、ソクラテス式の問答を学習していきます。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "### 2.1 トークナイザー設定\n",
    "# Model and tokenizer preparation\n",
    "model_name = \"google/gemma-2-2b-jpn-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    token=os.environ[\"HUGGINGFACE_TOKEN\"],  \n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "\n",
    "# Add special tokens to tokenizer\n",
    "tokenizer.add_special_tokens({\n",
    "    'additional_special_tokens': [\n",
    "        '。', '、', '！', '？',  # Punctuation marks\n",
    "    ]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "このコードについて、ソクラテス式チャットボットの文脈で説明させていただきます。\n",
    "\n",
    "### 1. モデルとトークナイザーの準備\n",
    "\n",
    "```python\n",
    "model_name = \"google/gemma-2-2b-jpn-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    token=os.environ[\"HUGGINGFACE_TOKEN\"],  \n",
    "    trust_remote_code=True\n",
    ")\n",
    "```\n",
    "\n",
    "このコードは、テキストを処理するための「トークナイザー」を設定しています。トークナイザーとは、文章を機械が理解できる形に変換するツールです。\n",
    "\n",
    "具体例を挙げると：\n",
    "- 入力：「なぜそう考えるのですか？」\n",
    "- トークナイザーの処理：「なぜ」「そう」「考える」「の」「です」「か」「？」のように分割\n",
    "\n",
    "ここでは、日本語に特化した Google の Gemma モデルのトークナイザーを使用しています。\n",
    "\n",
    "### 2. 特殊トークンの追加\n",
    "\n",
    "```python\n",
    "tokenizer.add_special_tokens({\n",
    "    'additional_special_tokens': [\n",
    "        '。', '、', '！', '？',  # Punctuation marks\n",
    "    ]\n",
    "})\n",
    "```\n",
    "\n",
    "この部分は、日本語の重要な句読点をトークナイザーに特別なトークンとして認識させています。\n",
    "\n",
    "ソクラテス式対話での具体例：\n",
    "```\n",
    "入力文：「その考えは興味深いですね。なぜそう思ったのですか？」\n",
    "\n",
    "通常の処理：\n",
    "「その」「考え」「は」「興味」「深い」「です」「ね」「。」「なぜ」「そう」「思った」「の」「です」「か」「？」\n",
    "\n",
    "特殊トークン追加により：\n",
    "- 「。」と「？」が特別な意味を持つトークンとして認識される\n",
    "- これにより、文の区切りや疑問文の特徴をAIがより正確に理解できる\n",
    "```\n",
    "\n",
    "この設定は特に重要です。なぜなら：\n",
    "1. ソクラテス式対話では、質問文（「？」）が頻繁に使用される\n",
    "2. 文の区切り（「。」）で、一つの問いかけや考えが完結する\n",
    "3. 「！」は驚きや気づきを表現する際に使用される\n",
    "4. 「、」は、考えを整理して話を進める際の区切りとして重要\n",
    "\n",
    "これらの句読点を特別に認識することで、AIがより自然なソクラテス式の対話を生成できるようになります。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "### 2.2 データセット準備\n",
    "def validate_message_format(message):\n",
    "    \"\"\"Validate message format\"\"\"\n",
    "    if not isinstance(message, dict):\n",
    "        return False\n",
    "    if 'role' not in message or 'content' not in message:\n",
    "        return False\n",
    "    if message['role'] not in ['user', 'model']:\n",
    "        return False\n",
    "    if not isinstance(message['content'], str):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def prepare_dataset():\n",
    "    conversations = []\n",
    "    \n",
    "    try:\n",
    "        with open(DIALOGUE_JSON_PATH, 'r', encoding='utf-8') as f:\n",
    "            dialogue_data = json.load(f)\n",
    "            \n",
    "        for dialogue in dialogue_data:\n",
    "            messages = dialogue.get('messages', [])\n",
    "            \n",
    "            # Validate message format\n",
    "            if not all(validate_message_format(msg) for msg in messages):\n",
    "                logging.warning(f\"Skipped dialogue due to invalid message format\")\n",
    "                continue\n",
    "                \n",
    "            # Build conversation checking user->model sequence\n",
    "            current_conversation = []\n",
    "            valid_sequence = True\n",
    "            \n",
    "            for i in range(0, len(messages)-1, 2):\n",
    "                if (i+1 < len(messages) and \n",
    "                    messages[i]['role'] == 'user' and \n",
    "                    messages[i+1]['role'] == 'model'):\n",
    "                    current_conversation.extend([messages[i], messages[i+1]])\n",
    "                else:\n",
    "                    valid_sequence = False\n",
    "                    break\n",
    "            \n",
    "            # Add only valid conversations\n",
    "            if valid_sequence and current_conversation:\n",
    "                # Apply Gemma chat template\n",
    "                formatted_text = tokenizer.apply_chat_template(\n",
    "                    current_conversation,\n",
    "                    tokenize=False,\n",
    "                    add_generation_prompt=True\n",
    "                )\n",
    "                \n",
    "                # Check token count\n",
    "                tokens = tokenizer.encode(formatted_text)\n",
    "                if len(tokens) <= MAX_SEQUENCE_LENGTH:\n",
    "                    conversations.append({\"text\": formatted_text})\n",
    "                else:\n",
    "                    logging.warning(f\"Skipped conversation due to length: {len(tokens)} tokens\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing dialogue file: {str(e)}\")\n",
    "        raise\n",
    "    \n",
    "    if not conversations:\n",
    "        raise ValueError(\"No valid conversations found in the dialogue file\")\n",
    "        \n",
    "    logging.info(f\"Processed {len(conversations)} valid conversations\")\n",
    "    return Dataset.from_list(conversations)\n",
    "dataset = prepare_dataset()\n",
    "\n",
    "# Check dataset structure\n",
    "print(\"Dataset structure:\")\n",
    "print(dataset[0])  # Display first element\n",
    "print(\"\\nDataset features:\")\n",
    "print(dataset.features)\n",
    "dataset = dataset.select(range(len(dataset))).shuffle(seed=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "このコードについて、ソクラテス式チャットボットの文脈で説明させていただきます。\n",
    "\n",
    "### 1. メッセージフォーマットの検証\n",
    "```python\n",
    "def validate_message_format(message):\n",
    "    \"\"\"Validate message format\"\"\"\n",
    "    if not isinstance(message, dict):\n",
    "        return False\n",
    "    if 'role' not in message or 'content' not in message:\n",
    "        return False\n",
    "    if message['role'] not in ['user', 'model']:\n",
    "        return False\n",
    "    if not isinstance(message['content'], str):\n",
    "        return False\n",
    "    return True\n",
    "```\n",
    "\n",
    "この関数は、対話データが正しい形式かどうかをチェックします。\n",
    "\n",
    "具体例：\n",
    "```python\n",
    "# 正しい形式：\n",
    "{\n",
    "    'role': 'user',\n",
    "    'content': 'なぜ哲学は重要だと思いますか？'\n",
    "}\n",
    "\n",
    "# 間違った形式：\n",
    "{\n",
    "    'speaker': 'user',  # 'role'ではないのでエラー\n",
    "    'text': '哲学について考えましょう'  # 'content'ではないのでエラー\n",
    "}\n",
    "```\n",
    "\n",
    "### 2. データセットの準備\n",
    "```python\n",
    "def prepare_dataset():\n",
    "    conversations = []\n",
    "    \n",
    "    try:\n",
    "        with open(DIALOGUE_JSON_PATH, 'r', encoding='utf-8') as f:\n",
    "            dialogue_data = json.load(f)\n",
    "```\n",
    "\n",
    "この部分は対話データをJSONファイルから読み込みます。\n",
    "\n",
    "### 3. 対話の検証とフォーマット\n",
    "```python\n",
    "for dialogue in dialogue_data:\n",
    "    messages = dialogue.get('messages', [])\n",
    "    \n",
    "    # メッセージ形式の検証\n",
    "    if not all(validate_message_format(msg) for msg in messages):\n",
    "        logging.warning(f\"Skipped dialogue due to invalid message format\")\n",
    "        continue\n",
    "```\n",
    "\n",
    "各対話が以下のような形式になっているか確認します：\n",
    "\n",
    "```python\n",
    "# 正しい対話の例：\n",
    "{\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"幸せとは何だと思いますか？\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"model\",\n",
    "            \"content\": \"その質問は興味深いですね。あなたにとって幸せとは何でしょうか？\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "### 4. 対話順序の確認\n",
    "```python\n",
    "for i in range(0, len(messages)-1, 2):\n",
    "    if (i+1 < len(messages) and \n",
    "        messages[i]['role'] == 'user' and \n",
    "        messages[i+1]['role'] == 'model'):\n",
    "        current_conversation.extend([messages[i], messages[i+1]])\n",
    "```\n",
    "\n",
    "この部分は、対話が「ユーザーの質問→モデルの応答」という順序になっているか確認します。\n",
    "\n",
    "例：\n",
    "```python\n",
    "# 正しい順序：\n",
    "1. ユーザー: \"知識とは何でしょうか？\"\n",
    "2. モデル: \"その問いについて一緒に考えてみましょう。あなたは知識をどのように定義しますか？\"\n",
    "3. ユーザー: \"自分が確実に理解していることだと思います\"\n",
    "4. モデル: \"なるほど。では、確実な理解とは何を指すのでしょうか？\"\n",
    "\n",
    "# 間違った順序（スキップされる）：\n",
    "1. モデル: \"こんにちは\"\n",
    "2. ユーザー: \"こんにちは\"\n",
    "```\n",
    "\n",
    "### 5. 対話の長さチェックと保存\n",
    "```python\n",
    "tokens = tokenizer.encode(formatted_text)\n",
    "if len(tokens) <= MAX_SEQUENCE_LENGTH:\n",
    "    conversations.append({\"text\": formatted_text})\n",
    "else:\n",
    "    logging.warning(f\"Skipped conversation due to length: {len(tokens)} tokens\")\n",
    "```\n",
    "\n",
    "この部分は、対話が適切な長さ（MAX_SEQUENCE_LENGTH以下）かどうかをチェックします。長すぎる対話は学習データから除外されます。\n",
    "\n",
    "最後に、検証を通過した対話データをシャッフルして、学習用データセットとして準備します：\n",
    "```python\n",
    "dataset = dataset.select(range(len(dataset))).shuffle(seed=42)\n",
    "```\n",
    "\n",
    "これにより、ソクラテス式の対話パターンを学習するための質の高いデータセットが準備されます。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #ffe6e6; padding: 10px; border-left: 5px solid #8b0000; margin: 10px; width: 95%;\">\n",
    "    <summary style=\"color: #8b0000; font-size: 1.17em; font-weight: bold;\">補足</summary>\n",
    "    <div style=\"color: #8b0000;\">\n",
    "\n",
    "\n",
    "対話がuserからはじまるかどうかはチェックしてない？\n",
    "\n",
    "\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div style=\"background-color: #e6f3ff; padding: 10px; border-left: 5px solid #2196F3; margin: 10px; width: 95%;\">\n",
    "<summary style=\"color: #2196F3; font-size: 1.17em; font-weight: bold;\">Q. 質問内容</summary>\n",
    "<p style=\"color: #2196F3;\">\n",
    "\n",
    "jsonの中に例えば\n",
    "\n",
    "user1発話目\n",
    "model１発話目\n",
    "user2発話目\n",
    "model2発話目\n",
    "\n",
    "の計４発話がひとくくりになって、１つの対話になっていて、その対話が１００個あった場合、シャッフルされるのはその１００個の対話の配置ですか？それとも\n",
    "”user1発話目とmodel１発話目の１ペア”を１つの単位として切り出して、１ペアを最小単位としてシャッフルされるんですか\n",
    "\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "\n",
    "<div style=\"background-color: #f5f5f5; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #4CAF50; font-size: 1.17em; font-weight: bold;\">A. 解答</summary>\n",
    "        <div style=\"color: #4CAF50;\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "コードを確認すると、シャッフルされるのは「対話単位」です。つまり、100個の対話の配置がシャッフルされます。\n",
    "\n",
    "これは以下のコードの流れから分かります：\n",
    "\n",
    "1. **対話の処理（`prepare_dataset`関数内）**:\n",
    "```python\n",
    "for dialogue in dialogue_data:\n",
    "    messages = dialogue.get('messages', [])\n",
    "    \n",
    "    # ... 検証処理 ...\n",
    "    \n",
    "    # user->model のペアを保持したまま会話を構築\n",
    "    current_conversation = []\n",
    "    for i in range(0, len(messages)-1, 2):\n",
    "        if (i+1 < len(messages) and \n",
    "            messages[i]['role'] == 'user' and \n",
    "            messages[i+1]['role'] == 'model'):\n",
    "            current_conversation.extend([messages[i], messages[i+1]])\n",
    "    \n",
    "    # 会話全体を1つのユニットとして保存\n",
    "    if valid_sequence and current_conversation:\n",
    "        formatted_text = tokenizer.apply_chat_template(\n",
    "            current_conversation,  # 会話全体を1つのテキストに\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        conversations.append({\"text\": formatted_text})\n",
    "```\n",
    "\n",
    "2. **シャッフル処理**:\n",
    "```python\n",
    "dataset = dataset.select(range(len(dataset))).shuffle(seed=42)\n",
    "```\n",
    "\n",
    "つまり：\n",
    "```\n",
    "[対話1]\n",
    "- user1発話目\n",
    "- model1発話目\n",
    "- user2発話目\n",
    "- model2発話目\n",
    "\n",
    "[対話2]\n",
    "- user1発話目\n",
    "- model1発話目\n",
    "- user2発話目\n",
    "- model2発話目\n",
    "\n",
    "... （100個の対話）\n",
    "```\n",
    "\n",
    "これらの対話単位でシャッフルされます。対話内の発話順序（user→model→user→model）は維持されます。\n",
    "\n",
    "シャッフル後のイメージ：\n",
    "```\n",
    "[対話54]\n",
    "- user1発話目\n",
    "- model1発話目\n",
    "- user2発話目\n",
    "- model2発話目\n",
    "\n",
    "[対話12]\n",
    "- user1発話目\n",
    "- model1発話目\n",
    "- user2発話目\n",
    "- model2発話目\n",
    "\n",
    "[対話87]\n",
    "...\n",
    "```\n",
    "\n",
    "個々の発話ペアをバラバラにシャッフルしないのは、文脈の一貫性を保つためです。対話の流れを保持したまま、異なる対話のトピックが適度に混ざるようにしています。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "### 2.3 データ前処理と検証\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    result = tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        max_length=TOKENIZE_MAX_LENGTH,      # 256 から TOKENIZE_MAX_LENGTH に変更\n",
    "        padding='max_length',\n",
    "        add_special_tokens=True,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    return result\n",
    "\n",
    "# Add dataset preprocessing\n",
    "def preprocess_function(examples):\n",
    "    # Pattern definitions\n",
    "    end_patterns = [\n",
    "        \"だろうか\", \"ではないか\", \"のではないか\", \"かね\",\n",
    "        \"なるほど\", \"興味深い\", \"考えてみよう\"\n",
    "    ]\n",
    "    \n",
    "    # Conjunction patterns\n",
    "    conjunctions = [\n",
    "        \"しかし\", \"だから\", \"それでは\", \"すなわち\",\n",
    "        \"たとえば\", \"つまり\", \"ならば\", \"もし\"\n",
    "    ]\n",
    "    \n",
    "    # Get tokenized texts\n",
    "    texts = tokenizer.batch_decode(examples['input_ids'])\n",
    "    new_attention_masks = []\n",
    "    \n",
    "    for text, mask in zip(texts, examples['attention_mask']):\n",
    "        if not isinstance(mask, list):\n",
    "            mask = mask.tolist()\n",
    "        \n",
    "        # Create new attention mask (base value 0.8)\n",
    "        new_mask = [0.8] * len(mask)\n",
    "        \n",
    "        # Split into sentences\n",
    "        sentences = text.split('。')\n",
    "        current_pos = 0\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            if not sentence.strip():\n",
    "                continue\n",
    "                \n",
    "            # Detect and emphasize end patterns\n",
    "            for pattern in end_patterns:\n",
    "                if pattern in sentence:\n",
    "                    # Locate pattern position\n",
    "                    pattern_tokens = tokenizer.encode(pattern, add_special_tokens=False)\n",
    "                    pattern_len = len(pattern_tokens)\n",
    "                    \n",
    "                    # Emphasize tokens containing pattern and surrounding tokens\n",
    "                    pattern_start = current_pos + len(tokenizer.encode(sentence, add_special_tokens=False)) - pattern_len\n",
    "                    for i in range(max(0, pattern_start - 2), min(len(mask), pattern_start + pattern_len + 2)):\n",
    "                        new_mask[i] = 1.0  # Maximum attention for pattern parts\n",
    "            \n",
    "            # Detect and emphasize conjunctions\n",
    "            for conj in conjunctions:\n",
    "                if conj in sentence:\n",
    "                    # Locate conjunction position\n",
    "                    conj_tokens = tokenizer.encode(conj, add_special_tokens=False)\n",
    "                    conj_pos = len(tokenizer.encode(sentence.split(conj)[0], add_special_tokens=False))\n",
    "                    \n",
    "                    # Emphasize tokens before and after conjunction (slightly lower)\n",
    "                    for i in range(max(0, current_pos + conj_pos - 1), \n",
    "                                 min(len(mask), current_pos + conj_pos + len(conj_tokens) + 1)):\n",
    "                        new_mask[i] = 0.9\n",
    "            \n",
    "            # Emphasize punctuation marks\n",
    "            for i, char in enumerate(sentence):\n",
    "                if char in '、。！？':\n",
    "                    # Locate punctuation position\n",
    "                    punct_pos = len(tokenizer.encode(sentence[:i], add_special_tokens=False))\n",
    "                    # Emphasize tokens around punctuation\n",
    "                    for j in range(max(0, current_pos + punct_pos - 1),\n",
    "                                 min(len(mask), current_pos + punct_pos + 2)):\n",
    "                        new_mask[j] = 0.95\n",
    "            \n",
    "            # Update position for next sentence\n",
    "            current_pos += len(tokenizer.encode(sentence + '。', add_special_tokens=False))\n",
    "        \n",
    "        # Set special token masks to 1.0\n",
    "        if tokenizer.bos_token_id is not None:\n",
    "            new_mask[0] = 1.0  # BOS token\n",
    "        if tokenizer.eos_token_id is not None:\n",
    "            new_mask[-1] = 1.0  # EOS token\n",
    "            \n",
    "        new_attention_masks.append(new_mask)\n",
    "\n",
    "    examples['attention_mask'] = new_attention_masks\n",
    "    return examples\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "\n",
    "\n",
    "このコードについて、ソクラテス式チャットボットの文脈で説明させていただきます。\n",
    "\n",
    "### 1. トークン化関数\n",
    "```python\n",
    "def tokenize_function(examples):\n",
    "    result = tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        max_length=TOKENIZE_MAX_LENGTH,\n",
    "        padding='max_length',\n",
    "        add_special_tokens=True,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    return result\n",
    "```\n",
    "\n",
    "これは対話テキストをトークン（機械が理解できる単位）に変換する関数です。\n",
    "\n",
    "例：\n",
    "```python\n",
    "入力テキスト：\n",
    "\"なぜそう考えるのですか？\"\n",
    "\n",
    "トークン化後：\n",
    "[BOS]  # 文章開始トークン\n",
    "\"なぜ\" \"そう\" \"考える\" \"の\" \"です\" \"か\" \"？\"\n",
    "[EOS]  # 文章終了トークン\n",
    "```\n",
    "\n",
    "### 2. 前処理関数\n",
    "```python\n",
    "def preprocess_function(examples):\n",
    "    # パターン定義\n",
    "    end_patterns = [\n",
    "        \"だろうか\", \"ではないか\", \"のではないか\", \"かね\",\n",
    "        \"なるほど\", \"興味深い\", \"考えてみよう\"\n",
    "    ]\n",
    "    \n",
    "    conjunctions = [\n",
    "        \"しかし\", \"だから\", \"それでは\", \"すなわち\",\n",
    "        \"たとえば\", \"つまり\", \"ならば\", \"もし\"\n",
    "    ]\n",
    "```\n",
    "\n",
    "この関数は、ソクラテス式対話に特徴的な表現パターンに注目して、それらにより強い注意を払うように設定します。\n",
    "\n",
    "### 3. アテンションマスクの作成\n",
    "```python\n",
    "# Create new attention mask (base value 0.8)\n",
    "new_mask = [0.8] * len(mask)\n",
    "```\n",
    "\n",
    "アテンションマスクとは、モデルがテキストのどの部分により注目すべきかを示す値です：\n",
    "- 基本値: 0.8（通常の注意度）\n",
    "- 1.0（最大注意）\n",
    "- 0.9（やや強い注意）\n",
    "- 0.95（強い注意）\n",
    "\n",
    "### 4. 重要表現の強調\n",
    "例えば以下のような対話で：\n",
    "```\n",
    "「なぜそう考えるのでしょうか？ なるほど、興味深い視点ですね。しかし、それについてもう少し考えてみましょう。」\n",
    "```\n",
    "\n",
    "各部分の注目度が以下のように設定されます：\n",
    "\n",
    "1. 文末パターン（1.0の最大注意）：\n",
    "```python\n",
    "# \"なるほど\", \"興味深い\", \"考えてみよう\" など\n",
    "pattern_start = current_pos + len(tokenizer.encode(sentence, add_special_tokens=False)) - pattern_len\n",
    "for i in range(max(0, pattern_start - 2), min(len(mask), pattern_start + pattern_len + 2)):\n",
    "    new_mask[i] = 1.0\n",
    "```\n",
    "\n",
    "2. 接続詞（0.9のやや強い注意）：\n",
    "```python\n",
    "# \"しかし\", \"だから\", \"すなわち\" など\n",
    "for i in range(max(0, current_pos + conj_pos - 1), \n",
    "             min(len(mask), current_pos + conj_pos + len(conj_tokens) + 1)):\n",
    "    new_mask[i] = 0.9\n",
    "```\n",
    "\n",
    "3. 句読点（0.95の強い注意）：\n",
    "```python\n",
    "# \"、\", \"。\", \"！\", \"？\"\n",
    "for j in range(max(0, current_pos + punct_pos - 1),\n",
    "             min(len(mask), current_pos + punct_pos + 2)):\n",
    "    new_mask[j] = 0.95\n",
    "```\n",
    "\n",
    "この設定により、モデルは：\n",
    "1. ソクラテス式の問いかけ表現（「だろうか」「ではないか」など）\n",
    "2. 論理的な接続（「しかし」「つまり」など）\n",
    "3. 文の区切り（句読点）\n",
    "\n",
    "に特に注意を払いながら学習を行い、より自然なソクラテス式の対話を生成できるようになります。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div style=\"background-color: #e6f3ff; padding: 10px; border-left: 5px solid #2196F3; margin: 10px; width: 95%;\">\n",
    "<summary style=\"color: #2196F3; font-size: 1.17em; font-weight: bold;\">Q. 質問内容</summary>\n",
    "<p style=\"color: #2196F3;\">これは目的関数に関わることですか？それともあくまで評価時の項目ですか</p>\n",
    "</div>\n",
    "\n",
    "\n",
    "<div style=\"background-color: #f5f5f5; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #4CAF50; font-size: 1.17em; font-weight: bold;\">A. 解答</summary>\n",
    "        <div style=\"color: #4CAF50;\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "このコードは**目的関数（損失関数）に直接影響を与えます**。具体的には、Attention Maskを通じてモデルの学習過程自体に影響を与えます。\n",
    "\n",
    "以下で詳しく説明します：\n",
    "\n",
    "1. **Attention Maskの役割**:\n",
    "```python\n",
    "# 通常のAttention Mask（0か1のみ）\n",
    "normal_mask = [1, 1, 1, 1, 0, 0]  # 1: 有効なトークン、0: パディング\n",
    "\n",
    "# このコードによるAttention Mask（0.8～1.0の値）\n",
    "custom_mask = [0.8, 1.0, 0.9, 0.95, 0, 0]\n",
    "# 0.8: 基本的な注意度\n",
    "# 1.0: 重要な文末表現（「だろうか」など）\n",
    "# 0.9: 接続詞\n",
    "# 0.95: 句読点\n",
    "```\n",
    "\n",
    "2. **学習への影響**:\n",
    "```python\n",
    "# 例：ソクラテス式の問答\n",
    "text = \"知識とは何だろうか。しかし、それは表面的な理解かもしれない。\"\n",
    "\n",
    "# 注意度の重み付け\n",
    "\"知識とは\"        -> 0.8  # 基本の注意度\n",
    "\"何だろうか\"      -> 1.0  # 問いかけ表現として重要\n",
    "\"しかし\"          -> 0.9  # 論理展開の接続詞\n",
    "\"、\"              -> 0.95 # 文の区切り\n",
    "\"かもしれない\"    -> 1.0  # 思考を促す表現\n",
    "```\n",
    "\n",
    "3. **目的関数への影響**:\n",
    "- モデルの学習時に、これらの重み付けが損失関数の計算に影響\n",
    "- より重要な部分（注意度が高い部分）の誤差がより大きく反映される\n",
    "- 結果として、モデルは:\n",
    "  - 問いかけ表現の適切な使用\n",
    "  - 論理的な文章の構造\n",
    "  - 適切な句読点の使用\n",
    "  を重点的に学習する\n",
    "\n",
    "4. **評価時との違い**:\n",
    "```python\n",
    "# 学習時：\n",
    "- Attention Maskの重みが損失関数に直接影響\n",
    "- モデルのパラメータ更新に影響\n",
    "\n",
    "# 評価時：\n",
    "- compute_metrics()関数で別途評価\n",
    "- モデルの出力を評価するだけで、学習には影響しない\n",
    "```\n",
    "\n",
    "したがって、このコードは：\n",
    "- 単なる評価指標ではなく\n",
    "- 学習プロセス自体に組み込まれた\n",
    "- モデルの振る舞いを直接制御する仕組み\n",
    "\n",
    "として機能しています。これにより、ソクラテス式問答の特徴をモデルにより効果的に学習させることができます。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://claude.ai/chat/389f6e53-d8ba-43cb-b7a1-a9d04963694b\n",
    "\n",
    "ここにもある通り消した方が良いかも。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "### 2.4 データセット最適化\n",
    "# Optimize dataset processing\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    batch_size=16,\n",
    "    num_proc=2,\n",
    "    load_from_cache_file=True,\n",
    "    desc=\"Tokenizing datasets\",\n",
    "    remove_columns=dataset.column_names,\n",
    ")\n",
    "\n",
    "# Add dataset validation\n",
    "def validate_dataset(dataset):\n",
    "    # Check first element\n",
    "    first_item = dataset[0]\n",
    "    print(\"Validated first item structure:\")\n",
    "    print(f\"Keys: {first_item.keys()}\")\n",
    "    print(f\"input_ids type: {type(first_item['input_ids'])}\")\n",
    "    print(f\"input_ids length: {len(first_item['input_ids'])}\")\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "tokenized_dataset = validate_dataset(tokenized_dataset)\n",
    "\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    desc=\"Applying attention masking\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "\n",
    "\n",
    "このコードについて、ソクラテス式チャットボットの文脈で説明させていただきます。\n",
    "\n",
    "### 1. データセットのトークン化処理\n",
    "```python\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    batch_size=16,\n",
    "    num_proc=2,\n",
    "    load_from_cache_file=True,\n",
    "    desc=\"Tokenizing datasets\",\n",
    "    remove_columns=dataset.column_names,\n",
    ")\n",
    "```\n",
    "\n",
    "これは対話データを効率的にトークン化する処理です。\n",
    "\n",
    "具体的な設定：\n",
    "- `batched=True, batch_size=16`: 16個の対話を一度に処理\n",
    "- `num_proc=2`: 2つのプロセスで並列処理\n",
    "- `load_from_cache_file=True`: 処理結果をキャッシュして再利用可能に\n",
    "\n",
    "例えば：\n",
    "```python\n",
    "# 入力データ\n",
    "[\n",
    "    \"哲学とは何でしょうか？\",\n",
    "    \"その質問は本質を突いていますね。あなたは哲学をどのように考えますか？\",\n",
    "    # ... 他の対話 ...\n",
    "]\n",
    "\n",
    "# トークン化後\n",
    "[\n",
    "    [1, 345, 67, 89, 2],  # 数値化されたトークン\n",
    "    [1, 234, 56, 78, 90, 2],\n",
    "    # ... 他の対話のトークン ...\n",
    "]\n",
    "```\n",
    "\n",
    "### 2. データセットの検証\n",
    "```python\n",
    "def validate_dataset(dataset):\n",
    "    first_item = dataset[0]\n",
    "    print(\"Validated first item structure:\")\n",
    "    print(f\"Keys: {first_item.keys()}\")\n",
    "    print(f\"input_ids type: {type(first_item['input_ids'])}\")\n",
    "    print(f\"input_ids length: {len(first_item['input_ids'])}\")\n",
    "    return dataset\n",
    "```\n",
    "\n",
    "この関数は、データセットが正しい形式になっているか確認します。\n",
    "\n",
    "出力例：\n",
    "```python\n",
    "Validated first item structure:\n",
    "Keys: dict_keys(['input_ids', 'attention_mask'])\n",
    "input_ids type: <class 'list'>\n",
    "input_ids length: 512  # 設定された最大長\n",
    "```\n",
    "\n",
    "### 3. アテンションマスクの適用\n",
    "```python\n",
    "tokenized_dataset = tokenized_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    desc=\"Applying attention masking\"\n",
    ")\n",
    "```\n",
    "\n",
    "これは先ほど説明した重み付けを対話データに適用します。\n",
    "\n",
    "例えば、以下のような対話：\n",
    "```python\n",
    "\"なぜそう考えるのでしょうか？（注目度1.0）\n",
    "なるほど（注目度1.0）、\n",
    "興味深い（注目度1.0）視点ですね。\n",
    "しかし（注目度0.9）、\n",
    "それについてもう少し考えてみましょう（注目度1.0）。\"\n",
    "```\n",
    "\n",
    "このように、ソクラテス式対話の重要な要素（問いかけ、応答、接続詞など）に適切な注目度が設定され、モデルがこれらのパターンをより効果的に学習できるようになります。\n",
    "\n",
    "この最適化により：\n",
    "1. 効率的なデータ処理（並列処理、キャッシュ利用）\n",
    "2. データ形式の検証（エラー防止）\n",
    "3. ソクラテス式対話に特化した重み付け\n",
    "\n",
    "が実現され、質の高い学習データセットが準備されます。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
