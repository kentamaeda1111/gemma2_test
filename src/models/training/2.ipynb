{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. データパイプライン"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "このデータパイプラインについて、ソクラテス式チャットボットの文脈で説明していきます。\n",
    "\n",
    "### 1. トークナイザー設定 (2.1)\n",
    "```python\n",
    "model_name = \"google/gemma-2-2b-jpn-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "```\n",
    "これは、文章を機械が理解できる数値（トークン）に変換するツールを設定しています。\n",
    "\n",
    "例えば：\n",
    "- ユーザー: 「なぜ空は青いのですか？」\n",
    "- ボット: 「その質問は興味深いですね。一緒に考えてみましょう。」\n",
    "\n",
    "という会話を、機械が理解できる数値の列に変換します。また、「。」「？」などの句読点も特別なトークンとして認識するように設定しています。\n",
    "\n",
    "### 2. データセットの準備 (2.2)\n",
    "```python\n",
    "def prepare_dataset():\n",
    "    conversations = []\n",
    "    # ...\n",
    "```\n",
    "この部分は、学習用の会話データを整理する工程です。以下のようなチェックを行います：\n",
    "\n",
    "1. **フォーマットの確認**：\n",
    "   - 各メッセージが正しい形式（ユーザーとボットの役割、内容があるか）かチェック\n",
    "\n",
    "2. **会話の順序確認**：\n",
    "   ```python\n",
    "   messages[i]['role'] == 'user' and messages[i+1]['role'] == 'model'\n",
    "   ```\n",
    "   - ユーザーの質問→ソクラテス式の返答→ユーザーの質問→...\n",
    "   という正しい順序になっているか確認\n",
    "\n",
    "3. **長さの確認**：\n",
    "   - 会話が長すぎないかチェック（256トークンまで）\n",
    "\n",
    "例えば：\n",
    "```json\n",
    "{\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"なぜ勉強は大切なのでしょうか？\"},\n",
    "        {\"role\": \"model\", \"content\": \"その質問について、あなたはどう考えていますか？まずはあなたの考えを聞かせてください。\"}\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "### 3. トークン化処理 (2.3, 2.4)\n",
    "```python\n",
    "def tokenize_function(examples):\n",
    "    result = tokenizer(examples['text'],...)\n",
    "```\n",
    "準備された会話データを実際に数値に変換します：\n",
    "\n",
    "1. **トークン化**：\n",
    "   - 文章→数値列に変換\n",
    "   - 例：「なぜ」→[245, 876]のような数値の列に\n",
    "\n",
    "2. **長さの調整**：\n",
    "   - すべての会話を同じ長さ（256）に調整\n",
    "   - 短い会話は特殊なパディングトークンで埋める\n",
    "   - 長い会話は切り詰める\n",
    "\n",
    "3. **検証**：\n",
    "   - 変換されたデータが正しい形式になっているか確認\n",
    "\n",
    "このように処理されたデータは、モデルが学習できる形式になり、ソクラテス式の対話方法を学習する準備が整います。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "### 2.1 トークナイザー設定\n",
    "# Model and tokenizer preparation\n",
    "model_name = \"google/gemma-2-2b-jpn-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    token=os.environ[\"HUGGINGFACE_TOKEN\"],  \n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "\n",
    "# Add special tokens to tokenizer\n",
    "tokenizer.add_special_tokens({\n",
    "    'additional_special_tokens': [\n",
    "        '。', '、', '！', '？',  # Punctuation marks\n",
    "    ]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "# トークナイザー設定の説明\n",
    "\n",
    "トークナイザー（Tokenizer）について、ソクラテス式チャットボットの具体例を交えて説明させていただきます。\n",
    "\n",
    "## 1. トークナイザーとは？\n",
    "トークナイザーは、テキストを小さな単位（トークン）に分割するツールです。人間が読む文章をAIが理解できる形に変換する「通訳」のような役割を果たします。\n",
    "\n",
    "### 具体例：\n",
    "以下のような会話があったとします：\n",
    "```\n",
    "ユーザー：「なぜ人は学ぶのでしょうか？」\n",
    "ソクラテスボット：「それは興味深い質問ですね。あなたはどう考えますか？」\n",
    "```\n",
    "\n",
    "トークナイザーは、この文章を以下のように分割します：\n",
    "- 「なぜ」「人」「は」「学ぶ」「の」「でしょう」「か」「？」\n",
    "\n",
    "## 2. コードの説明\n",
    "\n",
    "```python\n",
    "model_name = \"google/gemma-2-2b-jpn-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    token=os.environ[\"HUGGINGFACE_TOKEN\"],  \n",
    "    trust_remote_code=True\n",
    ")\n",
    "```\n",
    "\n",
    "このコードでは：\n",
    "1. `google/gemma-2-2b-jpn-it`という日本語対応のAIモデルを使用\n",
    "2. `AutoTokenizer.from_pretrained()`で、このモデル用の専用トークナイザーを読み込み\n",
    "3. `HUGGINGFACE_TOKEN`で認証を行い、モデルにアクセス\n",
    "\n",
    "## 3. 特殊トークンの追加\n",
    "\n",
    "```python\n",
    "tokenizer.add_special_tokens({\n",
    "    'additional_special_tokens': [\n",
    "        '。', '、', '！', '？',  # Punctuation marks\n",
    "    ]\n",
    "})\n",
    "```\n",
    "\n",
    "このコードでは：\n",
    "- 日本語の句読点（「。」「、」）や感嘆符（「！」「？」）を特別なトークンとして追加\n",
    "- これにより、ソクラテスボットの返答がより自然な日本語の区切りを持つように\n",
    "\n",
    "### 具体例：\n",
    "```\n",
    "「哲学とは何だと思いますか？」\n",
    "↓ トークナイザーが処理\n",
    "「哲学」「と」「は」「何」「だ」「と」「思い」「ます」「か」「？」\n",
    "```\n",
    "\n",
    "このように、文章を適切に区切ることで、AIがより正確に文章の意味を理解し、ソクラテス式の対話を自然に行うことができるようになります。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "### 2.2 データセット準備と検証\n",
    "#\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "def validate_message_format(message):\n",
    "    \"\"\"メッセージのフォーマットを検証\"\"\"\n",
    "    if not isinstance(message, dict):\n",
    "        return False\n",
    "    if 'role' not in message or 'content' not in message:\n",
    "        return False\n",
    "    if message['role'] not in ['user', 'model']:\n",
    "        return False\n",
    "    if not isinstance(message['content'], str):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def prepare_dataset():\n",
    "    conversations = []\n",
    "    \n",
    "    try:\n",
    "        with open(DIALOGUE_JSON_PATH, 'r', encoding='utf-8') as f:\n",
    "            dialogue_data = json.load(f)\n",
    "            \n",
    "        for dialogue in dialogue_data:\n",
    "            messages = dialogue.get('messages', [])\n",
    "            \n",
    "            # メッセージのフォーマットを検証\n",
    "            if not all(validate_message_format(msg) for msg in messages):\n",
    "                logging.warning(f\"Skipped dialogue due to invalid message format\")\n",
    "                continue\n",
    "                \n",
    "            # user->modelの順序を確認しながら会話を構築\n",
    "            current_conversation = []\n",
    "            valid_sequence = True\n",
    "            \n",
    "            for i in range(0, len(messages)-1, 2):\n",
    "                if (i+1 < len(messages) and \n",
    "                    messages[i]['role'] == 'user' and \n",
    "                    messages[i+1]['role'] == 'model'):\n",
    "                    current_conversation.extend([messages[i], messages[i+1]])\n",
    "                else:\n",
    "                    valid_sequence = False\n",
    "                    break\n",
    "            \n",
    "            # 有効な会話のみを追加\n",
    "            if valid_sequence and current_conversation:\n",
    "                # Gemmaのチャットテンプレートを適用\n",
    "                formatted_text = tokenizer.apply_chat_template(\n",
    "                    current_conversation,\n",
    "                    tokenize=False,\n",
    "                    add_generation_prompt=True\n",
    "                )\n",
    "                \n",
    "                # トークン数をチェック\n",
    "                tokens = tokenizer.encode(formatted_text)\n",
    "                if len(tokens) <= MAX_SEQUENCE_LENGTH:\n",
    "                    conversations.append({\"text\": formatted_text})\n",
    "                else:\n",
    "                    logging.warning(f\"Skipped conversation due to length: {len(tokens)} tokens\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing dialogue file: {str(e)}\")\n",
    "        raise\n",
    "    \n",
    "    if not conversations:\n",
    "        raise ValueError(\"No valid conversations found in the dialogue file\")\n",
    "        \n",
    "    logging.info(f\"Processed {len(conversations)} valid conversations\")\n",
    "    return Dataset.from_list(conversations)\n",
    "dataset = prepare_dataset()\n",
    "#\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "# データセットの構造を確認\n",
    "print(\"Dataset structure:\")\n",
    "print(dataset[0])  # 最初の要素を表示\n",
    "print(\"\\nDataset features:\")\n",
    "print(dataset.features)\n",
    "\n",
    "# データセットのバッチ処理を最適化\n",
    "dataset = dataset.select(range(len(dataset))).shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "\n",
    "\n",
    "# データセット準備と検証の説明\n",
    "\n",
    "このコードは、ソクラテス式チャットボットの学習データを準備し、その品質を確保するための重要な部分です。\n",
    "\n",
    "## 1. メッセージフォーマットの検証関数\n",
    "\n",
    "```python\n",
    "def validate_message_format(message):\n",
    "    \"\"\"メッセージのフォーマットを検証\"\"\"\n",
    "    if not isinstance(message, dict):\n",
    "        return False\n",
    "    if 'role' not in message or 'content' not in message:\n",
    "        return False\n",
    "    if message['role'] not in ['user', 'model']:\n",
    "        return False\n",
    "    if not isinstance(message['content'], str):\n",
    "        return False\n",
    "    return True\n",
    "```\n",
    "\n",
    "この関数は、各メッセージが正しい形式かチェックします：\n",
    "\n",
    "### 具体例：\n",
    "✅ 正しい形式：\n",
    "```python\n",
    "{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"知識とは何だと思いますか？\"\n",
    "}\n",
    "```\n",
    "\n",
    "❌ 間違った形式：\n",
    "```python\n",
    "{\n",
    "    \"speaker\": \"user\",  # \"role\"ではないので不正\n",
    "    \"text\": \"知識とは何だと思いますか？\"  # \"content\"ではないので不正\n",
    "}\n",
    "```\n",
    "\n",
    "## 2. データセット準備関数\n",
    "\n",
    "```python\n",
    "def prepare_dataset():\n",
    "    conversations = []\n",
    "    # ... 既存のコード ...\n",
    "```\n",
    "\n",
    "この関数は以下の重要な処理を行います：\n",
    "\n",
    "### a) 会話データの読み込みと検証\n",
    "- JSONファイルから会話データを読み込み\n",
    "- 各メッセージの形式を検証\n",
    "- ユーザーとモデルの発言が正しく交互になっているか確認\n",
    "\n",
    "### b) 会話形式の整形\n",
    "```python\n",
    "formatted_text = tokenizer.apply_chat_template(\n",
    "    current_conversation,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "```\n",
    "\n",
    "### 具体例：\n",
    "変換前：\n",
    "```python\n",
    "[\n",
    "    {\"role\": \"user\", \"content\": \"正義とは何ですか？\"},\n",
    "    {\"role\": \"model\", \"content\": \"その質問について、あなたはどう考えますか？\"}\n",
    "]\n",
    "```\n",
    "\n",
    "変換後：\n",
    "```\n",
    "<start>ユーザー: 正義とは何ですか？\n",
    "システム: その質問について、あなたはどう考えますか？<end>\n",
    "```\n",
    "\n",
    "### c) 長さの確認\n",
    "- 会話が長すぎないかチェック（`MAX_SEQUENCE_LENGTH`以下）\n",
    "- 長すぎる会話は学習データから除外\n",
    "\n",
    "## 3. データセットの最終処理\n",
    "\n",
    "```python\n",
    "dataset = dataset.select(range(len(dataset))).shuffle(seed=42)\n",
    "```\n",
    "\n",
    "- データをランダムに並び替え（シャッフル）\n",
    "- 42というシード値を使用して、毎回同じ順序になるように制御\n",
    "\n",
    "このような厳密な検証と準備により、高品質な学習データセットが作成され、ソクラテス式チャットボットの応答の質を向上させることができます。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "### 2.3 トークン化関数の定義\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    result = tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        max_length=TOKENIZE_MAX_LENGTH,      # 256 から TOKENIZE_MAX_LENGTH に変更\n",
    "        padding='max_length',\n",
    "        add_special_tokens=True,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    return result\n",
    "\n",
    "# Add dataset preprocessing\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"シンプルな前処理\"\"\"\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        max_length=TOKENIZE_MAX_LENGTH,\n",
    "        padding='max_length',\n",
    "        add_special_tokens=True,\n",
    "        return_tensors=None\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "\n",
    "\n",
    "# トークン化関数の説明\n",
    "\n",
    "このコードでは、テキストデータをAIモデルが理解できる形式に変換するための2つの関数が定義されています。\n",
    "\n",
    "## 1. トークン化とは？\n",
    "\n",
    "テキストデータを数値（トークン）に変換する処理です。これは人間の言葉をAIが理解できる「数字の言葉」に翻訳する作業だと考えてください。\n",
    "\n",
    "## 2. `tokenize_function`の解説\n",
    "\n",
    "```python\n",
    "def tokenize_function(examples):\n",
    "    result = tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        max_length=TOKENIZE_MAX_LENGTH,\n",
    "        padding='max_length',\n",
    "        add_special_tokens=True,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    return result\n",
    "```\n",
    "\n",
    "### パラメータの説明：\n",
    "\n",
    "1. `truncation=True`\n",
    "   - 長すぎるテキストを切り詰める\n",
    "   - 例：長い哲学的な議論を適切な長さに調整\n",
    "\n",
    "2. `max_length=TOKENIZE_MAX_LENGTH`\n",
    "   - テキストの最大長を設定\n",
    "   - 例：「知識とは何か？」という短い質問から、長い対話まで一定の長さに統一\n",
    "\n",
    "3. `padding='max_length'`\n",
    "   - 短いテキストを最大長まで埋める\n",
    "   - 例：\n",
    "     ```\n",
    "     短いテキスト：「なぜ？」\n",
    "     ↓\n",
    "     パディング後：「なぜ？」 + [PAD][PAD][PAD]...\n",
    "     ```\n",
    "\n",
    "4. `add_special_tokens=True`\n",
    "   - 特別な意味を持つトークンを追加\n",
    "   - 例：\n",
    "     ```\n",
    "     入力：「人生の意味とは何ですか？」\n",
    "     ↓\n",
    "     変換後：[START]人生の意味とは何ですか？[END]\n",
    "     ```\n",
    "\n",
    "## 3. `preprocess_function`の解説\n",
    "\n",
    "```python\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"シンプルな前処理\"\"\"\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        max_length=TOKENIZE_MAX_LENGTH,\n",
    "        padding='max_length',\n",
    "        add_special_tokens=True,\n",
    "        return_tensors=None\n",
    "    )\n",
    "```\n",
    "\n",
    "この関数は`tokenize_function`と同じ処理を行います。2つの関数が存在する理由は：\n",
    "- データの異なる処理段階で使用\n",
    "- 将来的な機能拡張に備える\n",
    "\n",
    "### 具体例：\n",
    "\n",
    "ソクラテス式の対話の処理例：\n",
    "```\n",
    "元のテキスト：\n",
    "ユーザー：「真実とは何でしょうか？」\n",
    "ソクラテスAI：「興味深い質問ですね。あなたにとって真実とは何を意味しますか？」\n",
    "\n",
    "↓ トークン化処理\n",
    "\n",
    "[START]\n",
    "[USER]真実とは何でしょうか？[/USER]\n",
    "[ASSISTANT]興味深い質問ですね。あなたにとって真実とは何を意味しますか？[/ASSISTANT]\n",
    "[END]\n",
    "[PAD][PAD]...（必要に応じてパディング）\n",
    "```\n",
    "\n",
    "このように、人間の対話をAIが学習しやすい形式に変換することで、より自然なソクラテス式の対話が可能になります。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "### 2.4 データセットのトークン化と検証\n",
    "# Optimize dataset processing\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    batch_size=16,\n",
    "    num_proc=2,\n",
    "    load_from_cache_file=True,\n",
    "    desc=\"Tokenizing datasets\",\n",
    "    remove_columns=dataset.column_names,\n",
    ")\n",
    "\n",
    "# Add dataset validation\n",
    "def validate_dataset(dataset):\n",
    "    # Check first element\n",
    "    first_item = dataset[0]\n",
    "    print(\"Validated first item structure:\")\n",
    "    print(f\"Keys: {first_item.keys()}\")\n",
    "    print(f\"input_ids type: {type(first_item['input_ids'])}\")\n",
    "    print(f\"input_ids length: {len(first_item['input_ids'])}\")\n",
    "    return dataset\n",
    "\n",
    "tokenized_dataset = validate_dataset(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "\n",
    "\n",
    "# データセットのトークン化と検証の説明\n",
    "\n",
    "このコードセクションは、データセットを実際にAIが学習できる形に変換し、その品質を確認する重要な部分です。\n",
    "\n",
    "## 1. データセットのトークン化\n",
    "\n",
    "```python\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    batch_size=16,\n",
    "    num_proc=2,\n",
    "    load_from_cache_file=True,\n",
    "    desc=\"Tokenizing datasets\",\n",
    "    remove_columns=dataset.column_names,\n",
    ")\n",
    "```\n",
    "\n",
    "### パラメータの説明：\n",
    "\n",
    "1. `batched=True, batch_size=16`\n",
    "   - 16件ずつまとめて処理\n",
    "   - 例：100個の対話データがあれば、16件ずつ6回（最後は4件）に分けて処理\n",
    "\n",
    "2. `num_proc=2`\n",
    "   - 2つのプロセスで並列処理\n",
    "   - 処理速度を向上させる\n",
    "\n",
    "3. `load_from_cache_file=True`\n",
    "   - 一度処理したデータを保存\n",
    "   - 次回から高速に読み込み可能\n",
    "\n",
    "4. `remove_columns=dataset.column_names`\n",
    "   - 元のテキストデータを削除\n",
    "   - トークン化されたデータのみを保持\n",
    "\n",
    "## 2. データセットの検証\n",
    "\n",
    "```python\n",
    "def validate_dataset(dataset):\n",
    "    first_item = dataset[0]\n",
    "    print(\"Validated first item structure:\")\n",
    "    print(f\"Keys: {first_item.keys()}\")\n",
    "    print(f\"input_ids type: {type(first_item['input_ids'])}\")\n",
    "    print(f\"input_ids length: {len(first_item['input_ids'])}\")\n",
    "    return dataset\n",
    "```\n",
    "\n",
    "### 具体例：\n",
    "\n",
    "元の対話データ：\n",
    "```\n",
    "ユーザー：「知恵とは何でしょうか？」\n",
    "ソクラテスAI：「その質問について、あなたはどのように考えていますか？」\n",
    "```\n",
    "\n",
    "トークン化後のデータ構造：\n",
    "```python\n",
    "{\n",
    "    'input_ids': [1, 1543, 235, 8867, ..., 2],  # テキストを数値に変換\n",
    "    'attention_mask': [1, 1, 1, 1, ..., 0]      # どの部分が実際のテキストか示す\n",
    "}\n",
    "```\n",
    "\n",
    "### 検証内容：\n",
    "\n",
    "1. データ構造の確認\n",
    "   ```python\n",
    "   print(f\"Keys: {first_item.keys()}\")\n",
    "   # 期待される出力：Keys: ['input_ids', 'attention_mask']\n",
    "   ```\n",
    "\n",
    "2. データ型の確認\n",
    "   ```python\n",
    "   print(f\"input_ids type: {type(first_item['input_ids'])}\")\n",
    "   # 期待される出力：input_ids type: <class 'list'>\n",
    "   ```\n",
    "\n",
    "3. データ長の確認\n",
    "   ```python\n",
    "   print(f\"input_ids length: {len(first_item['input_ids'])}\")\n",
    "   # 期待される出力：input_ids length: 256（設定した最大長）\n",
    "   ```\n",
    "\n",
    "このような検証により：\n",
    "- データが正しくトークン化されているか\n",
    "- AIモデルが学習できる形式になっているか\n",
    "- データの長さが適切か\n",
    "\n",
    "を確認することができます。これにより、ソクラテス式チャットボットの学習データの品質を保証します。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
