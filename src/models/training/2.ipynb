{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "このコードの「Data Preprocessing Pipeline（データ前処理パイプライン）」について、ソクラテス式チャットボットプロジェクトの文脈で説明させていただきます。\n",
    "\n",
    "大きく分けて5つの重要なステップがあります：\n",
    "\n",
    "### 1. トークナイザーの設定 (2.1)\n",
    "```python\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"google/gemma-2-2b-jpn-it\",\n",
    "    trust_remote_code=True,\n",
    "    token=huggingface_token \n",
    ")\n",
    "```\n",
    "これは、テキストを機械が理解できる数値（トークン）に変換するツールの準備です。例えば「君は幸せについてどう考えるかね？」という文章を、LLMが理解できる数値の配列に変換します。\n",
    "\n",
    "### 2. データの検証 (2.2)\n",
    "```python\n",
    "def validate_message_format(message):\n",
    "    \"\"\"Validate message format\"\"\"\n",
    "    if not isinstance(message, dict):\n",
    "        return False\n",
    "    # ...\n",
    "```\n",
    "これは、対話データが正しい形式かチェックします。例えば：\n",
    "- メッセージが「user」か「model」のものか\n",
    "- 内容がちゃんとテキストとして保存されているか\n",
    "などを確認します。\n",
    "\n",
    "### 3. データセットの準備 (2.3)\n",
    "```python\n",
    "def prepare_dataset():\n",
    "    conversations = []\n",
    "    # ...\n",
    "```\n",
    "このステップでは：\n",
    "1. 対話データを読み込み\n",
    "2. ユーザーとモデル（ソクラテス）の会話ペアを作成\n",
    "3. Gemmaモデル用の形式に変換\n",
    "4. 長すぎる会話は除外\n",
    "\n",
    "例えば：\n",
    "```json\n",
    "{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"幸せとは何でしょうか？\"\n",
    "},\n",
    "{\n",
    "    \"role\": \"model\",\n",
    "    \"content\": \"君にとって幸せとは何かね？\"\n",
    "}\n",
    "```\n",
    "このような対話ペアを処理します。\n",
    "\n",
    "### 4. 特別な処理 (2.4)\n",
    "```python\n",
    "def preprocess_function(examples):\n",
    "    socratic_patterns = [\n",
    "        \"かね\", \"だろうか\", \"のかね\", \"ではないかね\",\n",
    "        # ...\n",
    "    ]\n",
    "```\n",
    "ここでは、ソクラテス式の特徴的な表現（「かね」「だろうか」など）に特別な注目を集めるよう処理します。これにより、モデルがこれらの表現パターンをより効果的に学習できるようになります。\n",
    "\n",
    "### 5. 最終的なデータセット作成 (2.5)\n",
    "```python\n",
    "dataset = prepare_dataset()\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    # ...\n",
    ")\n",
    "```\n",
    "すべての処理を施したデータを、効率的に学習できるよう最終的な形に整えます。\n",
    "\n",
    "このパイプライン全体の目的は、生の対話データを、ソクラテス式の口調を効果的に学習できる形に整えることです。特に「かね」「だろうか」といった特徴的な表現パターンを強調することで、モデルがソクラテス風の話し方を身につけやすくなるよう工夫されています。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Tokenizer Setup and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"google/gemma-2-2b-jpn-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    token=huggingface_token \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "トークナイザーの設定について、より詳しく説明させていただきます。\n",
    "\n",
    "### トークナイザーとは？\n",
    "トークナイザーは、人間の言葉をAIが理解できる数値（トークン）に変換する翻訳機のようなものです。\n",
    "\n",
    "例えば、以下のようなソクラテス式の発言があるとします：\n",
    "```\n",
    "「君は幸せについて、どのように考えるかね？」\n",
    "```\n",
    "\n",
    "トークナイザーは、この文章を以下のような数値の配列に変換します（実際の数値は例示です）：\n",
    "```\n",
    "[2458, 15, 9874, 234, 567, 8901, 45, 789]\n",
    "```\n",
    "\n",
    "### コードの詳細説明\n",
    "```python\n",
    "model_name = \"google/gemma-2-2b-jpn-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    token=huggingface_token \n",
    ")\n",
    "```\n",
    "\n",
    "1. `model_name = \"google/gemma-2-2b-jpn-it\"`\n",
    "   - 使用するAIモデルを指定しています\n",
    "   - このモデルは日本語に特化した比較的小さめのモデルです\n",
    "\n",
    "2. `AutoTokenizer.from_pretrained()`\n",
    "   - Gemmaモデル専用のトークナイザーを読み込みます\n",
    "   - このトークナイザーは日本語テキストを適切に処理できるよう特別に調整されています\n",
    "\n",
    "3. `trust_remote_code=True`\n",
    "   - Gemmaモデル専用の特別な処理ルールを使用可能にします\n",
    "   - 例：「かね」「だろうか」などのソクラテス式の表現を適切に処理\n",
    "\n",
    "4. `token=huggingface_token`\n",
    "   - Hugging Faceというサービスからモデルを安全にダウンロードするための認証キー\n",
    "\n",
    "### 具体的な処理例\n",
    "このトークナイザーは以下のような処理を行います：\n",
    "\n",
    "1. 文章の分割：\n",
    "```python\n",
    "「君は幸せについて、どのように考えるかね？」\n",
    "↓\n",
    "['君', 'は', '幸せ', 'について', '、', 'どのように', '考える', 'かね', '？']\n",
    "```\n",
    "\n",
    "2. 数値への変換：\n",
    "```python\n",
    "'君' → 2458\n",
    "'は' → 15\n",
    "'幸せ' → 9874\n",
    "'について' → 234\n",
    "...\n",
    "'かね' → 789\n",
    "```\n",
    "\n",
    "3. 特別なトークンの追加：\n",
    "- 文章の始まりを示す特別な数値（BOS token）\n",
    "- 文章の終わりを示す特別な数値（EOS token）\n",
    "- 必要に応じてパディング（埋め合わせ）用の数値\n",
    "\n",
    "このように、トークナイザーは人間の言葉をAIが処理できる形に変換する重要な役割を果たします。特に今回のプロジェクトでは、ソクラテス式の特徴的な表現（「かね」「だろうか」など）を適切に処理できることが重要です。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Data Validation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def validate_message_format(message):\n",
    "    \"\"\"Validate message format\"\"\"\n",
    "    if not isinstance(message, dict):\n",
    "        return False\n",
    "    if 'role' not in message or 'content' not in message:\n",
    "        return False\n",
    "    if message['role'] not in ['user', 'model']:\n",
    "        return False\n",
    "    if not isinstance(message['content'], str):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "def validate_dataset(dataset):\n",
    "    \"\"\"Validate dataset structure\"\"\"\n",
    "    first_item = dataset[0]\n",
    "    print(\"Validated first item structure:\")\n",
    "    print(f\"Keys: {first_item.keys()}\")\n",
    "    print(f\"input_ids type: {type(first_item['input_ids'])}\")\n",
    "    print(f\"input_ids length: {len(first_item['input_ids'])}\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "\n",
    "\n",
    "この部分は、データの形式が正しいかをチェックする2つの検証関数について説明します。\n",
    "\n",
    "### 1. validate_message_format関数\n",
    "```python\n",
    "def validate_message_format(message):\n",
    "    \"\"\"Validate message format\"\"\"\n",
    "    if not isinstance(message, dict):\n",
    "        return False\n",
    "    if 'role' not in message or 'content' not in message:\n",
    "        return False\n",
    "    if message['role'] not in ['user', 'model']:\n",
    "        return False\n",
    "    if not isinstance(message['content'], str):\n",
    "        return False\n",
    "    return True\n",
    "```\n",
    "\n",
    "この関数は、各メッセージが正しい形式かをチェックします。具体的には：\n",
    "\n",
    "1. メッセージが辞書型（dict）かチェック\n",
    "   ```python\n",
    "   # 正しい形式：\n",
    "   {\n",
    "       \"role\": \"user\",\n",
    "       \"content\": \"幸せとは何でしょうか？\"\n",
    "   }\n",
    "   \n",
    "   # 間違った形式：\n",
    "   [\"user\", \"幸せとは何でしょうか？\"]  # これは配列なのでエラー\n",
    "   ```\n",
    "\n",
    "2. 必要なキー（'role'と'content'）があるかチェック\n",
    "   ```python\n",
    "   # 正しい形式：\n",
    "   {\n",
    "       \"role\": \"model\",\n",
    "       \"content\": \"君は幸せについてどう考えるかね？\"\n",
    "   }\n",
    "   \n",
    "   # 間違った形式：\n",
    "   {\n",
    "       \"role\": \"model\"  # contentが無いのでエラー\n",
    "   }\n",
    "   ```\n",
    "\n",
    "3. roleが'user'か'model'のどちらかかチェック\n",
    "   ```python\n",
    "   # 正しい形式：\n",
    "   {\"role\": \"user\", \"content\": \"...\"}\n",
    "   {\"role\": \"model\", \"content\": \"...\"}\n",
    "   \n",
    "   # 間違った形式：\n",
    "   {\"role\": \"assistant\", \"content\": \"...\"}  # roleが不正なのでエラー\n",
    "   ```\n",
    "\n",
    "4. contentが文字列かチェック\n",
    "   ```python\n",
    "   # 正しい形式：\n",
    "   {\"role\": \"model\", \"content\": \"君はどう思うかね？\"}\n",
    "   \n",
    "   # 間違った形式：\n",
    "   {\"role\": \"model\", \"content\": 123}  # 数値なのでエラー\n",
    "   ```\n",
    "\n",
    "### 2. validate_dataset関数\n",
    "```python\n",
    "def validate_dataset(dataset):\n",
    "    \"\"\"Validate dataset structure\"\"\"\n",
    "    first_item = dataset[0]\n",
    "    print(\"Validated first item structure:\")\n",
    "    print(f\"Keys: {first_item.keys()}\")\n",
    "    print(f\"input_ids type: {type(first_item['input_ids'])}\")\n",
    "    print(f\"input_ids length: {len(first_item['input_ids'])}\")\n",
    "    return dataset\n",
    "```\n",
    "\n",
    "この関数は、データセット全体の構造を確認します：\n",
    "\n",
    "1. データセットの最初の項目を取り出して内容を表示\n",
    "   ```python\n",
    "   # 表示される例：\n",
    "   Validated first item structure:\n",
    "   Keys: dict_keys(['input_ids', 'attention_mask'])\n",
    "   input_ids type: <class 'list'>\n",
    "   input_ids length: 256\n",
    "   ```\n",
    "\n",
    "2. 以下の情報を確認：\n",
    "   - どんなキーがあるか（input_ids, attention_maskなど）\n",
    "   - input_ids（トークン化されたテキスト）のデータ型\n",
    "   - input_idsの長さ（設定した最大長に収まっているか）\n",
    "\n",
    "これらの検証は、学習データが正しい形式でモデルに渡されることを保証する重要な役割を果たします。データの形式が間違っていると、学習が正しく行われない可能性があるためです。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Data Set Preparation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 2.3 Data Set Preparation Function\n",
    "def prepare_dataset():\n",
    "    conversations = []\n",
    "    \n",
    "    try:\n",
    "        with open(DIALOGUE_JSON_PATH, 'r', encoding='utf-8') as f:\n",
    "            dialogue_data = json.load(f)\n",
    "            \n",
    "        for dialogue in dialogue_data:\n",
    "            messages = dialogue.get('messages', [])\n",
    "            \n",
    "            # Validate message format\n",
    "            if not all(validate_message_format(msg) for msg in messages):\n",
    "                logging.warning(f\"Skipped dialogue due to invalid message format\")\n",
    "                continue\n",
    "                \n",
    "            # Construct conversation in user->model order\n",
    "            current_conversation = []\n",
    "            valid_sequence = True\n",
    "            \n",
    "            for i in range(0, len(messages)-1, 2):\n",
    "                if (i+1 < len(messages) and \n",
    "                    messages[i]['role'] == 'user' and \n",
    "                    messages[i+1]['role'] == 'model'):\n",
    "                    current_conversation.extend([messages[i], messages[i+1]])\n",
    "                else:\n",
    "                    valid_sequence = False\n",
    "                    break\n",
    "            \n",
    "            # Add only valid conversations\n",
    "            if valid_sequence and current_conversation:\n",
    "                # Apply Gemma chat template\n",
    "                formatted_text = tokenizer.apply_chat_template(\n",
    "                    current_conversation,\n",
    "                    tokenize=False,\n",
    "                    add_generation_prompt=True\n",
    "                )\n",
    "                \n",
    "                # Check token count\n",
    "                tokens = tokenizer.encode(formatted_text)\n",
    "                if len(tokens) <= MAX_SEQUENCE_LENGTH:\n",
    "                    conversations.append({\"text\": formatted_text})\n",
    "                else:\n",
    "                    logging.warning(f\"Skipped conversation due to length: {len(tokens)} tokens\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing dialogue file: {str(e)}\")\n",
    "        raise\n",
    "    \n",
    "    if not conversations:\n",
    "        raise ValueError(\"No valid conversations found in the dialogue file\")\n",
    "        \n",
    "    logging.info(f\"Processed {len(conversations)} valid conversations\")\n",
    "    return Dataset.from_list(conversations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "\n",
    "\n",
    "`prepare_dataset`関数について、ソクラテス式チャットボットプロジェクトの文脈で説明します。\n",
    "\n",
    "この関数は、対話データを学習用に整形する重要な処理を行います。主に以下の5つのステップがあります：\n",
    "\n",
    "### 1. データの読み込み\n",
    "```python\n",
    "with open(DIALOGUE_JSON_PATH, 'r', encoding='utf-8') as f:\n",
    "    dialogue_data = json.load(f)\n",
    "```\n",
    "JSONファイルから対話データを読み込みます。例えば：\n",
    "```json\n",
    "[\n",
    "  {\n",
    "    \"messages\": [\n",
    "      {\"role\": \"user\", \"content\": \"幸せとは何でしょうか？\"},\n",
    "      {\"role\": \"model\", \"content\": \"君は幸せをどのように定義するかね？\"}\n",
    "    ]\n",
    "  },\n",
    "  // 他の対話データ...\n",
    "]\n",
    "```\n",
    "\n",
    "### 2. メッセージ形式の検証\n",
    "```python\n",
    "if not all(validate_message_format(msg) for msg in messages):\n",
    "    logging.warning(f\"Skipped dialogue due to invalid message format\")\n",
    "    continue\n",
    "```\n",
    "各メッセージが正しい形式か確認します。不正な形式の例：\n",
    "```json\n",
    "{\n",
    "    \"speaker\": \"user\",  // \"role\"ではないのでエラー\n",
    "    \"text\": \"...\"      // \"content\"ではないのでエラー\n",
    "}\n",
    "```\n",
    "\n",
    "### 3. 対話順序の構築と検証\n",
    "```python\n",
    "for i in range(0, len(messages)-1, 2):\n",
    "    if (i+1 < len(messages) and \n",
    "        messages[i]['role'] == 'user' and \n",
    "        messages[i+1]['role'] == 'model'):\n",
    "        current_conversation.extend([messages[i], messages[i+1]])\n",
    "```\n",
    "必ずuser→modelの順序になっているか確認します：\n",
    "```python\n",
    "# 正しい順序：\n",
    "[\n",
    "    {\"role\": \"user\", \"content\": \"幸せとは何でしょうか？\"},\n",
    "    {\"role\": \"model\", \"content\": \"君は幸せをどのように定義するかね？\"}\n",
    "]\n",
    "\n",
    "# 誤った順序（スキップされる）：\n",
    "[\n",
    "    {\"role\": \"model\", \"content\": \"...\"},  # modelが先\n",
    "    {\"role\": \"user\", \"content\": \"...\"}\n",
    "]\n",
    "```\n",
    "\n",
    "### 4. Gemmaモデル用の形式に変換\n",
    "```python\n",
    "formatted_text = tokenizer.apply_chat_template(\n",
    "    current_conversation,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "```\n",
    "対話をGemmaモデルが理解できる特別な形式に変換します。例：\n",
    "```python\n",
    "# 変換前：\n",
    "[{\"role\": \"user\", \"content\": \"幸せとは？\"}, \n",
    " {\"role\": \"model\", \"content\": \"君はどう考えるかね？\"}]\n",
    "\n",
    "# 変換後：\n",
    "\"<start_of_turn>user\\n幸せとは？<end_of_turn>\n",
    "<start_of_turn>model\\n君はどう考えるかね？<end_of_turn>\"\n",
    "```\n",
    "\n",
    "### 5. 長さの確認と最終データセット作成\n",
    "```python\n",
    "tokens = tokenizer.encode(formatted_text)\n",
    "if len(tokens) <= MAX_SEQUENCE_LENGTH:\n",
    "    conversations.append({\"text\": formatted_text})\n",
    "```\n",
    "対話が長すぎないか確認し（256トークン以内）、問題なければ最終的なデータセットに追加します。\n",
    "\n",
    "最後に、有効な対話が1つも無い場合はエラーを発生させ、あれば処理済みの対話データをデータセット形式で返します：\n",
    "```python\n",
    "return Dataset.from_list(conversations)\n",
    "```\n",
    "\n",
    "この関数は、生の対話データを、ソクラテス式チャットボットの学習に適した形式に変換する重要な役割を果たしています。特に、user→modelの順序を保証することで、ソクラテスのような応答パターンを学習しやすくしています。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 Data Processing Pipeline Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 2.4 Data Processing Pipeline Function\n",
    "def tokenize_function(examples):\n",
    "    result = tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        max_length=MAX_TOKENIZE_LENGTH,      # Use global setting\n",
    "        padding='max_length',\n",
    "        add_special_tokens=True,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    return result\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # Focus on Socratic tone and inquiry patterns\n",
    "    socratic_patterns = [\n",
    "        # Question patterns\n",
    "        \"かね\", \"だろうか\", \"のかね\", \"ではないかね\",\n",
    "        # Question introduction\n",
    "        \"では\", \"について\",\n",
    "        # Second person (characteristic of mature tone)\n",
    "        \"君は\", \"君が\", \"君の\"\n",
    "    ]\n",
    "    \n",
    "    # Get tokenized text\n",
    "    texts = tokenizer.batch_decode(examples['input_ids'])\n",
    "    new_attention_masks = []\n",
    "    \n",
    "    for text, mask in zip(texts, examples['attention_mask']):\n",
    "        if not isinstance(mask, list):\n",
    "            mask = mask.tolist()\n",
    "\n",
    "        new_mask = mask.copy() \n",
    "        \n",
    "        # Split text\n",
    "        sentences = text.split('。')\n",
    "        current_pos = 0\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            if not sentence.strip():\n",
    "                continue\n",
    "            \n",
    "            # Detect and highlight Socratic patterns\n",
    "            for pattern in socratic_patterns:\n",
    "                if pattern in sentence:\n",
    "                    # Identify pattern position\n",
    "                    pattern_tokens = tokenizer.encode(pattern, add_special_tokens=False)\n",
    "                    pattern_len = len(pattern_tokens)\n",
    "                    \n",
    "                    # Highlight tokens containing the pattern and its surroundings\n",
    "                    pattern_start = current_pos + len(tokenizer.encode(sentence, add_special_tokens=False)) - pattern_len\n",
    "                    for i in range(max(0, pattern_start - 2), min(len(mask), pattern_start + pattern_len + 2)):\n",
    "                        new_mask[i] = 1.0  # Max attention to pattern part\n",
    "            \n",
    "            # Update position for each sentence segment\n",
    "            current_pos += len(tokenizer.encode(sentence + '。', add_special_tokens=False))\n",
    "        \n",
    "        # Special token masks are set to 1.0\n",
    "        if tokenizer.bos_token_id is not None:\n",
    "            new_mask[0] = 1.0  # BOS token\n",
    "        if tokenizer.eos_token_id is not None:\n",
    "            new_mask[-1] = 1.0  # EOS token\n",
    "            \n",
    "        new_attention_masks.append(new_mask)\n",
    "\n",
    "    examples['attention_mask'] = new_attention_masks\n",
    "    return examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "\n",
    "\n",
    "この部分は、データの前処理を行う2つの重要な関数について説明します。\n",
    "\n",
    "### 1. tokenize_function\n",
    "```python\n",
    "def tokenize_function(examples):\n",
    "    result = tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        max_length=MAX_TOKENIZE_LENGTH,\n",
    "        padding='max_length',\n",
    "        add_special_tokens=True,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    return result\n",
    "```\n",
    "\n",
    "この関数は、テキストをトークン化（数値化）します。\n",
    "\n",
    "例えば：\n",
    "```python\n",
    "# 入力テキスト\n",
    "text = \"君は幸せについて、どのように考えるかね？\"\n",
    "\n",
    "# トークン化の結果\n",
    "result = {\n",
    "    'input_ids': [1, 245, 15, 987, 234, 567, 890, 45, 789, 2],  # 数値化されたテキスト\n",
    "    'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  # どの部分を注目するか\n",
    "}\n",
    "```\n",
    "\n",
    "パラメータの説明：\n",
    "- `truncation=True`: 長すぎるテキストを切り詰める\n",
    "- `max_length=MAX_TOKENIZE_LENGTH`: 最大256トークンまで\n",
    "- `padding='max_length'`: 短いテキストを最大長まで埋める\n",
    "- `add_special_tokens=True`: 文章の開始・終了を示す特別なトークンを追加\n",
    "\n",
    "### 2. preprocess_function\n",
    "```python\n",
    "def preprocess_function(examples):\n",
    "    socratic_patterns = [\n",
    "        # Question patterns\n",
    "        \"かね\", \"だろうか\", \"のかね\", \"ではないかね\",\n",
    "        # Question introduction\n",
    "        \"では\", \"について\",\n",
    "        # Second person\n",
    "        \"君は\", \"君が\", \"君の\"\n",
    "    ]\n",
    "    # ...\n",
    "```\n",
    "\n",
    "この関数は、ソクラテス式の特徴的な表現に特別な注目を集めるよう処理します。\n",
    "\n",
    "#### 処理の流れ：\n",
    "\n",
    "1. ソクラテス式パターンの定義\n",
    "```python\n",
    "# 特に注目する表現パターン\n",
    "\"かね\"        # 例：「どう思うかね？」\n",
    "\"だろうか\"    # 例：「それは本当だろうか？」\n",
    "\"君は/が/の\"  # 例：「君はどう考える？」\n",
    "```\n",
    "\n",
    "2. パターンの検出と強調\n",
    "```python\n",
    "# 例：「君は幸せについて、どのように考えるかね？」という文で\n",
    "# 「君は」と「かね」を検出して強調\n",
    "\n",
    "# attention_maskの例（1.0が強調部分）\n",
    "[1.0, 1.0, 0.8, 0.8, 0.8, 0.8, 1.0, 1.0]\n",
    "#  ↑    ↑                         ↑    ↑\n",
    "# 「君」「は」        ...        「か」「ね」\n",
    "```\n",
    "\n",
    "3. 特別トークンの処理\n",
    "```python\n",
    "# 文章の開始(BOS)と終了(EOS)トークンも強調\n",
    "if tokenizer.bos_token_id is not None:\n",
    "    new_mask[0] = 1.0  # 開始トークン\n",
    "if tokenizer.eos_token_id is not None:\n",
    "    new_mask[-1] = 1.0  # 終了トークン\n",
    "```\n",
    "\n",
    "この処理により、モデルは：\n",
    "- 「かね」「だろうか」などの質問パターン\n",
    "- 「君は」などの二人称表現\n",
    "- 「では」「について」などの導入表現\n",
    "\n",
    "に特に注目して学習を行います。これにより、ソクラテス式の口調をより効果的に学習することができます。\n",
    "\n",
    "例えば、モデルは以下のような応答パターンを学習しやすくなります：\n",
    "```\n",
    "User: 幸せとは何でしょうか？\n",
    "Model: 君は幸せについて、どのように考えるかね？\n",
    "       ↑    ↑         ↑           ↑\n",
    "       特に注目するパターン\n",
    "```\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 2.5 Data Set Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 2.5 Data Set Preparation\n",
    "# Prepare base dataset\n",
    "dataset = prepare_dataset()\n",
    "logging.info(f\"Total dataset size: {len(dataset)}\")\n",
    "\n",
    "# Validate dataset structure\n",
    "print(\"Dataset structure:\")\n",
    "print(dataset[0])  # Display first element\n",
    "print(\"\\nDataset features:\")\n",
    "print(dataset.features)\n",
    "\n",
    "# Optimize dataset batch processing\n",
    "dataset = dataset.select(range(len(dataset))).shuffle(seed=42)\n",
    "\n",
    "# Optimize dataset processing\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    batch_size=32,  \n",
    "    num_proc=4,     \n",
    "    load_from_cache_file=True,\n",
    "    desc=\"Tokenizing datasets\",\n",
    "    remove_columns=dataset.column_names,\n",
    ")\n",
    "\n",
    "# Apply preprocessing\n",
    "tokenized_dataset = tokenized_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    desc=\"Applying attention masking\"\n",
    ")\n",
    "\n",
    "# Final dataset validation\n",
    "tokenized_dataset = validate_dataset(tokenized_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "\n",
    "\n",
    "この部分は、データセットの最終的な準備段階について説明します。順を追って解説していきます。\n",
    "\n",
    "### 1. 基本データセットの準備と確認\n",
    "```python\n",
    "dataset = prepare_dataset()\n",
    "logging.info(f\"Total dataset size: {len(dataset)}\")\n",
    "\n",
    "# データセット構造の確認\n",
    "print(\"Dataset structure:\")\n",
    "print(dataset[0])  # 最初の要素を表示\n",
    "```\n",
    "\n",
    "例えば以下のような出力が得られます：\n",
    "```python\n",
    "Total dataset size: 5000  # 総対話数\n",
    "\n",
    "Dataset structure:\n",
    "{\n",
    "    'text': '<start_of_turn>user\\n幸せとは何でしょうか？<end_of_turn>\\n<start_of_turn>model\\n君は幸せをどのように定義するかね？<end_of_turn>'\n",
    "}\n",
    "```\n",
    "\n",
    "### 2. データセットのシャッフルと最適化\n",
    "```python\n",
    "dataset = dataset.select(range(len(dataset))).shuffle(seed=42)\n",
    "```\n",
    "\n",
    "- データをランダムに並び替えて、学習の偏りを防ぎます\n",
    "- `seed=42`で、毎回同じシャッフル結果になるようにしています\n",
    "- 例えば：\n",
    "  ```python\n",
    "  # シャッフル前\n",
    "  1. 幸せについての対話\n",
    "  2. 正義についての対話\n",
    "  3. 知識についての対話\n",
    "  \n",
    "  # シャッフル後\n",
    "  1. 知識についての対話\n",
    "  2. 幸せについての対話\n",
    "  3. 正義についての対話\n",
    "  ```\n",
    "\n",
    "### 3. トークン化の実行\n",
    "```python\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    batch_size=32,  # 32個ずつ処理\n",
    "    num_proc=4,     # 4つのプロセスで並列処理\n",
    "    load_from_cache_file=True,  # キャッシュを使用\n",
    "    desc=\"Tokenizing datasets\",\n",
    "    remove_columns=dataset.column_names,\n",
    ")\n",
    "```\n",
    "\n",
    "例えば：\n",
    "```python\n",
    "# 変換前\n",
    "\"君は幸せについて、どのように考えるかね？\"\n",
    "\n",
    "# 変換後\n",
    "{\n",
    "    'input_ids': [1, 245, 15, 987, ...],  # 数値化されたテキスト\n",
    "    'attention_mask': [1, 1, 1, 1, ...]   # 注目する部分\n",
    "}\n",
    "```\n",
    "\n",
    "### 4. ソクラテス式パターンの処理適用\n",
    "```python\n",
    "tokenized_dataset = tokenized_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    desc=\"Applying attention masking\"\n",
    ")\n",
    "```\n",
    "\n",
    "この処理で、ソクラテス式の特徴的な表現により注目するように設定します：\n",
    "```python\n",
    "# 例：「君は幸せについて、どのように考えるかね？」\n",
    "\n",
    "# attention_maskの値\n",
    "[1.0, 1.0, 0.8, 0.8, 0.8, 0.8, 1.0, 1.0]\n",
    "#  ↑    ↑                         ↑    ↑\n",
    "# 「君」「は」        ...        「か」「ね」\n",
    "# （1.0は特に注目する部分）\n",
    "```\n",
    "\n",
    "### 5. 最終確認\n",
    "```python\n",
    "tokenized_dataset = validate_dataset(tokenized_dataset)\n",
    "```\n",
    "\n",
    "最終的なデータセットの構造を確認します：\n",
    "```python\n",
    "Validated first item structure:\n",
    "Keys: dict_keys(['input_ids', 'attention_mask'])\n",
    "input_ids type: <class 'list'>\n",
    "input_ids length: 256\n",
    "```\n",
    "\n",
    "この一連の処理により、生の対話データが：\n",
    "1. 適切にシャッフルされ\n",
    "2. 数値化され\n",
    "3. ソクラテス式の特徴が強調され\n",
    "4. 効率的に処理できる形に整理された\n",
    "\n",
    "最終的なデータセットが完成します。これにより、モデルがソクラテス式の対話パターンを効果的に学習できる準備が整います。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
