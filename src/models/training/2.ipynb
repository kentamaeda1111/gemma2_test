{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. データパイプライン"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "### 2.1 トークナイザー設定\n",
    "# Model and tokenizer preparation\n",
    "model_name = \"google/gemma-2-2b-jpn-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    token=os.environ[\"HUGGINGFACE_TOKEN\"],  \n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "\n",
    "# Add special tokens to tokenizer\n",
    "tokenizer.add_special_tokens({\n",
    "    'additional_special_tokens': [\n",
    "        '。', '、', '！', '？',  # Punctuation marks\n",
    "    ]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "このコードについて、ソクラテス式チャットボットの文脈で説明させていただきます。\n",
    "\n",
    "### 1. モデルとトークナイザーの準備\n",
    "\n",
    "```python\n",
    "model_name = \"google/gemma-2-2b-jpn-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    token=os.environ[\"HUGGINGFACE_TOKEN\"],  \n",
    "    trust_remote_code=True\n",
    ")\n",
    "```\n",
    "\n",
    "このコードは、テキストを処理するための「トークナイザー」を設定しています。トークナイザーとは、文章を機械が理解できる形に変換するツールです。\n",
    "\n",
    "具体例を挙げると：\n",
    "- 入力：「なぜそう考えるのですか？」\n",
    "- トークナイザーの処理：「なぜ」「そう」「考える」「の」「です」「か」「？」のように分割\n",
    "\n",
    "ここでは、日本語に特化した Google の Gemma モデルのトークナイザーを使用しています。\n",
    "\n",
    "### 2. 特殊トークンの追加\n",
    "\n",
    "```python\n",
    "tokenizer.add_special_tokens({\n",
    "    'additional_special_tokens': [\n",
    "        '。', '、', '！', '？',  # Punctuation marks\n",
    "    ]\n",
    "})\n",
    "```\n",
    "\n",
    "この部分は、日本語の重要な句読点をトークナイザーに特別なトークンとして認識させています。\n",
    "\n",
    "ソクラテス式対話での具体例：\n",
    "```\n",
    "入力文：「その考えは興味深いですね。なぜそう思ったのですか？」\n",
    "\n",
    "通常の処理：\n",
    "「その」「考え」「は」「興味」「深い」「です」「ね」「。」「なぜ」「そう」「思った」「の」「です」「か」「？」\n",
    "\n",
    "特殊トークン追加により：\n",
    "- 「。」と「？」が特別な意味を持つトークンとして認識される\n",
    "- これにより、文の区切りや疑問文の特徴をAIがより正確に理解できる\n",
    "```\n",
    "\n",
    "この設定は特に重要です。なぜなら：\n",
    "1. ソクラテス式対話では、質問文（「？」）が頻繁に使用される\n",
    "2. 文の区切り（「。」）で、一つの問いかけや考えが完結する\n",
    "3. 「！」は驚きや気づきを表現する際に使用される\n",
    "4. 「、」は、考えを整理して話を進める際の区切りとして重要\n",
    "\n",
    "これらの句読点を特別に認識することで、AIがより自然なソクラテス式の対話を生成できるようになります。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "### 2.2 データセット準備\n",
    "def validate_message_format(message):\n",
    "    \"\"\"Validate message format\"\"\"\n",
    "    if not isinstance(message, dict):\n",
    "        return False\n",
    "    if 'role' not in message or 'content' not in message:\n",
    "        return False\n",
    "    if message['role'] not in ['user', 'model']:\n",
    "        return False\n",
    "    if not isinstance(message['content'], str):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def prepare_dataset():\n",
    "    conversations = []\n",
    "    \n",
    "    try:\n",
    "        with open(DIALOGUE_JSON_PATH, 'r', encoding='utf-8') as f:\n",
    "            dialogue_data = json.load(f)\n",
    "            \n",
    "        for dialogue in dialogue_data:\n",
    "            messages = dialogue.get('messages', [])\n",
    "            \n",
    "            # Validate message format\n",
    "            if not all(validate_message_format(msg) for msg in messages):\n",
    "                logging.warning(f\"Skipped dialogue due to invalid message format\")\n",
    "                continue\n",
    "                \n",
    "            # Build conversation checking user->model sequence\n",
    "            current_conversation = []\n",
    "            valid_sequence = True\n",
    "            \n",
    "            for i in range(0, len(messages)-1, 2):\n",
    "                if (i+1 < len(messages) and \n",
    "                    messages[i]['role'] == 'user' and \n",
    "                    messages[i+1]['role'] == 'model'):\n",
    "                    current_conversation.extend([messages[i], messages[i+1]])\n",
    "                else:\n",
    "                    valid_sequence = False\n",
    "                    break\n",
    "            \n",
    "            # Add only valid conversations\n",
    "            if valid_sequence and current_conversation:\n",
    "                # Apply Gemma chat template\n",
    "                formatted_text = tokenizer.apply_chat_template(\n",
    "                    current_conversation,\n",
    "                    tokenize=False,\n",
    "                    add_generation_prompt=True\n",
    "                )\n",
    "                \n",
    "                # Check token count\n",
    "                tokens = tokenizer.encode(formatted_text)\n",
    "                if len(tokens) <= MAX_SEQUENCE_LENGTH:\n",
    "                    conversations.append({\"text\": formatted_text})\n",
    "                else:\n",
    "                    logging.warning(f\"Skipped conversation due to length: {len(tokens)} tokens\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing dialogue file: {str(e)}\")\n",
    "        raise\n",
    "    \n",
    "    if not conversations:\n",
    "        raise ValueError(\"No valid conversations found in the dialogue file\")\n",
    "        \n",
    "    logging.info(f\"Processed {len(conversations)} valid conversations\")\n",
    "    return Dataset.from_list(conversations)\n",
    "dataset = prepare_dataset()\n",
    "\n",
    "# Check dataset structure\n",
    "print(\"Dataset structure:\")\n",
    "print(dataset[0])  # Display first element\n",
    "print(\"\\nDataset features:\")\n",
    "print(dataset.features)\n",
    "dataset = dataset.select(range(len(dataset))).shuffle(seed=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "このコードについて、ソクラテス式チャットボットの文脈で説明させていただきます。\n",
    "\n",
    "### 1. メッセージフォーマットの検証\n",
    "```python\n",
    "def validate_message_format(message):\n",
    "    \"\"\"Validate message format\"\"\"\n",
    "    if not isinstance(message, dict):\n",
    "        return False\n",
    "    if 'role' not in message or 'content' not in message:\n",
    "        return False\n",
    "    if message['role'] not in ['user', 'model']:\n",
    "        return False\n",
    "    if not isinstance(message['content'], str):\n",
    "        return False\n",
    "    return True\n",
    "```\n",
    "\n",
    "この関数は、対話データが正しい形式かどうかをチェックします。\n",
    "\n",
    "具体例：\n",
    "```python\n",
    "# 正しい形式：\n",
    "{\n",
    "    'role': 'user',\n",
    "    'content': 'なぜ哲学は重要だと思いますか？'\n",
    "}\n",
    "\n",
    "# 間違った形式：\n",
    "{\n",
    "    'speaker': 'user',  # 'role'ではないのでエラー\n",
    "    'text': '哲学について考えましょう'  # 'content'ではないのでエラー\n",
    "}\n",
    "```\n",
    "\n",
    "### 2. データセットの準備\n",
    "```python\n",
    "def prepare_dataset():\n",
    "    conversations = []\n",
    "    \n",
    "    try:\n",
    "        with open(DIALOGUE_JSON_PATH, 'r', encoding='utf-8') as f:\n",
    "            dialogue_data = json.load(f)\n",
    "```\n",
    "\n",
    "この部分は対話データをJSONファイルから読み込みます。\n",
    "\n",
    "### 3. 対話の検証とフォーマット\n",
    "```python\n",
    "for dialogue in dialogue_data:\n",
    "    messages = dialogue.get('messages', [])\n",
    "    \n",
    "    # メッセージ形式の検証\n",
    "    if not all(validate_message_format(msg) for msg in messages):\n",
    "        logging.warning(f\"Skipped dialogue due to invalid message format\")\n",
    "        continue\n",
    "```\n",
    "\n",
    "各対話が以下のような形式になっているか確認します：\n",
    "\n",
    "```python\n",
    "# 正しい対話の例：\n",
    "{\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"幸せとは何だと思いますか？\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"model\",\n",
    "            \"content\": \"その質問は興味深いですね。あなたにとって幸せとは何でしょうか？\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "### 4. 対話順序の確認\n",
    "```python\n",
    "for i in range(0, len(messages)-1, 2):\n",
    "    if (i+1 < len(messages) and \n",
    "        messages[i]['role'] == 'user' and \n",
    "        messages[i+1]['role'] == 'model'):\n",
    "        current_conversation.extend([messages[i], messages[i+1]])\n",
    "```\n",
    "\n",
    "この部分は、対話が「ユーザーの質問→モデルの応答」という順序になっているか確認します。\n",
    "\n",
    "例：\n",
    "```python\n",
    "# 正しい順序：\n",
    "1. ユーザー: \"知識とは何でしょうか？\"\n",
    "2. モデル: \"その問いについて一緒に考えてみましょう。あなたは知識をどのように定義しますか？\"\n",
    "3. ユーザー: \"自分が確実に理解していることだと思います\"\n",
    "4. モデル: \"なるほど。では、確実な理解とは何を指すのでしょうか？\"\n",
    "\n",
    "# 間違った順序（スキップされる）：\n",
    "1. モデル: \"こんにちは\"\n",
    "2. ユーザー: \"こんにちは\"\n",
    "```\n",
    "\n",
    "### 5. 対話の長さチェックと保存\n",
    "```python\n",
    "tokens = tokenizer.encode(formatted_text)\n",
    "if len(tokens) <= MAX_SEQUENCE_LENGTH:\n",
    "    conversations.append({\"text\": formatted_text})\n",
    "else:\n",
    "    logging.warning(f\"Skipped conversation due to length: {len(tokens)} tokens\")\n",
    "```\n",
    "\n",
    "この部分は、対話が適切な長さ（MAX_SEQUENCE_LENGTH以下）かどうかをチェックします。長すぎる対話は学習データから除外されます。\n",
    "\n",
    "最後に、検証を通過した対話データをシャッフルして、学習用データセットとして準備します：\n",
    "```python\n",
    "dataset = dataset.select(range(len(dataset))).shuffle(seed=42)\n",
    "```\n",
    "\n",
    "これにより、ソクラテス式の対話パターンを学習するための質の高いデータセットが準備されます。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "### 2.3 データ前処理と検証\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    result = tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        max_length=TOKENIZE_MAX_LENGTH,      # 256 から TOKENIZE_MAX_LENGTH に変更\n",
    "        padding='max_length',\n",
    "        add_special_tokens=True,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    return result\n",
    "\n",
    "# Add dataset preprocessing\n",
    "def preprocess_function(examples):\n",
    "    # Pattern definitions\n",
    "    end_patterns = [\n",
    "        \"だろうか\", \"ではないか\", \"のではないか\", \"かね\",\n",
    "        \"なるほど\", \"興味深い\", \"考えてみよう\"\n",
    "    ]\n",
    "    \n",
    "    # Conjunction patterns\n",
    "    conjunctions = [\n",
    "        \"しかし\", \"だから\", \"それでは\", \"すなわち\",\n",
    "        \"たとえば\", \"つまり\", \"ならば\", \"もし\"\n",
    "    ]\n",
    "    \n",
    "    # Get tokenized texts\n",
    "    texts = tokenizer.batch_decode(examples['input_ids'])\n",
    "    new_attention_masks = []\n",
    "    \n",
    "    for text, mask in zip(texts, examples['attention_mask']):\n",
    "        if not isinstance(mask, list):\n",
    "            mask = mask.tolist()\n",
    "        \n",
    "        # Create new attention mask (base value 0.8)\n",
    "        new_mask = [0.8] * len(mask)\n",
    "        \n",
    "        # Split into sentences\n",
    "        sentences = text.split('。')\n",
    "        current_pos = 0\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            if not sentence.strip():\n",
    "                continue\n",
    "                \n",
    "            # Detect and emphasize end patterns\n",
    "            for pattern in end_patterns:\n",
    "                if pattern in sentence:\n",
    "                    # Locate pattern position\n",
    "                    pattern_tokens = tokenizer.encode(pattern, add_special_tokens=False)\n",
    "                    pattern_len = len(pattern_tokens)\n",
    "                    \n",
    "                    # Emphasize tokens containing pattern and surrounding tokens\n",
    "                    pattern_start = current_pos + len(tokenizer.encode(sentence, add_special_tokens=False)) - pattern_len\n",
    "                    for i in range(max(0, pattern_start - 2), min(len(mask), pattern_start + pattern_len + 2)):\n",
    "                        new_mask[i] = 1.0  # Maximum attention for pattern parts\n",
    "            \n",
    "            # Detect and emphasize conjunctions\n",
    "            for conj in conjunctions:\n",
    "                if conj in sentence:\n",
    "                    # Locate conjunction position\n",
    "                    conj_tokens = tokenizer.encode(conj, add_special_tokens=False)\n",
    "                    conj_pos = len(tokenizer.encode(sentence.split(conj)[0], add_special_tokens=False))\n",
    "                    \n",
    "                    # Emphasize tokens before and after conjunction (slightly lower)\n",
    "                    for i in range(max(0, current_pos + conj_pos - 1), \n",
    "                                 min(len(mask), current_pos + conj_pos + len(conj_tokens) + 1)):\n",
    "                        new_mask[i] = 0.9\n",
    "            \n",
    "            # Emphasize punctuation marks\n",
    "            for i, char in enumerate(sentence):\n",
    "                if char in '、。！？':\n",
    "                    # Locate punctuation position\n",
    "                    punct_pos = len(tokenizer.encode(sentence[:i], add_special_tokens=False))\n",
    "                    # Emphasize tokens around punctuation\n",
    "                    for j in range(max(0, current_pos + punct_pos - 1),\n",
    "                                 min(len(mask), current_pos + punct_pos + 2)):\n",
    "                        new_mask[j] = 0.95\n",
    "            \n",
    "            # Update position for next sentence\n",
    "            current_pos += len(tokenizer.encode(sentence + '。', add_special_tokens=False))\n",
    "        \n",
    "        # Set special token masks to 1.0\n",
    "        if tokenizer.bos_token_id is not None:\n",
    "            new_mask[0] = 1.0  # BOS token\n",
    "        if tokenizer.eos_token_id is not None:\n",
    "            new_mask[-1] = 1.0  # EOS token\n",
    "            \n",
    "        new_attention_masks.append(new_mask)\n",
    "\n",
    "    examples['attention_mask'] = new_attention_masks\n",
    "    return examples\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "\n",
    "\n",
    "このコードについて、ソクラテス式チャットボットの文脈で説明させていただきます。\n",
    "\n",
    "### 1. トークン化関数\n",
    "```python\n",
    "def tokenize_function(examples):\n",
    "    result = tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        max_length=TOKENIZE_MAX_LENGTH,\n",
    "        padding='max_length',\n",
    "        add_special_tokens=True,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    return result\n",
    "```\n",
    "\n",
    "これは対話テキストをトークン（機械が理解できる単位）に変換する関数です。\n",
    "\n",
    "例：\n",
    "```python\n",
    "入力テキスト：\n",
    "\"なぜそう考えるのですか？\"\n",
    "\n",
    "トークン化後：\n",
    "[BOS]  # 文章開始トークン\n",
    "\"なぜ\" \"そう\" \"考える\" \"の\" \"です\" \"か\" \"？\"\n",
    "[EOS]  # 文章終了トークン\n",
    "```\n",
    "\n",
    "### 2. 前処理関数\n",
    "```python\n",
    "def preprocess_function(examples):\n",
    "    # パターン定義\n",
    "    end_patterns = [\n",
    "        \"だろうか\", \"ではないか\", \"のではないか\", \"かね\",\n",
    "        \"なるほど\", \"興味深い\", \"考えてみよう\"\n",
    "    ]\n",
    "    \n",
    "    conjunctions = [\n",
    "        \"しかし\", \"だから\", \"それでは\", \"すなわち\",\n",
    "        \"たとえば\", \"つまり\", \"ならば\", \"もし\"\n",
    "    ]\n",
    "```\n",
    "\n",
    "この関数は、ソクラテス式対話に特徴的な表現パターンに注目して、それらにより強い注意を払うように設定します。\n",
    "\n",
    "### 3. アテンションマスクの作成\n",
    "```python\n",
    "# Create new attention mask (base value 0.8)\n",
    "new_mask = [0.8] * len(mask)\n",
    "```\n",
    "\n",
    "アテンションマスクとは、モデルがテキストのどの部分により注目すべきかを示す値です：\n",
    "- 基本値: 0.8（通常の注意度）\n",
    "- 1.0（最大注意）\n",
    "- 0.9（やや強い注意）\n",
    "- 0.95（強い注意）\n",
    "\n",
    "### 4. 重要表現の強調\n",
    "例えば以下のような対話で：\n",
    "```\n",
    "「なぜそう考えるのでしょうか？ なるほど、興味深い視点ですね。しかし、それについてもう少し考えてみましょう。」\n",
    "```\n",
    "\n",
    "各部分の注目度が以下のように設定されます：\n",
    "\n",
    "1. 文末パターン（1.0の最大注意）：\n",
    "```python\n",
    "# \"なるほど\", \"興味深い\", \"考えてみよう\" など\n",
    "pattern_start = current_pos + len(tokenizer.encode(sentence, add_special_tokens=False)) - pattern_len\n",
    "for i in range(max(0, pattern_start - 2), min(len(mask), pattern_start + pattern_len + 2)):\n",
    "    new_mask[i] = 1.0\n",
    "```\n",
    "\n",
    "2. 接続詞（0.9のやや強い注意）：\n",
    "```python\n",
    "# \"しかし\", \"だから\", \"すなわち\" など\n",
    "for i in range(max(0, current_pos + conj_pos - 1), \n",
    "             min(len(mask), current_pos + conj_pos + len(conj_tokens) + 1)):\n",
    "    new_mask[i] = 0.9\n",
    "```\n",
    "\n",
    "3. 句読点（0.95の強い注意）：\n",
    "```python\n",
    "# \"、\", \"。\", \"！\", \"？\"\n",
    "for j in range(max(0, current_pos + punct_pos - 1),\n",
    "             min(len(mask), current_pos + punct_pos + 2)):\n",
    "    new_mask[j] = 0.95\n",
    "```\n",
    "\n",
    "この設定により、モデルは：\n",
    "1. ソクラテス式の問いかけ表現（「だろうか」「ではないか」など）\n",
    "2. 論理的な接続（「しかし」「つまり」など）\n",
    "3. 文の区切り（句読点）\n",
    "\n",
    "に特に注意を払いながら学習を行い、より自然なソクラテス式の対話を生成できるようになります。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "### 2.4 データセット最適化\n",
    "# Optimize dataset processing\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    batch_size=16,\n",
    "    num_proc=2,\n",
    "    load_from_cache_file=True,\n",
    "    desc=\"Tokenizing datasets\",\n",
    "    remove_columns=dataset.column_names,\n",
    ")\n",
    "\n",
    "# Add dataset validation\n",
    "def validate_dataset(dataset):\n",
    "    # Check first element\n",
    "    first_item = dataset[0]\n",
    "    print(\"Validated first item structure:\")\n",
    "    print(f\"Keys: {first_item.keys()}\")\n",
    "    print(f\"input_ids type: {type(first_item['input_ids'])}\")\n",
    "    print(f\"input_ids length: {len(first_item['input_ids'])}\")\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "tokenized_dataset = validate_dataset(tokenized_dataset)\n",
    "\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    desc=\"Applying attention masking\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "\n",
    "\n",
    "このコードについて、ソクラテス式チャットボットの文脈で説明させていただきます。\n",
    "\n",
    "### 1. データセットのトークン化処理\n",
    "```python\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    batch_size=16,\n",
    "    num_proc=2,\n",
    "    load_from_cache_file=True,\n",
    "    desc=\"Tokenizing datasets\",\n",
    "    remove_columns=dataset.column_names,\n",
    ")\n",
    "```\n",
    "\n",
    "これは対話データを効率的にトークン化する処理です。\n",
    "\n",
    "具体的な設定：\n",
    "- `batched=True, batch_size=16`: 16個の対話を一度に処理\n",
    "- `num_proc=2`: 2つのプロセスで並列処理\n",
    "- `load_from_cache_file=True`: 処理結果をキャッシュして再利用可能に\n",
    "\n",
    "例えば：\n",
    "```python\n",
    "# 入力データ\n",
    "[\n",
    "    \"哲学とは何でしょうか？\",\n",
    "    \"その質問は本質を突いていますね。あなたは哲学をどのように考えますか？\",\n",
    "    # ... 他の対話 ...\n",
    "]\n",
    "\n",
    "# トークン化後\n",
    "[\n",
    "    [1, 345, 67, 89, 2],  # 数値化されたトークン\n",
    "    [1, 234, 56, 78, 90, 2],\n",
    "    # ... 他の対話のトークン ...\n",
    "]\n",
    "```\n",
    "\n",
    "### 2. データセットの検証\n",
    "```python\n",
    "def validate_dataset(dataset):\n",
    "    first_item = dataset[0]\n",
    "    print(\"Validated first item structure:\")\n",
    "    print(f\"Keys: {first_item.keys()}\")\n",
    "    print(f\"input_ids type: {type(first_item['input_ids'])}\")\n",
    "    print(f\"input_ids length: {len(first_item['input_ids'])}\")\n",
    "    return dataset\n",
    "```\n",
    "\n",
    "この関数は、データセットが正しい形式になっているか確認します。\n",
    "\n",
    "出力例：\n",
    "```python\n",
    "Validated first item structure:\n",
    "Keys: dict_keys(['input_ids', 'attention_mask'])\n",
    "input_ids type: <class 'list'>\n",
    "input_ids length: 512  # 設定された最大長\n",
    "```\n",
    "\n",
    "### 3. アテンションマスクの適用\n",
    "```python\n",
    "tokenized_dataset = tokenized_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    desc=\"Applying attention masking\"\n",
    ")\n",
    "```\n",
    "\n",
    "これは先ほど説明した重み付けを対話データに適用します。\n",
    "\n",
    "例えば、以下のような対話：\n",
    "```python\n",
    "\"なぜそう考えるのでしょうか？（注目度1.0）\n",
    "なるほど（注目度1.0）、\n",
    "興味深い（注目度1.0）視点ですね。\n",
    "しかし（注目度0.9）、\n",
    "それについてもう少し考えてみましょう（注目度1.0）。\"\n",
    "```\n",
    "\n",
    "このように、ソクラテス式対話の重要な要素（問いかけ、応答、接続詞など）に適切な注目度が設定され、モデルがこれらのパターンをより効果的に学習できるようになります。\n",
    "\n",
    "この最適化により：\n",
    "1. 効率的なデータ処理（並列処理、キャッシュ利用）\n",
    "2. データ形式の検証（エラー防止）\n",
    "3. ソクラテス式対話に特化した重み付け\n",
    "\n",
    "が実現され、質の高い学習データセットが準備されます。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
