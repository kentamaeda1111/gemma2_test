{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. トレーニングインフラ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "### 4.1 評価メトリクス定義\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds  # Get logits and labels from eval_preds\n",
    "    \n",
    "    # Relax size limit for evaluation dataset\n",
    "    max_samples = 100\n",
    "    \n",
    "    # Improve decoding process\n",
    "    with torch.no_grad():\n",
    "        logits = torch.tensor(logits).cpu()\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        # Decode batch\n",
    "        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "        \n",
    "        # Add more detailed logging\n",
    "        logging.info(f\"Sample prediction: {decoded_preds[0][:100]}...\")\n",
    "        \n",
    "        del logits, predictions  # Memory release\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Define sentence ending patterns more flexibly\n",
    "        sentence_end_patterns = {\n",
    "            'question_patterns': [\n",
    "                'かね', 'だろうか', 'ではないか',\n",
    "                'のか', 'と思わないか', '考えてみよう',\n",
    "            ],\n",
    "            'statement_patterns': [\n",
    "                'だね', 'なるほど', '興味深い',\n",
    "                'といえよう', 'というべきだ'\n",
    "            ],\n",
    "            'reflection_patterns': [\n",
    "                'かもしれない', 'のではないか',\n",
    "                'と考えられる', 'といえそうだ'\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Auxiliary verb patterns\n",
    "        auxiliary_patterns = [\n",
    "            'である', 'だ', 'です', 'ます',\n",
    "            'のだ', 'のです', 'のである'\n",
    "        ]\n",
    "        \n",
    "        def calculate_style_consistency(text):\n",
    "            sentences = text.split('。')\n",
    "            if not sentences:\n",
    "                return 0.0\n",
    "                \n",
    "            # Evaluate sentence ending style consistency\n",
    "            end_style_scores = []\n",
    "            for sent in sentences:\n",
    "                if not sent.strip():\n",
    "                    continue\n",
    "                    \n",
    "                # Evaluate sentence ending patterns (partial match allowed)\n",
    "                pattern_found = False\n",
    "                for pattern_type, patterns in sentence_end_patterns.items():\n",
    "                    if any(p in sent[-10:] for p in patterns):  # Search within 10 characters at the end\n",
    "                        pattern_found = True\n",
    "                        break\n",
    "                end_style_scores.append(1.0 if pattern_found else 0.0)\n",
    "            \n",
    "            # Evaluate auxiliary verb consistency\n",
    "            aux_style_scores = []\n",
    "            for sent in sentences:\n",
    "                if not sent.strip():\n",
    "                    continue\n",
    "                    \n",
    "                # Evaluate auxiliary verb usage in the sentence\n",
    "                aux_found = any(p in sent for p in auxiliary_patterns)\n",
    "                aux_style_scores.append(1.0 if aux_found else 0.0)\n",
    "            \n",
    "            # Evaluate sentence length consistency\n",
    "            lengths = [len(s.strip()) for s in sentences if s.strip()]\n",
    "            length_variance = np.var(lengths) if lengths else 0\n",
    "            length_score = 1.0 / (1.0 + length_variance/100)  # Higher score if variance is small\n",
    "            \n",
    "            # Overall evaluation\n",
    "            end_style_avg = np.mean(end_style_scores) if end_style_scores else 0\n",
    "            aux_style_avg = np.mean(aux_style_scores) if aux_style_scores else 0\n",
    "            \n",
    "            # Weighting\n",
    "            weights = {\n",
    "                'end_style': 0.5,\n",
    "                'aux_style': 0.3,\n",
    "                'length_consistency': 0.2\n",
    "            }\n",
    "            \n",
    "            return (\n",
    "                weights['end_style'] * end_style_avg +\n",
    "                weights['aux_style'] * aux_style_avg +\n",
    "                weights['length_consistency'] * length_score\n",
    "            )\n",
    "        \n",
    "        # Evaluate style consistency for each prediction\n",
    "        style_scores = [calculate_style_consistency(pred) for pred in decoded_preds]\n",
    "        \n",
    "        # Evaluate dialogue flow\n",
    "        def calculate_dialogue_flow(text):\n",
    "            sentences = text.split('。')\n",
    "            if not sentences:\n",
    "                return 0.0\n",
    "            \n",
    "            # 質問文判定の改善\n",
    "            question_markers = {\n",
    "                'explicit': ['？', '?'],  # 明示的な質問符号\n",
    "                'patterns': [\n",
    "                    'かね', 'だろうか', 'ではないか', 'のか', \n",
    "                    'と思わないか', '考えてみよう',\n",
    "                    'どう', 'いかが', 'なぜ', 'どのように',\n",
    "                    '問', '教えて', '聞かせて'\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "            def is_question(sentence):\n",
    "                # 明示的な質問符号のチェック\n",
    "                if any(marker in sentence for marker in question_markers['explicit']):\n",
    "                    return True\n",
    "                # 質問パターンのチェック\n",
    "                if any(pattern in sentence for pattern in question_markers['patterns']):\n",
    "                    return True\n",
    "                return False\n",
    "            \n",
    "            # 各文を評価\n",
    "            questions = sum(1 for s in sentences if is_question(s))\n",
    "            total_sentences = len([s for s in sentences if s.strip()])\n",
    "            ratio = questions / total_sentences if total_sentences else 0\n",
    "            \n",
    "            # 理想の比率(0.3)からの距離に基づいてスコアを計算\n",
    "            balance_score = max(0.0, 1.0 - min(abs(0.3 - ratio), 0.2) * 2)\n",
    "            \n",
    "            # 2. Sentence length change\n",
    "            lengths = [len(s.strip()) for s in sentences if s.strip()]\n",
    "            length_variance = np.var(lengths) if len(lengths) > 1 else 0\n",
    "            length_score = 1.0 / (1.0 + length_variance/500)  # Higher score if variance is small\n",
    "            \n",
    "            # 3. Use of conjunctions\n",
    "            conjunctions = ['しかし', 'だから', 'また', 'そして', 'したがって']\n",
    "            conj_count = sum(1 for s in sentences if any(c in s for c in conjunctions))\n",
    "            conj_ratio = conj_count / len(sentences)\n",
    "            conj_score = min(1.0, conj_ratio * 2)  # Evaluate moderate usage\n",
    "            \n",
    "            # Weighted average of scores\n",
    "            weights = [0.5, 0.25, 0.25]  # Balance, length, conjunction weights\n",
    "            final_score = sum(s * w for s, w in zip([balance_score, length_score, conj_score], weights))\n",
    "            \n",
    "            return max(0.1, min(1.0, final_score))  # Limit to range 0.1 to 1.0\n",
    "        \n",
    "        flow_scores = [calculate_dialogue_flow(pred) for pred in decoded_preds]\n",
    "        \n",
    "        style_score = np.mean(style_scores)\n",
    "        flow_score = np.mean(flow_scores)\n",
    "        \n",
    "        # Add overall evaluation score\n",
    "        combined_score = (style_score * 0.6 + flow_score * 0.4)  # Increase flow_score weight\n",
    "        \n",
    "        return {\n",
    "            'style_consistency': style_score,\n",
    "            'dialogue_flow': flow_score,\n",
    "            'combined_score': combined_score\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "このコードは、ソクラテス式の対話スタイルがどれだけ上手く実現できているかを評価するための仕組みを定義しています。以下に分かりやすく説明します：\n",
    "\n",
    "## 1. 全体の目的\n",
    "このコードは、モデルが生成した文章を評価して、以下の3つのスコアを計算します：\n",
    "- スタイルの一貫性（style_consistency）\n",
    "- 対話の流れ（dialogue_flow）\n",
    "- 総合評価（combined_score）\n",
    "\n",
    "## 2. スタイルの一貫性の評価\n",
    "### 評価する3つのポイント：\n",
    "\n",
    "1. **文末表現のパターン**\n",
    "```python\n",
    "sentence_end_patterns = {\n",
    "    'question_patterns': ['かね', 'だろうか', 'ではないか'...],\n",
    "    'statement_patterns': ['だね', 'なるほど', '興味深い'...],\n",
    "    'reflection_patterns': ['かもしれない', 'のではないか'...]\n",
    "}\n",
    "```\n",
    "例：\n",
    "- 良い例：「その考えはどうだろうか」「なるほど、興味深い視点ですね」\n",
    "- 悪い例：「はい」「いいえ」（ソクラテス式らしくない単純な返答）\n",
    "\n",
    "2. **助動詞の使用**\n",
    "```python\n",
    "auxiliary_patterns = ['である', 'だ', 'です', 'ます'...]\n",
    "```\n",
    "例：\n",
    "- 良い例：「それは本当である」「そう考えられるのです」\n",
    "- 悪い例：「うん」「そう」（口語的すぎる表現）\n",
    "\n",
    "3. **文の長さの一貫性**\n",
    "- 文の長さがバラバラすぎないかをチェック\n",
    "例：\n",
    "- 良い例：「それはなぜだろうか。その理由について考えてみましょう。」（バランスの取れた文長）\n",
    "- 悪い例：「なぜ？」「それについて詳しく説明すると、哲学的な観点から見た場合、様々な解釈が可能となり...」（極端に短い文と長い文が混在）\n",
    "\n",
    "## 3. 対話の流れの評価\n",
    "### 評価する3つのポイント：\n",
    "\n",
    "1. **質問と説明のバランス**\n",
    "- 理想的な質問の割合は30%程度\n",
    "例：\n",
    "- 良い例：「その点について深く考えてみましょう。なぜそう考えるのでしょうか？」\n",
    "- 悪い例：「それはなぜですか？どう思いますか？どうしてそう考えましたか？」（質問が多すぎる）\n",
    "\n",
    "2. **文の長さの変化**\n",
    "- 文の長さに極端な変化がないかチェック\n",
    "\n",
    "3. **接続詞の使用**\n",
    "```python\n",
    "conjunctions = ['しかし', 'だから', 'また', 'そして', 'したがって']\n",
    "```\n",
    "例：\n",
    "- 良い例：「なるほど。しかし、その考えには別の視点もありそうですね。」\n",
    "- 悪い例：「そうですね。そうですね。そうですね。」（接続詞がなく単調）\n",
    "\n",
    "## 4. 最終スコアの計算\n",
    "```python\n",
    "combined_score = (style_score * 0.6 + flow_score * 0.4)\n",
    "```\n",
    "- スタイルの一貫性（60%）と対話の流れ（40%）を組み合わせて最終的な評価を行います\n",
    "- スコアは0.0〜1.0の範囲で、1.0が最高評価です\n",
    "\n",
    "このように、ソクラテス式の対話らしさを数値化して評価することで、モデルの学習がうまくいっているかを判断できます。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "### 4.2 メモリ管理とモニタリング\n",
    "def log_memory_usage():\n",
    "    import psutil\n",
    "    import torch\n",
    "    \n",
    "    # CPU memory\n",
    "    process = psutil.Process()\n",
    "    cpu_memory = process.memory_info().rss / 1024 / 1024  # MB\n",
    "    \n",
    "    # GPU memory\n",
    "    gpu_memory = []\n",
    "    if torch.cuda.is_available():\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            gpu_memory.append({\n",
    "                'device': i,\n",
    "                'allocated': torch.cuda.memory_allocated(i) / 1024 / 1024,  # MB\n",
    "                'reserved': torch.cuda.memory_reserved(i) / 1024 / 1024,    # MB\n",
    "                'max_allocated': torch.cuda.max_memory_allocated(i) / 1024 / 1024  # MB\n",
    "            })\n",
    "    \n",
    "    logging.info(f\"CPU Memory usage: {cpu_memory:.2f} MB\")\n",
    "    for gpu in gpu_memory:\n",
    "        logging.info(f\"GPU {gpu['device']} Memory:\")\n",
    "        logging.info(f\"  - Allocated: {gpu['allocated']:.2f} MB\")\n",
    "        logging.info(f\"  - Reserved: {gpu['reserved']:.2f} MB\")\n",
    "        logging.info(f\"  - Max Allocated: {gpu['max_allocated']:.2f} MB\")\n",
    "\n",
    "# Log dataset size\n",
    "logging.info(f\"Total dataset size: {len(dataset)}\")\n",
    "log_memory_usage()\n",
    "\n",
    "def clear_memory():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "このコードはメモリの使用状況を監視・管理するための機能を実装しています。以下に分かりやすく説明します：\n",
    "\n",
    "## 1. メモリ使用状況のログ記録（log_memory_usage関数）\n",
    "\n",
    "### CPUメモリの監視\n",
    "```python\n",
    "process = psutil.Process()\n",
    "cpu_memory = process.memory_info().rss / 1024 / 1024  # MB\n",
    "```\n",
    "- プログラムが使用しているCPUメモリ量を測定\n",
    "- MBに変換して記録（例：「CPU Memory usage: 4562.34 MB」）\n",
    "\n",
    "### GPUメモリの監視\n",
    "```python\n",
    "gpu_memory.append({\n",
    "    'device': i,\n",
    "    'allocated': torch.cuda.memory_allocated(i) / 1024 / 1024,  # MB\n",
    "    'reserved': torch.cuda.memory_reserved(i) / 1024 / 1024,    # MB\n",
    "    'max_allocated': torch.cuda.max_memory_allocated(i) / 1024 / 1024  # MB\n",
    "})\n",
    "```\n",
    "\n",
    "3種類のGPUメモリ情報を記録：\n",
    "1. **allocated（割り当て済み）**: 現在実際に使用中のメモリ\n",
    "   - 例：「GPU 0 Memory: Allocated: 2048.00 MB」\n",
    "\n",
    "2. **reserved（予約済み）**: 確保されているが未使用のメモリ\n",
    "   - 例：「GPU 0 Memory: Reserved: 3072.00 MB」\n",
    "\n",
    "3. **max_allocated（最大割り当て）**: プログラム開始からの最大使用量\n",
    "   - 例：「GPU 0 Memory: Max Allocated: 4096.00 MB」\n",
    "\n",
    "## 2. メモリのクリーンアップ（clear_memory関数）\n",
    "\n",
    "```python\n",
    "def clear_memory():\n",
    "    gc.collect()           # Pythonのガベージコレクションを実行\n",
    "    torch.cuda.empty_cache() # GPUのキャッシュをクリア\n",
    "```\n",
    "\n",
    "このクリーンアップが重要な理由：\n",
    "- ソクラテス式対話モデルの学習では大量のメモリを使用\n",
    "- メモリリークを防ぎ、長時間の学習を安定して行える\n",
    "- 例：長い対話履歴を処理した後のメモリ解放\n",
    "\n",
    "## 3. 使用例と重要性\n",
    "\n",
    "### データセットサイズの記録\n",
    "```python\n",
    "logging.info(f\"Total dataset size: {len(dataset)}\")\n",
    "```\n",
    "- 例：「Total dataset size: 10000」\n",
    "\n",
    "### メモリ監視の実際の使用シーン：\n",
    "1. **学習開始前**：\n",
    "   - 利用可能なメモリを確認\n",
    "   - 適切なバッチサイズの決定に使用\n",
    "\n",
    "2. **学習中**：\n",
    "   - メモリリークの検出\n",
    "   - 異常な使用量の早期発見\n",
    "\n",
    "3. **エラー発生時**：\n",
    "   - メモリ不足が原因かどうかの診断\n",
    "   - 適切な対処方法の決定\n",
    "\n",
    "### 具体例：\n",
    "```python\n",
    "# 学習の各ステップで\n",
    "log_memory_usage()  # メモリ使用状況を確認\n",
    "if 大きな対話データの処理後:\n",
    "    clear_memory()  # メモリをクリーンアップ\n",
    "```\n",
    "\n",
    "このモニタリングにより：\n",
    "- 安定した学習環境の維持\n",
    "- メモリ関連の問題を早期発見\n",
    "- システムリソースの効率的な使用\n",
    "が可能になります。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "### 4.3 コールバック実装\n",
    "class StyleCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.style_scores = []\n",
    "        self.flow_scores = []\n",
    "        \n",
    "    def on_evaluate(self, args, state, control, metrics, **kwargs):\n",
    "        if 'eval_style_consistency' in metrics:\n",
    "            self.style_scores.append(metrics['eval_style_consistency'])\n",
    "            self.flow_scores.append(metrics['eval_dialogue_flow'])\n",
    "            \n",
    "            # Log detailed information\n",
    "            logging.info(f\"Step {state.global_step}:\")\n",
    "            logging.info(f\"Style Consistency: {metrics['eval_style_consistency']:.3f}\")\n",
    "            logging.info(f\"Dialogue Flow: {metrics['eval_dialogue_flow']:.3f}\")\n",
    "    \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        # Log overall evaluation\n",
    "        avg_style = sum(self.style_scores) / len(self.style_scores) if self.style_scores else 0\n",
    "        avg_flow = sum(self.flow_scores) / len(self.flow_scores) if self.flow_scores else 0\n",
    "        \n",
    "        logging.info(\"Training Complete!\")\n",
    "        logging.info(f\"Average Style Consistency: {avg_style:.3f}\")\n",
    "        logging.info(f\"Average Dialogue Flow: {avg_flow:.3f}\")\n",
    "\n",
    "# Extend custom callbacks\n",
    "\n",
    "class TrainingMonitorCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        # Import psutil here as well for safety\n",
    "        import psutil\n",
    "        self.train_start_time = None\n",
    "        self.metrics_history = {\n",
    "            'step': [],\n",
    "            'style_consistency': [],\n",
    "            'dialogue_flow': [],\n",
    "            'combined_score': [],\n",
    "            'loss': [],\n",
    "            'learning_rate': [],\n",
    "            'epoch': [],\n",
    "            'cpu_ram_usage': [],\n",
    "            'gpu_vram_usage': [],\n",
    "            'gpu_utilization': [],\n",
    "            'batch_size': [],\n",
    "            'moving_avg_loss': [],\n",
    "            # 新しい詳細メトリクス\n",
    "            'lr_schedule': [],\n",
    "            'batch_metrics': [],\n",
    "            'gpu_metrics': [],\n",
    "            'grad_norm': []\n",
    "        }\n",
    "        self.peak_metrics = {\n",
    "            'cpu_ram': 0,\n",
    "            'gpu_vram': 0,\n",
    "            'gpu_util': 0\n",
    "        }\n",
    "        self.output_dir = Path(f\"{BASE_OUTPUT_DIR}/training_progress\")\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "    def _record_resource_usage(self):\n",
    "        \"\"\"Record current resource usage with timestamp\"\"\"\n",
    "        import psutil\n",
    "        import torch\n",
    "        current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        \n",
    "        # CPU RAM\n",
    "        cpu_ram = psutil.Process().memory_info().rss / (1024 * 1024 * 1024)  # GB\n",
    "        self.peak_metrics['cpu_ram'] = max(self.peak_metrics['cpu_ram'], cpu_ram)\n",
    "        \n",
    "        # GPU metrics with timestamp\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_metrics = []\n",
    "            for i in range(torch.cuda.device_count()):\n",
    "                vram_used = torch.cuda.memory_allocated(i) / (1024 * 1024 * 1024)  # GB\n",
    "                self.peak_metrics['gpu_vram'] = max(self.peak_metrics['gpu_vram'], vram_used)\n",
    "                \n",
    "                # GPU utilization (requires nvidia-smi)\n",
    "                try:\n",
    "                    import subprocess\n",
    "                    result = subprocess.check_output(['nvidia-smi', '--query-gpu=utilization.gpu', '--format=csv,noheader,nounits'])\n",
    "                    gpu_util = float(result.decode('utf-8').strip())\n",
    "                    self.peak_metrics['gpu_util'] = max(self.peak_metrics['gpu_util'], gpu_util)\n",
    "                except:\n",
    "                    gpu_util = 0\n",
    "                \n",
    "                gpu_metrics.append({\n",
    "                    'device': i,\n",
    "                    'vram_used': vram_used,\n",
    "                    'utilization': gpu_util\n",
    "                })\n",
    "                \n",
    "            # 時系列データとして保存\n",
    "            self.metrics_history['gpu_metrics'].append({\n",
    "                'timestamp': current_time,\n",
    "                'metrics': gpu_metrics\n",
    "            })\n",
    "                \n",
    "            self.metrics_history['cpu_ram_usage'].append(cpu_ram)\n",
    "            self.metrics_history['gpu_vram_usage'].append(vram_used)\n",
    "            self.metrics_history['gpu_utilization'].append(gpu_util)\n",
    "    \n",
    "    def on_train_begin(self, args, state, control, **kwargs):\n",
    "        self.train_start_time = datetime.now()\n",
    "        logging.info(\"Training started at: %s\", self.train_start_time)\n",
    "        self._record_resource_usage()\n",
    "        \n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs:\n",
    "            current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            \n",
    "            # 学習率とスケジューリングの記録\n",
    "            if 'learning_rate' in logs:\n",
    "                self.metrics_history['lr_schedule'].append({\n",
    "                    'timestamp': current_time,\n",
    "                    'step': state.global_step,\n",
    "                    'learning_rate': logs['learning_rate'],\n",
    "                    'schedule_type': args.lr_scheduler_type\n",
    "                })\n",
    "                self.metrics_history['learning_rate'].append(logs['learning_rate'])\n",
    "            \n",
    "            # バッチサイズと損失値の関連を記録\n",
    "            if 'loss' in logs:\n",
    "                self.metrics_history['batch_metrics'].append({\n",
    "                    'timestamp': current_time,\n",
    "                    'step': state.global_step,\n",
    "                    'batch_size': args.per_device_train_batch_size,\n",
    "                    'loss': logs['loss'],\n",
    "                    'grad_norm': logs.get('grad_norm', None)\n",
    "                })\n",
    "                self.metrics_history['loss'].append(logs['loss'])\n",
    "                self.metrics_history['batch_size'].append(args.per_device_train_batch_size)\n",
    "                if 'grad_norm' in logs:\n",
    "                    self.metrics_history['grad_norm'].append(logs['grad_norm'])\n",
    "            \n",
    "            # 移動平均の計算と記録\n",
    "            if len(self.metrics_history['loss']) > 10:\n",
    "                avg_loss = sum(self.metrics_history['loss'][-10:]) / 10\n",
    "                self.metrics_history['moving_avg_loss'].append(avg_loss)\n",
    "                logging.info(f\"Moving average loss (last 10 steps): {avg_loss:.4f}\")\n",
    "            \n",
    "            logging.info(f\"Step {state.global_step}: {logs}\")\n",
    "            if 'grad_norm' in logs:\n",
    "                logging.info(f\"Gradient norm: {logs['grad_norm']:.4f}\")\n",
    "            \n",
    "        self._record_resource_usage()\n",
    "        \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        training_duration = datetime.now() - self.train_start_time\n",
    "        \n",
    "        # 詳細な学習履歴の保存\n",
    "        training_history = {\n",
    "            'lr_schedule': self.metrics_history['lr_schedule'],\n",
    "            'batch_metrics': self.metrics_history['batch_metrics'],\n",
    "            'gpu_metrics': self.metrics_history['gpu_metrics'],\n",
    "            'moving_avg_loss': self.metrics_history['moving_avg_loss']\n",
    "        }\n",
    "        \n",
    "        # 学習履歴をJSONファイルとして保存\n",
    "        history_file = self.output_dir / 'training_history.json'\n",
    "        with open(history_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(training_history, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        # 基本的なメトリクスのログ出力\n",
    "        logging.info(f\"Training completed. Total duration: {training_duration}\")\n",
    "        logging.info(f\"Peak CPU RAM usage: {self.peak_metrics['cpu_ram']:.2f} GB\")\n",
    "        logging.info(f\"Peak GPU VRAM usage: {self.peak_metrics['gpu_vram']:.2f} GB\")\n",
    "        logging.info(f\"Peak GPU utilization: {self.peak_metrics['gpu_util']:.1f}%\")\n",
    "        \n",
    "        # 最終サマリーの作成と保存\n",
    "        summary = {\n",
    "            'training_duration': str(training_duration),\n",
    "            'final_loss': self.metrics_history['loss'][-1] if self.metrics_history['loss'] else None,\n",
    "            'best_combined_score': max(filter(None, self.metrics_history['combined_score'])) if self.metrics_history['combined_score'] else None,\n",
    "            'total_steps': len(self.metrics_history['step']),\n",
    "            'final_epoch': self.metrics_history['epoch'][-1] if self.metrics_history['epoch'] else None,\n",
    "            'learning_rate_summary': {\n",
    "                'initial': self.metrics_history['learning_rate'][0] if self.metrics_history['learning_rate'] else None,\n",
    "                'final': self.metrics_history['learning_rate'][-1] if self.metrics_history['learning_rate'] else None,\n",
    "                'schedule_type': args.lr_scheduler_type\n",
    "            },\n",
    "            'loss_summary': {\n",
    "                'final_moving_avg': self.metrics_history['moving_avg_loss'][-1] if self.metrics_history['moving_avg_loss'] else None,\n",
    "                'best_loss': min(self.metrics_history['loss']) if self.metrics_history['loss'] else None\n",
    "            },\n",
    "            'resource_usage': {\n",
    "                'peak_cpu_ram_gb': self.peak_metrics['cpu_ram'],\n",
    "                'peak_gpu_vram_gb': self.peak_metrics['gpu_vram'],\n",
    "                'peak_gpu_utilization': self.peak_metrics['gpu_util']\n",
    "            },\n",
    "            'hardware_info': {\n",
    "                'cpu_info': self._get_cpu_info(),\n",
    "                'gpu_info': self._get_gpu_info(),\n",
    "                'total_ram': self._get_total_ram()\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # サマリーをJSONファイルとして保存\n",
    "        with open(self.output_dir / 'training_summary.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(summary, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "        logging.info(\"Training Complete!\")\n",
    "        logging.info(f\"Training duration: {summary['training_duration']}\")\n",
    "        \n",
    "        # Noneチェックを追加\n",
    "        if summary['loss_summary']['final_moving_avg'] is not None:\n",
    "            logging.info(f\"Final moving average loss: {summary['loss_summary']['final_moving_avg']:.4f}\")\n",
    "        if summary['loss_summary']['best_loss'] is not None:\n",
    "            logging.info(f\"Best loss achieved: {summary['loss_summary']['best_loss']:.4f}\")\n",
    "        \n",
    "        logging.info(f\"Peak CPU RAM usage: {summary['resource_usage']['peak_cpu_ram_gb']:.2f} GB\")\n",
    "        logging.info(f\"Peak GPU VRAM usage: {summary['resource_usage']['peak_gpu_vram_gb']:.2f} GB\")\n",
    "        logging.info(f\"Peak GPU utilization: {summary['resource_usage']['peak_gpu_utilization']:.1f}%\")\n",
    "\n",
    "    def _get_cpu_info(self):\n",
    "        import cpuinfo\n",
    "        try:\n",
    "            info = cpuinfo.get_cpu_info()\n",
    "            return {\n",
    "                'model': info.get('brand_raw', 'Unknown'),\n",
    "                'cores': psutil.cpu_count(logical=False),\n",
    "                'threads': psutil.cpu_count(logical=True)\n",
    "            }\n",
    "        except:\n",
    "            return \"Failed to get CPU info\"\n",
    "            \n",
    "    def _get_gpu_info(self):\n",
    "        if not torch.cuda.is_available():\n",
    "            return \"No GPU available\"\n",
    "        try:\n",
    "            import subprocess\n",
    "            result = subprocess.check_output(['nvidia-smi', '--query-gpu=gpu_name,memory.total', '--format=csv,noheader,nounits'])\n",
    "            gpus = result.decode('utf-8').strip().split('\\n')\n",
    "            return [{'model': g.split(',')[0], 'memory': float(g.split(',')[1])/1024} for g in gpus]\n",
    "        except:\n",
    "            return \"Failed to get GPU info\"\n",
    "            \n",
    "    def _get_total_ram(self):\n",
    "        return psutil.virtual_memory().total / (1024 * 1024 * 1024)  # GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "\n",
    "\n",
    "このコードは学習の進行状況を監視・記録するためのコールバック（監視プログラム）を実装しています。2つの主要なクラスについて説明します：\n",
    "\n",
    "## 1. StyleCallback クラス\n",
    "ソクラテス式対話の品質を監視するためのクラスです。\n",
    "\n",
    "### 主な機能：\n",
    "```python\n",
    "class StyleCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.style_scores = []  # スタイルスコアの履歴\n",
    "        self.flow_scores = []   # 対話の流れスコアの履歴\n",
    "```\n",
    "\n",
    "### 評価時の処理：\n",
    "```python\n",
    "def on_evaluate(self, args, state, control, metrics, **kwargs):\n",
    "    # スタイルと対話の流れのスコアを記録\n",
    "    logging.info(f\"Style Consistency: {metrics['eval_style_consistency']:.3f}\")\n",
    "    logging.info(f\"Dialogue Flow: {metrics['eval_dialogue_flow']:.3f}\")\n",
    "```\n",
    "\n",
    "例：\n",
    "```\n",
    "Step 100:\n",
    "Style Consistency: 0.856\n",
    "Dialogue Flow: 0.742\n",
    "```\n",
    "\n",
    "### 学習終了時の処理：\n",
    "- 全期間の平均スコアを計算して表示\n",
    "```\n",
    "Training Complete!\n",
    "Average Style Consistency: 0.823\n",
    "Average Dialogue Flow: 0.751\n",
    "```\n",
    "\n",
    "## 2. TrainingMonitorCallback クラス\n",
    "学習全体の進行状況を詳細に監視・記録するクラスです。\n",
    "\n",
    "### 記録する主な情報：\n",
    "```python\n",
    "self.metrics_history = {\n",
    "    'style_consistency': [],    # スタイルの一貫性\n",
    "    'dialogue_flow': [],        # 対話の流れ\n",
    "    'loss': [],                # 損失値\n",
    "    'learning_rate': [],       # 学習率\n",
    "    'cpu_ram_usage': [],       # CPUメモリ使用量\n",
    "    'gpu_vram_usage': [],      # GPUメモリ使用量\n",
    "    # その他の指標...\n",
    "}\n",
    "```\n",
    "\n",
    "### 主な機能：\n",
    "\n",
    "1. **学習開始時の記録**\n",
    "```python\n",
    "def on_train_begin(self, args, state, control, **kwargs):\n",
    "    self.train_start_time = datetime.now()\n",
    "    logging.info(\"Training started at: %s\", self.train_start_time)\n",
    "```\n",
    "\n",
    "2. **定期的な学習状況の記録**\n",
    "```python\n",
    "def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "    # 学習率の記録\n",
    "    # 損失値の記録\n",
    "    # メモリ使用量の記録\n",
    "```\n",
    "\n",
    "例：\n",
    "```\n",
    "Step 100: \n",
    "Learning rate: 0.0001\n",
    "Loss: 2.345\n",
    "Moving average loss (last 10 steps): 2.456\n",
    "```\n",
    "\n",
    "3. **学習終了時のサマリー作成**\n",
    "```python\n",
    "summary = {\n",
    "    'training_duration': '2時間30分',\n",
    "    'final_loss': 1.234,\n",
    "    'best_combined_score': 0.876,\n",
    "    'resource_usage': {\n",
    "        'peak_cpu_ram_gb': 16.5,\n",
    "        'peak_gpu_vram_gb': 8.2,\n",
    "        'peak_gpu_utilization': 95.4\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "### 出力例：\n",
    "```\n",
    "Training Complete!\n",
    "Training duration: 2:30:45\n",
    "Final moving average loss: 1.234\n",
    "Peak CPU RAM usage: 16.50 GB\n",
    "Peak GPU VRAM usage: 8.20 GB\n",
    "Peak GPU utilization: 95.4%\n",
    "```\n",
    "\n",
    "このように、学習の進行状況を多角的に監視・記録することで：\n",
    "- モデルの品質改善の追跡\n",
    "- リソース使用の最適化\n",
    "- 問題の早期発見\n",
    "が可能になります。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "### 4.4 カスタムトレーナー定義\n",
    "# Training step customization\n",
    "class CustomTrainer(Trainer):\n",
    "    def training_step(self, *args, **kwargs):\n",
    "        loss = super().training_step(*args, **kwargs)\n",
    "        if self.state.global_step % 50 == 0:\n",
    "            clear_memory()\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "        return loss\n",
    "\n",
    "# Evaluation customization\n",
    "class CustomTrainer(Trainer):\n",
    "    def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix=\"eval\"):\n",
    "        eval_dataset = eval_dataset if eval_dataset is not None else self.eval_dataset\n",
    "        if eval_dataset is not None:\n",
    "            # Limit evaluation dataset to 100 samples\n",
    "            eval_dataset = eval_dataset.select(range(min(100, len(eval_dataset))))\n",
    "        return super().evaluate(eval_dataset, ignore_keys, metric_key_prefix)\n",
    "\n",
    "# Trainer initialization\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[StyleCallback(), TrainingMonitorCallback()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "\n",
    "\n",
    "このコードはカスタマイズされたトレーニング処理を定義しています。以下に分かりやすく説明します：\n",
    "\n",
    "## 1. カスタムトレーナーの学習ステップ\n",
    "```python\n",
    "class CustomTrainer(Trainer):\n",
    "    def training_step(self, *args, **kwargs):\n",
    "        loss = super().training_step(*args, **kwargs)\n",
    "        if self.state.global_step % 50 == 0:  # 50ステップごとに\n",
    "            clear_memory()                     # メモリクリア\n",
    "            gc.collect()                       # ガベージコレクション\n",
    "            torch.cuda.empty_cache()           # GPUキャッシュクリア\n",
    "        return loss\n",
    "```\n",
    "\n",
    "### 目的：\n",
    "- メモリの効率的な管理\n",
    "- 長時間の学習を安定して行う\n",
    "\n",
    "### 具体例：\n",
    "```python\n",
    "ステップ 50: メモリクリア実行\n",
    "- Before: GPU使用量 8GB\n",
    "- After:  GPU使用量 6GB\n",
    "\n",
    "ステップ 100: メモリクリア実行\n",
    "- Before: GPU使用量 8.5GB\n",
    "- After:  GPU使用量 6GB\n",
    "```\n",
    "\n",
    "## 2. カスタムトレーナーの評価機能\n",
    "```python\n",
    "class CustomTrainer(Trainer):\n",
    "    def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix=\"eval\"):\n",
    "        eval_dataset = eval_dataset if eval_dataset is not None else self.eval_dataset\n",
    "        if eval_dataset is not None:\n",
    "            # 評価用データセットを100サンプルに制限\n",
    "            eval_dataset = eval_dataset.select(range(min(100, len(eval_dataset))))\n",
    "        return super().evaluate(eval_dataset, ignore_keys, metric_key_prefix)\n",
    "```\n",
    "\n",
    "### 目的：\n",
    "- 評価処理の効率化\n",
    "- メモリ使用量の削減\n",
    "\n",
    "### 具体例：\n",
    "```python\n",
    "元のデータセット: 1000対話\n",
    "↓\n",
    "評価用に制限: 100対話\n",
    "- 「その考えはどうでしょうか？」\n",
    "- 「なるほど、興味深い視点ですね」\n",
    "- 「それはなぜだと思いますか？」\n",
    "など、100の対話サンプルで評価\n",
    "```\n",
    "\n",
    "## 3. トレーナーの初期化\n",
    "```python\n",
    "trainer = CustomTrainer(\n",
    "    model=model,                    # 学習対象のモデル\n",
    "    args=training_args,            # 学習設定\n",
    "    train_dataset=train_dataset,   # 学習用データセット\n",
    "    eval_dataset=eval_dataset,     # 評価用データセット\n",
    "    data_collator=data_collator,   # データ整形用関数\n",
    "    compute_metrics=compute_metrics,# 評価指標計算\n",
    "    callbacks=[\n",
    "        StyleCallback(),           # スタイル評価用コールバック\n",
    "        TrainingMonitorCallback()  # 学習監視用コールバック\n",
    "    ],\n",
    ")\n",
    "```\n",
    "\n",
    "### 設定の意味：\n",
    "1. **model**: ソクラテス式対話を生成するモデル\n",
    "2. **training_args**: バッチサイズ、学習率などの設定\n",
    "3. **datasets**: 学習用と評価用の対話データ\n",
    "4. **compute_metrics**: スタイルと対話の流れを評価\n",
    "5. **callbacks**: 学習進捗の監視と記録\n",
    "\n",
    "### 実際の使用例：\n",
    "```python\n",
    "# トレーナーの使用\n",
    "trainer.train()\n",
    "\n",
    "# 出力例：\n",
    "Epoch 1/30:\n",
    "- Loss: 2.345\n",
    "- Style Consistency: 0.756\n",
    "- Dialogue Flow: 0.678\n",
    "- Memory Usage: 6.5GB\n",
    "\n",
    "Epoch 2/30:\n",
    "- Loss: 2.123\n",
    "- Style Consistency: 0.789\n",
    "- Dialogue Flow: 0.701\n",
    "- Memory Usage: 6.5GB\n",
    "```\n",
    "\n",
    "このカスタムトレーナーにより：\n",
    "- メモリ使用の最適化\n",
    "- 効率的な評価処理\n",
    "- 詳細な学習進捗の監視\n",
    "が実現され、安定した学習が可能になります。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
