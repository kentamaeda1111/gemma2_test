{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. トレーニングインフラ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "トレーニングインフラの部分について、ソクラテス式チャットボットの学習を例に説明します。\n",
    "\n",
    "### 4.1 評価メトリクス定義\n",
    "この部分は、モデルがどれだけ上手く学習できているかを測る指標を定義しています。\n",
    "\n",
    "例えば、チャットボットが「なぜそう考えるのですか？」と質問するとき、その返答がどれだけ自然で適切かを数値化します。主に以下の2つの指標を使用：\n",
    "\n",
    "1. loss（損失）：モデルの予測と実際の正解との差\n",
    "2. perplexity（パープレキシティ）：モデルの予測の確実性を示す値。低いほど良い\n",
    "\n",
    "### 4.2 システムリソース監視\n",
    "学習中のコンピュータのリソース使用状況を監視します。これは以下のような状況を防ぐために重要です：\n",
    "- メモリ不足でプログラムが突然停止する\n",
    "- GPUのメモリが溢れて学習が失敗する\n",
    "\n",
    "### 4.3 データセット分割とトレーニング設定\n",
    "データを「学習用」と「評価用」に分けて、学習の設定を行います：\n",
    "\n",
    "- 学習データ（80%）：実際にモデルが学習に使うデータ\n",
    "  例：「どうしてそう思いましたか？」「その根拠は何ですか？」などの質問と回答のペア\n",
    "- 評価データ（20%）：モデルの性能をテストするためのデータ\n",
    "\n",
    "また、学習の詳細な設定も行います：\n",
    "- 学習回数：30エポック\n",
    "- 学習率：0.0002（モデルの学習速度）\n",
    "- チェックポイントの保存：100ステップごと\n",
    "\n",
    "### 4.4 トレーニング監視システム\n",
    "学習の進行状況を監視するシステムです。以下のような機能があります：\n",
    "\n",
    "1. 学習の安定性チェック：\n",
    "   - モデルが「なぜですか？」と適切なタイミングで質問できているか\n",
    "   - 質問が不自然に繰り返されていないか\n",
    "\n",
    "2. チェックポイント保存：\n",
    "   - 良い性能を示したモデルを自動的に保存\n",
    "   - 問題が起きても途中から再開できるように\n",
    "\n",
    "3. 問題検知：\n",
    "   - モデルが同じ質問を繰り返すような異常を検知\n",
    "   - 学習が進まない状況を検知\n",
    "\n",
    "### 4.5 トレーナー実装\n",
    "実際の学習を実行する部分です：\n",
    "- メモリ管理：定期的にメモリを解放して長時間の学習を可能に\n",
    "- 評価の効率化：評価時のデータ数を制限して処理を高速化\n",
    "- 進捗モニタリング：学習の状況をリアルタイムで確認\n",
    "\n",
    "これらの機能により、ソクラテス式チャットボットが適切なタイミングで適切な質問ができるように、安定した学習を実現します。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "### 4.1 評価メトリクス定義\n",
    "def compute_metrics(eval_preds):\n",
    "    \"\"\"基本的な評価メトリクスの計算\"\"\"\n",
    "    logits, labels = eval_preds\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Convert logits to CPU tensor\n",
    "        logits = torch.tensor(logits).cpu()\n",
    "        labels = torch.tensor(labels).cpu()\n",
    "        \n",
    "        # Calculate perplexity\n",
    "        loss = torch.nn.functional.cross_entropy(\n",
    "            logits.view(-1, logits.size(-1)), \n",
    "            labels.view(-1),\n",
    "            ignore_index=-100\n",
    "        )\n",
    "        perplexity = torch.exp(loss)\n",
    "        \n",
    "        # Clean up memory\n",
    "        del logits, labels\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        return {\n",
    "            'perplexity': perplexity.item(),\n",
    "            'loss': loss.item()\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "この評価メトリクス定義について、ソクラテス式チャットボットの文脈で説明させていただきます。\n",
    "\n",
    "### 基本的な説明\n",
    "\n",
    "この`compute_metrics`関数は、モデルの性能を評価するための指標を計算する関数です。主に2つの重要な指標を計算しています：\n",
    "\n",
    "1. **Loss（損失）**: モデルの予測がどれだけ間違っているかを示す値\n",
    "2. **Perplexity（パープレキシティ）**: モデルの予測の確実性を示す値\n",
    "\n",
    "### 具体例で説明\n",
    "\n",
    "例えば、ソクラテス式チャットボットの会話でこう考えてみましょう：\n",
    "\n",
    "```\n",
    "ユーザー: なぜ空は青いのですか？\n",
    "ボット: その質問は興味深いですね。あなたはなぜ空が青いと思いますか？\n",
    "```\n",
    "\n",
    "このとき、モデルは：\n",
    "\n",
    "1. **Logits（予測値）**: モデルが次に来る単語として考えられる全ての可能性とその確率\n",
    "   - 例：「興味深い」(90%), 「面白い」(5%), 「難しい」(3%), その他(2%)\n",
    "\n",
    "2. **Labels（正解）**: 実際にあるべき応答\n",
    "   - 例：「興味深い」という単語が正解\n",
    "\n",
    "### 計算の流れ\n",
    "\n",
    "1. `with torch.no_grad():`: \n",
    "   - 評価時は学習を行わないので、メモリを節約するために勾配計算をオフにします\n",
    "\n",
    "2. `logits = torch.tensor(logits).cpu()`:\n",
    "   - 予測値をCPUメモリに移動して処理します\n",
    "\n",
    "3. `loss = torch.nn.functional.cross_entropy()`:\n",
    "   - モデルの予測と正解の差を計算します\n",
    "   - 例：「興味深い」を90%の確率で予測できていれば低いloss、20%の確率だと高いloss\n",
    "\n",
    "4. `perplexity = torch.exp(loss)`:\n",
    "   - lossから計算される、モデルの「困惑度」\n",
    "   - 低いほど良い（モデルが自信を持って回答している）\n",
    "   - 例：\n",
    "     - perplexity = 1.1 → モデルが非常に自信を持って「その質問は興味深いですね」と応答\n",
    "     - perplexity = 5.0 → モデルが迷いながら応答している\n",
    "\n",
    "### 実用的な意味\n",
    "\n",
    "- **Loss値が低い**：モデルがソクラテス式の対話パターンをよく学習できている\n",
    "- **Perplexityが低い**：モデルが自信を持って質問を返せている\n",
    "\n",
    "例えば：\n",
    "```\n",
    "ユーザー: 幸せとは何だと思いますか？\n",
    "ボット: （低perplexity = 自信あり）\n",
    "       まず、あなたにとって幸せとは何を意味するのでしょうか？\n",
    "\n",
    "ボット: （高perplexity = 自信なし）\n",
    "       えーと...その...幸せについて...どう思いますか...？\n",
    "```\n",
    "\n",
    "このように、これらのメトリクスを監視することで、モデルがソクラテス式の対話スタイルをどれだけ上手く学習できているかを評価できます。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "### 4.2 システムリソース監視\n",
    "def log_memory_usage():\n",
    "    import psutil\n",
    "    import torch\n",
    "    \n",
    "    # CPU memory\n",
    "    process = psutil.Process()\n",
    "    cpu_memory = process.memory_info().rss / 1024 / 1024  # MB\n",
    "    \n",
    "    # GPU memory\n",
    "    gpu_memory = []\n",
    "    if torch.cuda.is_available():\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            gpu_memory.append({\n",
    "                'device': i,\n",
    "                'allocated': torch.cuda.memory_allocated(i) / 1024 / 1024,  # MB\n",
    "                'reserved': torch.cuda.memory_reserved(i) / 1024 / 1024,    # MB\n",
    "                'max_allocated': torch.cuda.max_memory_allocated(i) / 1024 / 1024  # MB\n",
    "            })\n",
    "    \n",
    "    logging.info(f\"CPU Memory usage: {cpu_memory:.2f} MB\")\n",
    "    for gpu in gpu_memory:\n",
    "        logging.info(f\"GPU {gpu['device']} Memory:\")\n",
    "        logging.info(f\"  - Allocated: {gpu['allocated']:.2f} MB\")\n",
    "        logging.info(f\"  - Reserved: {gpu['reserved']:.2f} MB\")\n",
    "        logging.info(f\"  - Max Allocated: {gpu['max_allocated']:.2f} MB\")\n",
    "\n",
    "# Log dataset size\n",
    "logging.info(f\"Total dataset size: {len(dataset)}\")\n",
    "log_memory_usage()\n",
    "\n",
    "def clear_memory():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "このシステムリソース監視の部分について、分かりやすく説明させていただきます。\n",
    "\n",
    "### 基本的な説明\n",
    "この部分のコードは、コンピュータのメモリ（記憶領域）の使用状況を監視するためのものです。大規模な言語モデルの学習では、メモリ管理が非常に重要になります。\n",
    "\n",
    "### 主な機能\n",
    "\n",
    "1. **CPU メモリの監視**\n",
    "```python\n",
    "process = psutil.Process()\n",
    "cpu_memory = process.memory_info().rss / 1024 / 1024  # MB\n",
    "```\n",
    "- コンピュータの主記憶域（RAM）の使用量を監視します\n",
    "- MByte単位で表示されます\n",
    "- 例：`CPU Memory usage: 8542.34 MB`（約8.5GB使用中）\n",
    "\n",
    "2. **GPU メモリの監視**\n",
    "```python\n",
    "gpu_memory.append({\n",
    "    'device': i,\n",
    "    'allocated': torch.cuda.memory_allocated(i) / 1024 / 1024,  # MB\n",
    "    'reserved': torch.cuda.memory_reserved(i) / 1024 / 1024,    # MB\n",
    "    'max_allocated': torch.cuda.max_memory_allocated(i) / 1024 / 1024  # MB\n",
    "})\n",
    "```\n",
    "3つの重要な指標を監視します：\n",
    "- `allocated`: 現在使用中のメモリ\n",
    "- `reserved`: 確保されている（予約済み）メモリ\n",
    "- `max_allocated`: これまでの最大使用量\n",
    "\n",
    "### 実際の使用例\n",
    "ソクラテス式チャットボットの学習時には、以下のような出力が見られるかもしれません：\n",
    "\n",
    "```\n",
    "CPU Memory usage: 12456.78 MB\n",
    "GPU 0 Memory:\n",
    "  - Allocated: 5234.45 MB\n",
    "  - Reserved: 6000.00 MB\n",
    "  - Max Allocated: 5500.67 MB\n",
    "```\n",
    "\n",
    "これは以下を意味します：\n",
    "- CPUで約12.5GBのメモリを使用中\n",
    "- GPUで：\n",
    "  - 現在約5.2GBを使用中\n",
    "  - 6GBを予約済み\n",
    "  - これまでの最大使用量は約5.5GB\n",
    "\n",
    "### メモリクリア機能\n",
    "```python\n",
    "def clear_memory():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "```\n",
    "この関数は、不要になったメモリを解放します：\n",
    "- 例：チャットボットが長い会話を処理した後、そのデータが不要になった時\n",
    "- `gc.collect()`: Python側のメモリを解放\n",
    "- `torch.cuda.empty_cache()`: GPU側のメモリを解放\n",
    "\n",
    "### なぜ重要か？\n",
    "ソクラテス式チャットボットの学習では：\n",
    "1. 大量の対話データを処理\n",
    "2. 複雑な言語モデルを使用\n",
    "3. 長い会話履歴を保持\n",
    "\n",
    "これらが**メモリを大量に消費**するため、メモリ監視は：\n",
    "- システムのクラッシュを防ぐ\n",
    "- 効率的な学習を可能にする\n",
    "- リソースの最適な使用を確保\n",
    "\n",
    "という重要な役割を果たします。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "### 4.3 データセット分割とトレーニング設定\n",
    "# Split dataset into training and evaluation sets\n",
    "dataset_size = len(tokenized_dataset)\n",
    "indices = np.random.permutation(dataset_size)\n",
    "split_idx = int(dataset_size * 0.8)\n",
    "train_dataset = tokenized_dataset.select(indices[:split_idx])\n",
    "# Limit evaluation dataset size\n",
    "eval_dataset = tokenized_dataset.select(indices[split_idx:split_idx+50])  # Maximum 50 samples\n",
    "\n",
    "logging.info(f\"Training dataset size: {len(train_dataset)}\")\n",
    "logging.info(f\"Evaluation dataset size: {len(eval_dataset)}\")\n",
    "\n",
    "# Disable wandb via environment variable\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "# Update training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=MODEL_OUTPUT_DIR,  \n",
    "    num_train_epochs=30,\n",
    "    learning_rate=2e-4,           # 8e-5から2e-4に増加\n",
    "    weight_decay=0.01,            # 0.06から0.01に減少\n",
    "    warmup_ratio=0.1,             # 0.25から0.1に減少\n",
    "    lr_scheduler_type=\"cosine\",   # cosine_with_restartsからcosineに変更\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    gradient_accumulation_steps=8,    # バッチサイズを小さくした分、これを8に増やして補完\n",
    "    max_steps=-1,\n",
    "    disable_tqdm=False,\n",
    "    logging_dir=LOG_OUTPUT_DIR,   \n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    no_cuda=False,\n",
    "    dataloader_num_workers=1,         # CPUメモリ節約のため1に戻す\n",
    "    report_to=[],\n",
    "    run_name=None,\n",
    "    per_device_train_batch_size=2,    # CPUメモリ節約のため2に戻す\n",
    "    per_device_eval_batch_size=2,     # 同上\n",
    "    gradient_checkpointing=True,\n",
    "    max_grad_norm=1.0,             # 0.5から1.0に増加\n",
    "    dataloader_pin_memory=True,\n",
    "    save_total_limit=10,  # 10に増やす\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    optim=\"adamw_torch\",\n",
    "    eval_accumulation_steps=4,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"perplexity\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "\n",
    "\n",
    "このデータセット分割とトレーニング設定について、分かりやすく説明させていただきます。\n",
    "\n",
    "### 1. データセット分割\n",
    "```python\n",
    "dataset_size = len(tokenized_dataset)\n",
    "indices = np.random.permutation(dataset_size)\n",
    "split_idx = int(dataset_size * 0.8)\n",
    "train_dataset = tokenized_dataset.select(indices[:split_idx])\n",
    "eval_dataset = tokenized_dataset.select(indices[split_idx:split_idx+50])\n",
    "```\n",
    "\n",
    "これは、データを「学習用」と「評価用」に分ける部分です。\n",
    "\n",
    "例えば、1000個の対話データがあった場合：\n",
    "- 学習用（train_dataset）: 800個（80%）\n",
    "  - モデルが実際に学習に使うデータ\n",
    "  - 例：「なぜ勉強は大切ですか？」→「あなたはなぜ勉強が大切だと考えていますか？」\n",
    "\n",
    "- 評価用（eval_dataset）: 50個\n",
    "  - モデルの性能をテストするためのデータ\n",
    "  - 学習には使わない\n",
    "  - 最大50個に制限（メモリ効率のため）\n",
    "\n",
    "### 2. トレーニング設定（TrainingArguments）\n",
    "\n",
    "重要なパラメータを見ていきましょう：\n",
    "\n",
    "#### 基本設定\n",
    "```python\n",
    "num_train_epochs=30,              # 30回全データを学習\n",
    "learning_rate=2e-4,              # 学習率（どれだけ大きく更新するか）\n",
    "per_device_train_batch_size=2,    # 一度に処理する対話数\n",
    "```\n",
    "\n",
    "#### 学習の進め方\n",
    "```python\n",
    "evaluation_strategy=\"steps\",      # 100ステップごとに評価\n",
    "eval_steps=100,\n",
    "save_strategy=\"steps\",           # 100ステップごとに保存\n",
    "save_steps=100,\n",
    "```\n",
    "\n",
    "例えば：\n",
    "1. 100個の対話を処理\n",
    "2. モデルの性能を評価\n",
    "3. モデルを保存\n",
    "4. また100個処理...という流れ\n",
    "\n",
    "#### メモリ管理関連\n",
    "```python\n",
    "gradient_accumulation_steps=8,    # 8回分の計算をまとめて更新\n",
    "per_device_train_batch_size=2,    # 一度に2つの対話を処理\n",
    "dataloader_num_workers=1,         # データ読み込みの並列処理数\n",
    "```\n",
    "\n",
    "これは、限られたメモリで効率よく学習するための設定です。\n",
    "\n",
    "#### 学習の最適化設定\n",
    "```python\n",
    "weight_decay=0.01,               # モデルの複雑さを抑制\n",
    "warmup_ratio=0.1,               # 学習率を徐々に上げる期間\n",
    "lr_scheduler_type=\"cosine\",      # 学習率の変化パターン\n",
    "```\n",
    "\n",
    "ソクラテス式チャットボットの文脈で例えると：\n",
    "1. 最初は慎重に学習（warmup）\n",
    "   - 「簡単な質問への返し方」から始める\n",
    "2. 徐々に本格的な学習\n",
    "   - 「より深い問いかけ」の練習\n",
    "3. 最後は微調整\n",
    "   - 「洗練された対話」の完成\n",
    "\n",
    "#### モデルの保存設定\n",
    "```python\n",
    "load_best_model_at_end=True,     # 最も性能の良かったモデルを保存\n",
    "metric_for_best_model=\"perplexity\", # perplexityが最も良いモデルを選択\n",
    "```\n",
    "\n",
    "例：\n",
    "- perplexity = 1.2 のモデル → 自信を持って対話できる\n",
    "- perplexity = 4.0 のモデル → 迷いがある対話\n",
    "\n",
    "より良い方（低いperplexity）のモデルを最終的に採用します。\n",
    "\n",
    "これらの設定により、効率的かつ効果的にソクラテス式の対話スタイルを学習できるようになっています。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "### 4.4 トレーニング監視システム実装\n",
    "class TrainingMonitorCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.train_start_time = None\n",
    "        self.metrics_history = {\n",
    "            'step': [],\n",
    "            'train_loss': [],\n",
    "            'eval_loss': [],\n",
    "            'learning_rate': [],\n",
    "            'perplexity': [],\n",
    "            'grad_norm': [],\n",
    "            'gpu_memory_usage': [],\n",
    "        }\n",
    "        self.output_dir = Path(f\"{BASE_OUTPUT_DIR}/training_progress\")\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # 安定性監視用の設定\n",
    "        self.window_size = 5  # 移動平均のウィンドウサイズ\n",
    "        self.last_checkpoint_step = 0\n",
    "        self.min_steps_between_checkpoints = 100  # チェックポイント間の最小ステップ数\n",
    "        self.stable_checkpoints = []  # 安定したチェックポイントを記録\n",
    "        \n",
    "        # メトリクスの閾値設定\n",
    "        self.perplexity_threshold = 2.5  # 良好なperplexityの閾値\n",
    "        self.eval_loss_variance_threshold = 0.1  # eval_lossの許容変動幅\n",
    "        self.grad_norm_bounds = (0.1, 2.0)  # grad_normの適正範囲\n",
    "        self.max_stable_checkpoints = 5  # 安定チェックポイントの最大数を設定\n",
    "        self.error_count = 0  # エラー回数を追跡\n",
    "        self.max_consecutive_errors = 3  # 連続エラーの許容回数\n",
    "        \n",
    "        # バランス監視用の設定を追加\n",
    "        self.variance_bias_window = 10  # より長いウィンドウで傾向を見る\n",
    "        self.train_losses = []  # 訓練損失の履歴\n",
    "        self.eval_losses = []   # 評価損失の履歴\n",
    "        self.optimal_gap_range = (0.1, 0.3)  # 訓練損失と評価損失の理想的な差分範囲\n",
    "    \n",
    "    def _calculate_stability_metrics(self, state):\n",
    "        \"\"\"安定性メトリクスを計算\"\"\"\n",
    "        if len(self.metrics_history['perplexity']) < self.window_size:\n",
    "            return None\n",
    "            \n",
    "        recent_perplexity = self.metrics_history['perplexity'][-self.window_size:]\n",
    "        recent_eval_loss = self.metrics_history['eval_loss'][-self.window_size:]\n",
    "        recent_grad_norm = self.metrics_history['grad_norm'][-self.window_size:]\n",
    "        \n",
    "        # 移動平均と標準偏差を計算\n",
    "        perplexity_mean = np.mean(recent_perplexity)\n",
    "        eval_loss_std = np.std(recent_eval_loss)\n",
    "        grad_norm_mean = np.mean(recent_grad_norm)\n",
    "        \n",
    "        return {\n",
    "            'perplexity_mean': perplexity_mean,\n",
    "            'eval_loss_std': eval_loss_std,\n",
    "            'grad_norm_mean': grad_norm_mean\n",
    "        }\n",
    "    \n",
    "    def _calculate_variance_bias_metrics(self):\n",
    "        \"\"\"分散と偏りのバランスを計算\"\"\"\n",
    "        if len(self.train_losses) < self.variance_bias_window or \\\n",
    "           len(self.eval_losses) < self.variance_bias_window:\n",
    "            return None\n",
    "            \n",
    "        recent_train = self.train_losses[-self.variance_bias_window:]\n",
    "        recent_eval = self.eval_losses[-self.variance_bias_window:]\n",
    "        \n",
    "        # 訓練損失と評価損失の差（バイアスの指標）\n",
    "        loss_gap = np.mean(recent_eval) - np.mean(recent_train)\n",
    "        \n",
    "        # 損失の変動（分散の指標）\n",
    "        train_variance = np.var(recent_train)\n",
    "        eval_variance = np.var(recent_eval)\n",
    "        \n",
    "        return {\n",
    "            'loss_gap': loss_gap,\n",
    "            'train_variance': train_variance,\n",
    "            'eval_variance': eval_variance,\n",
    "            'total_variance': (train_variance + eval_variance) / 2\n",
    "        }\n",
    "    \n",
    "    def _is_balanced_state(self, variance_bias_metrics):\n",
    "        \"\"\"バランスの取れた状態かを判断\"\"\"\n",
    "        if variance_bias_metrics is None:\n",
    "            return False\n",
    "            \n",
    "        # 理想的な差分範囲内にあるか\n",
    "        good_gap = (self.optimal_gap_range[0] <= variance_bias_metrics['loss_gap'] <= self.optimal_gap_range[1])\n",
    "        \n",
    "        # 分散が適度に小さいか\n",
    "        stable_variance = variance_bias_metrics['total_variance'] < 0.1\n",
    "        \n",
    "        # 訓練と評価の分散が近いか（安定性の指標）\n",
    "        variance_ratio = min(variance_bias_metrics['train_variance'], variance_bias_metrics['eval_variance']) / \\\n",
    "                        max(variance_bias_metrics['train_variance'], variance_bias_metrics['eval_variance'])\n",
    "        balanced_variance = variance_ratio > 0.7  # 70%以上の類似性\n",
    "        \n",
    "        return good_gap and stable_variance and balanced_variance\n",
    "    \n",
    "    def _should_save_checkpoint(self, state, metrics):\n",
    "        \"\"\"チェックポイント保存の判断を拡張\"\"\"\n",
    "        # 既存の条件をチェック\n",
    "        basic_conditions = super()._should_save_checkpoint(state, metrics)\n",
    "        \n",
    "        # バランス状態もチェック\n",
    "        variance_bias_metrics = self._calculate_variance_bias_metrics()\n",
    "        balanced_state = self._is_balanced_state(variance_bias_metrics)\n",
    "        \n",
    "        if balanced_state:\n",
    "            logging.info(f\"Found balanced state at step {state.global_step}\")\n",
    "            logging.info(f\"Variance-Bias metrics: {variance_bias_metrics}\")\n",
    "        \n",
    "        return basic_conditions or balanced_state  # どちらかの条件を満たせば保存\n",
    "    \n",
    "    def _safe_save_checkpoint(self, checkpoint_dir, state, metrics, stability_metrics):\n",
    "        \"\"\"安全にチェックポイントを保存\"\"\"\n",
    "        try:\n",
    "            # チェックポイントディレクトリの作成を試みる\n",
    "            os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "            \n",
    "            # モデルの保存を試みる\n",
    "            try:\n",
    "                self.trainer.save_model(checkpoint_dir)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Failed to save model checkpoint: {str(e)}\")\n",
    "                return False\n",
    "            \n",
    "            # メトリクス情報の保存を試みる\n",
    "            try:\n",
    "                metrics_path = os.path.join(checkpoint_dir, \"stability_metrics.json\")\n",
    "                with open(metrics_path, 'w') as f:\n",
    "                    json.dump({\n",
    "                        'step': state.global_step,\n",
    "                        'metrics': stability_metrics,\n",
    "                        'eval_metrics': metrics,\n",
    "                        'timestamp': datetime.now().isoformat()\n",
    "                    }, f, indent=2)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Failed to save metrics: {str(e)}\")\n",
    "                # メトリクスの保存に失敗してもチェックポイントは有効\n",
    "                \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.error_count += 1\n",
    "            logging.error(f\"Checkpoint creation failed (attempt {self.error_count}): {str(e)}\")\n",
    "            if self.error_count >= self.max_consecutive_errors:\n",
    "                logging.warning(\"Too many consecutive checkpoint errors. Will skip future checkpoint attempts.\")\n",
    "            return False\n",
    "    \n",
    "    def _safe_remove_checkpoint(self, checkpoint_path):\n",
    "        \"\"\"安全にチェックポイントを削除\"\"\"\n",
    "        try:\n",
    "            if os.path.exists(checkpoint_path):\n",
    "                shutil.rmtree(checkpoint_path)\n",
    "                logging.info(f\"Successfully removed old checkpoint: {checkpoint_path}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to remove old checkpoint {checkpoint_path}: {str(e)}\")\n",
    "    \n",
    "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "        \"\"\"評価時のチェックポイント判断\"\"\"\n",
    "        if not metrics:\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            stability_metrics = self._calculate_stability_metrics(state)\n",
    "            if stability_metrics and self._should_save_checkpoint(state, stability_metrics):\n",
    "                # 安定したチェックポイントとして保存を試みる\n",
    "                checkpoint_dir = os.path.join(\n",
    "                    args.output_dir,\n",
    "                    f\"stable_checkpoint-{state.global_step}\"\n",
    "                )\n",
    "                \n",
    "                if self._safe_save_checkpoint(checkpoint_dir, state, metrics, stability_metrics):\n",
    "                    self.stable_checkpoints.append({\n",
    "                        'step': state.global_step,\n",
    "                        'path': checkpoint_dir,\n",
    "                        'metrics': stability_metrics\n",
    "                    })\n",
    "                    self.last_checkpoint_step = state.global_step\n",
    "                    self.error_count = 0  # 成功したらエラーカウントをリセット\n",
    "                    logging.info(f\"Saved stable checkpoint at step {state.global_step}\")\n",
    "                    logging.info(f\"Stability metrics: {stability_metrics}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during evaluation callback: {str(e)}\")\n",
    "            # エラーが発生しても処理を継続\n",
    "    \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        \"\"\"トレーニング終了時の処理\"\"\"\n",
    "        try:\n",
    "            training_duration = datetime.now() - self.train_start_time\n",
    "            \n",
    "            # 安定したチェックポイントの概要を保存\n",
    "            try:\n",
    "                checkpoints_summary = os.path.join(self.output_dir, 'stable_checkpoints_summary.json')\n",
    "                with open(checkpoints_summary, 'w') as f:\n",
    "                    json.dump({\n",
    "                        'total_checkpoints': len(self.stable_checkpoints),\n",
    "                        'checkpoints': self.stable_checkpoints\n",
    "                    }, f, indent=2)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Failed to save checkpoints summary: {str(e)}\")\n",
    "            \n",
    "            # メトリクス履歴の保存を試みる\n",
    "            try:\n",
    "                history_file = self.output_dir / 'training_metrics.json'\n",
    "                with open(history_file, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(self.metrics_history, f, indent=2)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Failed to save training metrics: {str(e)}\")\n",
    "            \n",
    "            # 終了ログの出力\n",
    "            logging.info(f\"Training completed. Duration: {training_duration}\")\n",
    "            self._log_final_metrics()\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during training end callback: {str(e)}\")\n",
    "    \n",
    "    def _log_final_metrics(self):\n",
    "        \"\"\"最終メトリクスのログ出力\"\"\"\n",
    "        try:\n",
    "            if self.metrics_history['train_loss']:\n",
    "                logging.info(f\"Final training loss: {self.metrics_history['train_loss'][-1]:.4f}\")\n",
    "            if self.metrics_history['eval_loss']:\n",
    "                logging.info(f\"Final evaluation loss: {self.metrics_history['eval_loss'][-1]:.4f}\")\n",
    "            if self.metrics_history['perplexity']:\n",
    "                logging.info(f\"Final perplexity: {self.metrics_history['perplexity'][-1]:.4f}\")\n",
    "            logging.info(f\"Total stable checkpoints saved: {len(self.stable_checkpoints)}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error logging final metrics: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "\n",
    "\n",
    "トレーニング監視システムについて、ソクラテス式チャットボットの文脈で分かりやすく説明させていただきます。\n",
    "\n",
    "### 1. 基本的な監視項目（初期化部分）\n",
    "```python\n",
    "self.metrics_history = {\n",
    "    'step': [],            # 学習のステップ数\n",
    "    'train_loss': [],      # 学習時の誤差\n",
    "    'eval_loss': [],       # 評価時の誤差\n",
    "    'learning_rate': [],   # 学習率\n",
    "    'perplexity': [],      # モデルの確信度\n",
    "    'grad_norm': [],       # 学習の安定性\n",
    "    'gpu_memory_usage': [] # GPUメモリ使用量\n",
    "}\n",
    "```\n",
    "\n",
    "これは、学習の進行状況を記録する「学習日誌」のようなものです。\n",
    "\n",
    "### 2. 安定性の監視\n",
    "```python\n",
    "# 安定性監視用の設定\n",
    "self.window_size = 5  # 直近5回分の結果を見る\n",
    "self.perplexity_threshold = 2.5  # perplexityの目標値\n",
    "```\n",
    "\n",
    "例えば、ソクラテス式の対話で：\n",
    "- 直近5回の対話で：\n",
    "  - perplexity < 2.5 → 「安定して質問を返せている」\n",
    "  - perplexity > 2.5 → 「まだ迷いがある」\n",
    "\n",
    "### 3. 学習のバランス監視\n",
    "```python\n",
    "self.optimal_gap_range = (0.1, 0.3)  # 訓練と評価の理想的な差\n",
    "```\n",
    "\n",
    "これは「過学習」を防ぐための監視です。\n",
    "\n",
    "例：\n",
    "```\n",
    "訓練データでの会話：\n",
    "ユーザー: なぜ運動は大切ですか？\n",
    "ボット: あなたにとって運動とはどのような意味を持ちますか？\n",
    "（Loss: 0.2）\n",
    "\n",
    "評価データでの会話：\n",
    "ユーザー: なぜ読書は重要ですか？\n",
    "ボット: 読書の重要性について、あなたはどのようにお考えですか？\n",
    "（Loss: 0.4）\n",
    "\n",
    "差分: 0.2 → 適切なバランス\n",
    "```\n",
    "\n",
    "### 4. 安定性の計算（_calculate_stability_metrics）\n",
    "```python\n",
    "def _calculate_stability_metrics(self, state):\n",
    "    recent_perplexity = self.metrics_history['perplexity'][-self.window_size:]\n",
    "    # ...\n",
    "```\n",
    "\n",
    "直近の結果を分析して、モデルの安定性を確認します：\n",
    "- perplexityの平均値\n",
    "- 評価時の誤差のばらつき\n",
    "- 学習の安定度\n",
    "\n",
    "### 5. チェックポイントの保存判断\n",
    "```python\n",
    "def _should_save_checkpoint(self, state, metrics):\n",
    "    variance_bias_metrics = self._calculate_variance_bias_metrics()\n",
    "    balanced_state = self._is_balanced_state(variance_bias_metrics)\n",
    "```\n",
    "\n",
    "以下の場合にモデルを保存します：\n",
    "1. 基本条件を満たす（一定間隔）\n",
    "2. 特に良い結果が出た時\n",
    "\n",
    "例：\n",
    "```\n",
    "Step 1000:\n",
    "- perplexity: 2.0（良好）\n",
    "- 学習と評価のバランスが取れている\n",
    "→ このモデルを保存\n",
    "```\n",
    "\n",
    "### 6. トレーニング終了時の処理\n",
    "```python\n",
    "def on_train_end(self, args, state, control, **kwargs):\n",
    "    # 学習結果のまとめを保存\n",
    "```\n",
    "\n",
    "最終的な学習結果をまとめます：\n",
    "- 総学習時間\n",
    "- 最終的な性能\n",
    "- 保存したチェックポイントの一覧\n",
    "\n",
    "例：\n",
    "```\n",
    "学習完了レポート：\n",
    "- 学習時間: 12時間30分\n",
    "- 最終perplexity: 1.8\n",
    "- 安定チェックポイント: 5個保存\n",
    "```\n",
    "\n",
    "このように、モデルの学習過程を細かく監視し、問題があれば早期に発見できるようになっています。特にソクラテス式の対話モデルでは、一貫性のある質問の仕方を学習できているかが重要なので、この監視システムが重要な役割を果たします。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "### 4.5 トレーナー実装と初期化\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    "    pad_to_multiple_of=8\n",
    ")\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def training_step(self, *args, **kwargs):\n",
    "        loss = super().training_step(*args, **kwargs)\n",
    "        if self.state.global_step % 50 == 0:\n",
    "            clear_memory()\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "        return loss\n",
    "\n",
    "    def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix=\"eval\"):\n",
    "        eval_dataset = eval_dataset if eval_dataset is not None else self.eval_dataset\n",
    "        if eval_dataset is not None:\n",
    "            # Limit evaluation dataset to 100 samples\n",
    "            eval_dataset = eval_dataset.select(range(min(100, len(eval_dataset))))\n",
    "        return super().evaluate(eval_dataset, ignore_keys, metric_key_prefix)\n",
    "\n",
    "# Trainer initialization\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[TrainingMonitorCallback()],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "\n",
    "\n",
    "トレーナーの実装と初期化について、ソクラテス式チャットボットの文脈で分かりやすく説明させていただきます。\n",
    "\n",
    "### 1. データコレーター（データの整形役）\n",
    "```python\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,           # マスク言語モデリングを使用しない\n",
    "    pad_to_multiple_of=8 # データを8の倍数に揃える\n",
    ")\n",
    "```\n",
    "\n",
    "これは、対話データを学習に適した形に整えるツールです。\n",
    "\n",
    "例えば：\n",
    "```\n",
    "入力データ：\n",
    "ユーザー: 幸せとは何ですか？\n",
    "ボット: あなたにとって幸せとは何を意味しますか？\n",
    "\n",
    "↓ データコレーターが処理\n",
    "\n",
    "[トークン化されたデータ]\n",
    "[1234, 5678, 9012, ...] （8の倍数長に調整）\n",
    "```\n",
    "\n",
    "### 2. カスタムトレーナー\n",
    "```python\n",
    "class CustomTrainer(Trainer):\n",
    "    def training_step(self, *args, **kwargs):\n",
    "        loss = super().training_step(*args, **kwargs)\n",
    "        if self.state.global_step % 50 == 0:  # 50ステップごとに\n",
    "            clear_memory()                     # メモリクリア\n",
    "        return loss\n",
    "```\n",
    "\n",
    "通常のトレーナーを改良して：\n",
    "1. メモリ管理を強化\n",
    "2. 評価データサイズを制限\n",
    "\n",
    "例えば：\n",
    "```\n",
    "学習の流れ：\n",
    "Step 1-49: 通常の学習\n",
    "Step 50: メモリクリア\n",
    "Step 51-99: 通常の学習\n",
    "Step 100: メモリクリア\n",
    "...\n",
    "```\n",
    "\n",
    "### 3. 評価機能のカスタマイズ\n",
    "```python\n",
    "def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix=\"eval\"):\n",
    "    if eval_dataset is not None:\n",
    "        # 評価データを最大100サンプルに制限\n",
    "        eval_dataset = eval_dataset.select(range(min(100, len(eval_dataset))))\n",
    "```\n",
    "\n",
    "評価時の工夫：\n",
    "- 最大100の対話のみを評価\n",
    "- メモリ使用量を抑制\n",
    "- 評価時間を短縮\n",
    "\n",
    "例：\n",
    "```\n",
    "全評価データ: 500対話\n",
    "↓\n",
    "実際に使用: 100対話\n",
    "- 「なぜ...」で始まる質問への応答\n",
    "- 「どのように...」で始まる質問への応答\n",
    "など、バランスよく選択\n",
    "```\n",
    "\n",
    "### 4. トレーナーの初期化\n",
    "```python\n",
    "trainer = CustomTrainer(\n",
    "    model=model,                          # 学習するモデル\n",
    "    args=training_args,                   # 学習設定\n",
    "    train_dataset=train_dataset,          # 学習データ\n",
    "    eval_dataset=eval_dataset,            # 評価データ\n",
    "    data_collator=data_collator,         # データ整形ツール\n",
    "    compute_metrics=compute_metrics,      # 評価指標の計算\n",
    "    callbacks=[TrainingMonitorCallback()] # 学習監視システム\n",
    ")\n",
    "```\n",
    "\n",
    "これは「先生」を設定するようなものです：\n",
    "\n",
    "1. **教材の準備**\n",
    "   - train_dataset: 練習用の対話集\n",
    "   - eval_dataset: テスト用の対話集\n",
    "\n",
    "2. **教え方の設定**\n",
    "   - training_args: 学習のペース、方法\n",
    "   - data_collator: 教材の整理方法\n",
    "\n",
    "3. **進捗管理**\n",
    "   - compute_metrics: テストの採点方法\n",
    "   - TrainingMonitorCallback: 学習の記録係\n",
    "\n",
    "実際の学習例：\n",
    "```\n",
    "Step 1: 基本的な問いかけの練習\n",
    "ユーザー: 何故ですか？\n",
    "ボット: もう少し具体的に教えていただけますか？\n",
    "\n",
    "Step 100: より深い問いかけの練習\n",
    "ユーザー: 幸せとは何ですか？\n",
    "ボット: あなたにとって、幸せはどのような形で現れますか？\n",
    "\n",
    "Step 1000: 複雑な対話の練習\n",
    "ユーザー: AIと人間の関係性について\n",
    "ボット: その問いは興味深いですね。AIと人間の関係について、\n",
    "あなたはどのようなお考えをお持ちですか？\n",
    "```\n",
    "\n",
    "このように、効率的かつ効果的な学習を実現するための「先生」の役割を果たします。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
