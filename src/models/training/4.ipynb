{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 評価メトリクスとコールバック"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "この部分のコードについて、わかりやすく説明させていただきます。\n",
    "\n",
    "# 評価メトリクスとコールバックの主な役割\n",
    "\n",
    "このセクションは、AIモデルの学習過程を監視し、モデルがどれだけ「ソクラテス式の対話」をうまく再現できているかを評価する部分です。大きく分けて3つの主要な機能があります：\n",
    "\n",
    "## 1. 文章スタイルの評価 (compute_metrics関数)\n",
    "\n",
    "この関数は、モデルが生成した文章が「ソクラテス式の対話」らしさを持っているかを評価します。\n",
    "\n",
    "例えば：\n",
    "```python\n",
    "sentence_end_patterns = {\n",
    "    'question_patterns': [\n",
    "        'かね', 'だろうか', 'ではないか',\n",
    "        'のか', 'と思わないか', '考えてみよう',\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "これは、以下のような文末表現を探します：\n",
    "- 「それについてどう考えるかね？」\n",
    "- 「その考えは正しいのだろうか？」\n",
    "- 「別の視点から見てみてはどうだろうか？」\n",
    "\n",
    "このような表現がどれだけ適切に使われているかをスコア化します。\n",
    "\n",
    "## 2. 対話の流れの評価 (calculate_dialogue_flow関数)\n",
    "\n",
    "対話の自然な流れを評価します。具体的には：\n",
    "\n",
    "1. 質問と説明のバランス（理想は30%が質問文）\n",
    "2. 文の長さの変化（一定の長さに偏りすぎていないか）\n",
    "3. 接続詞の使用（「しかし」「だから」などで文章をつなげているか）\n",
    "\n",
    "例えば：\n",
    "```text\n",
    "ユーザー: 幸せとは何だと思いますか？\n",
    "AI: なるほど、幸せの定義について考えてみましょう。\n",
    "まず、あなたにとって幸せとは何でしょうか？\n",
    "そして、その幸せは誰もが共有できるものだと思いますか？\n",
    "```\n",
    "\n",
    "このような対話の流れが自然かどうかを評価します。\n",
    "\n",
    "## 3. システムリソースの監視 (TrainingMonitorCallback)\n",
    "\n",
    "学習中のコンピュータの状態を監視します：\n",
    "- メモリ使用量\n",
    "- GPU使用率\n",
    "- 学習の進行状況\n",
    "- エラーの検出\n",
    "\n",
    "これにより、学習が正常に進んでいるか、問題が発生していないかを確認できます。\n",
    "\n",
    "## まとめ\n",
    "\n",
    "このコードは、単にAIモデルを学習させるだけでなく、「ソクラテス式の対話」という特殊な対話スタイルをどれだけ正確に再現できているかを細かくチェックし、その品質を保証する役割を果たしています。また、学習中のシステムの健全性も同時に監視することで、安定した学習プロセスを実現しています。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Update evaluation metrics\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds  # Get logits and labels from eval_preds\n",
    "    \n",
    "    # Relax size limit for evaluation dataset\n",
    "    max_samples = 100\n",
    "    \n",
    "    # Improve decoding process\n",
    "    with torch.no_grad():\n",
    "        logits = torch.tensor(logits).cpu()\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        # Decode batch\n",
    "        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "        \n",
    "        # Add more detailed logging\n",
    "        logging.info(f\"Sample prediction: {decoded_preds[0][:100]}...\")\n",
    "        \n",
    "        del logits, predictions  # Memory release\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Define sentence ending patterns more flexibly\n",
    "        sentence_end_patterns = {\n",
    "            'question_patterns': [\n",
    "                'かね', 'だろうか', 'ではないか',\n",
    "                'のか', 'と思わないか', '考えてみよう',\n",
    "            ],\n",
    "            'statement_patterns': [\n",
    "                'だね', 'なるほど', '興味深い',\n",
    "                'といえよう', 'というべきだ'\n",
    "            ],\n",
    "            'reflection_patterns': [\n",
    "                'かもしれない', 'のではないか',\n",
    "                'と考えられる', 'といえそうだ'\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Auxiliary verb patterns\n",
    "        auxiliary_patterns = [\n",
    "            'である', 'だ', 'です', 'ます',\n",
    "            'のだ', 'のです', 'のである'\n",
    "        ]\n",
    "        \n",
    "        def calculate_style_consistency(text):\n",
    "            sentences = text.split('。')\n",
    "            if not sentences:\n",
    "                return 0.0\n",
    "                \n",
    "            # Evaluate sentence ending style consistency\n",
    "            end_style_scores = []\n",
    "            for sent in sentences:\n",
    "                if not sent.strip():\n",
    "                    continue\n",
    "                    \n",
    "                # Evaluate sentence ending patterns (partial match allowed)\n",
    "                pattern_found = False\n",
    "                for pattern_type, patterns in sentence_end_patterns.items():\n",
    "                    if any(p in sent[-10:] for p in patterns):  # Search within 10 characters at the end\n",
    "                        pattern_found = True\n",
    "                        break\n",
    "                end_style_scores.append(1.0 if pattern_found else 0.0)\n",
    "            \n",
    "            # Evaluate auxiliary verb consistency\n",
    "            aux_style_scores = []\n",
    "            for sent in sentences:\n",
    "                if not sent.strip():\n",
    "                    continue\n",
    "                    \n",
    "                # Evaluate auxiliary verb usage in the sentence\n",
    "                aux_found = any(p in sent for p in auxiliary_patterns)\n",
    "                aux_style_scores.append(1.0 if aux_found else 0.0)\n",
    "            \n",
    "            # Evaluate sentence length consistency\n",
    "            lengths = [len(s.strip()) for s in sentences if s.strip()]\n",
    "            length_variance = np.var(lengths) if lengths else 0\n",
    "            length_score = 1.0 / (1.0 + length_variance/100)  # Higher score if variance is small\n",
    "            \n",
    "            # Overall evaluation\n",
    "            end_style_avg = np.mean(end_style_scores) if end_style_scores else 0\n",
    "            aux_style_avg = np.mean(aux_style_scores) if aux_style_scores else 0\n",
    "            \n",
    "            # Weighting\n",
    "            weights = {\n",
    "                'end_style': 0.5,\n",
    "                'aux_style': 0.3,\n",
    "                'length_consistency': 0.2\n",
    "            }\n",
    "            \n",
    "            return (\n",
    "                weights['end_style'] * end_style_avg +\n",
    "                weights['aux_style'] * aux_style_avg +\n",
    "                weights['length_consistency'] * length_score\n",
    "            )\n",
    "        \n",
    "        # Evaluate style consistency for each prediction\n",
    "        style_scores = [calculate_style_consistency(pred) for pred in decoded_preds]\n",
    "        \n",
    "        # Evaluate dialogue flow\n",
    "        def calculate_dialogue_flow(text):\n",
    "            sentences = text.split('。')\n",
    "            if not sentences:\n",
    "                return 0.0\n",
    "            \n",
    "            # 質問文判定の改善\n",
    "            question_markers = {\n",
    "                'explicit': ['？', '?'],  # 明示的な質問符号\n",
    "                'patterns': [\n",
    "                    'かね', 'だろうか', 'ではないか', 'のか', \n",
    "                    'と思わないか', '考えてみよう',\n",
    "                    'どう', 'いかが', 'なぜ', 'どのように',\n",
    "                    '問', '教えて', '聞かせて'\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "            def is_question(sentence):\n",
    "                # 明示的な質問符号のチェック\n",
    "                if any(marker in sentence for marker in question_markers['explicit']):\n",
    "                    return True\n",
    "                # 質問パターンのチェック\n",
    "                if any(pattern in sentence for pattern in question_markers['patterns']):\n",
    "                    return True\n",
    "                return False\n",
    "            \n",
    "            # 各文を評価\n",
    "            questions = sum(1 for s in sentences if is_question(s))\n",
    "            total_sentences = len([s for s in sentences if s.strip()])\n",
    "            ratio = questions / total_sentences if total_sentences else 0\n",
    "            \n",
    "            # 理想の比率(0.3)からの距離に基づいてスコアを計算\n",
    "            balance_score = max(0.0, 1.0 - min(abs(0.3 - ratio), 0.2) * 2)\n",
    "            \n",
    "            # 2. Sentence length change\n",
    "            lengths = [len(s.strip()) for s in sentences if s.strip()]\n",
    "            length_variance = np.var(lengths) if len(lengths) > 1 else 0\n",
    "            length_score = 1.0 / (1.0 + length_variance/500)  # Higher score if variance is small\n",
    "            \n",
    "            # 3. Use of conjunctions\n",
    "            conjunctions = ['しかし', 'だから', 'また', 'そして', 'したがって']\n",
    "            conj_count = sum(1 for s in sentences if any(c in s for c in conjunctions))\n",
    "            conj_ratio = conj_count / len(sentences)\n",
    "            conj_score = min(1.0, conj_ratio * 2)  # Evaluate moderate usage\n",
    "            \n",
    "            # Weighted average of scores\n",
    "            weights = [0.5, 0.25, 0.25]  # Balance, length, conjunction weights\n",
    "            final_score = sum(s * w for s, w in zip([balance_score, length_score, conj_score], weights))\n",
    "            \n",
    "            return max(0.1, min(1.0, final_score))  # Limit to range 0.1 to 1.0\n",
    "        \n",
    "        flow_scores = [calculate_dialogue_flow(pred) for pred in decoded_preds]\n",
    "        \n",
    "        style_score = np.mean(style_scores)\n",
    "        flow_score = np.mean(flow_scores)\n",
    "        \n",
    "        # Add overall evaluation score\n",
    "        combined_score = (style_score * 0.6 + flow_score * 0.4)  # Increase flow_score weight\n",
    "        \n",
    "        return {\n",
    "            'style_consistency': style_score,\n",
    "            'dialogue_flow': flow_score,\n",
    "            'combined_score': combined_score\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "このコードについて、ソクラテス式対話チャットボットの文脈で説明させていただきます。\n",
    "\n",
    "# 評価メトリクスの概要\n",
    "\n",
    "このコードは、チャットボットの応答の質を評価する仕組みを実装しています。主に以下の2つの側面から評価を行っています：\n",
    "\n",
    "1. スタイルの一貫性 (`style_consistency`)\n",
    "2. 対話の流れ (`dialogue_flow`)\n",
    "\n",
    "## スタイルの一貫性の評価\n",
    "\n",
    "```python\n",
    "sentence_end_patterns = {\n",
    "    'question_patterns': [\n",
    "        'かね', 'だろうか', 'ではないか',\n",
    "        'のか', 'と思わないか', '考えてみよう',\n",
    "    ],\n",
    "    # ...\n",
    "}\n",
    "```\n",
    "\n",
    "### 具体例：\n",
    "元の応答：\n",
    "```\n",
    "その考えは興味深いですね。しかし、なぜそのように考えるのでしょうか？私たちは、この問題についてもう少し深く考えてみる必要があるのではないでしょうか。\n",
    "```\n",
    "\n",
    "このような応答は以下の点で高評価となります：\n",
    "- 「興味深い」という反応的表現の使用\n",
    "- 「のでしょうか」という質問形式\n",
    "- 「ではないでしょうか」というソクラテス式の問いかけ\n",
    "\n",
    "## 対話の流れの評価\n",
    "\n",
    "```python\n",
    "def calculate_dialogue_flow(text):\n",
    "    # 質問と説明のバランスを評価\n",
    "    ratio = questions / total_sentences\n",
    "    # 理想の比率(0.3 = 30%が質問文)を目指す\n",
    "```\n",
    "\n",
    "### 具体例：\n",
    "良い応答の例：\n",
    "```\n",
    "その点について、私も考えを巡らせていました（説明）。\n",
    "しかし、そもそもなぜその前提に立つのでしょうか？（質問）\n",
    "もし別の角度から見たとすれば、どのような可能性が見えてくるでしょうか？（質問）\n",
    "```\n",
    "\n",
    "この応答は以下の理由で高評価となります：\n",
    "- 質問と説明のバランスが良い（約30%が質問文）\n",
    "- 「しかし」という接続詞の適切な使用\n",
    "- 文の長さに極端な差がない\n",
    "\n",
    "## 評価の重み付け\n",
    "\n",
    "```python\n",
    "combined_score = (style_score * 0.6 + flow_score * 0.4)\n",
    "```\n",
    "\n",
    "最終的な評価は：\n",
    "- スタイルの一貫性（60%）\n",
    "  - 文末表現の一貫性（50%）\n",
    "  - 助動詞の使用（30%）\n",
    "  - 文長の一貫性（20%）\n",
    "- 対話の流れ（40%）\n",
    "  - 質問と説明のバランス（50%）\n",
    "  - 文の長さの変化（25%）\n",
    "  - 接続詞の使用（25%）\n",
    "\n",
    "### 具体例：\n",
    "低評価となる応答：\n",
    "```\n",
    "はい。そうですね。わかりました。\n",
    "```\n",
    "理由：\n",
    "- 文末表現が単調\n",
    "- 質問がない\n",
    "- 接続詞がない\n",
    "- 文が短すぎる\n",
    "\n",
    "高評価となる応答：\n",
    "```\n",
    "なるほど、その視点は興味深いですね。\n",
    "しかし、その考えの根底にある前提について、もう少し掘り下げて考えてみましょうか。\n",
    "たとえば、この状況を別の角度から見たとき、どのような可能性が見えてくるのでしょうか。\n",
    "```\n",
    "理由：\n",
    "- 文末表現が多様（「ですね」「みましょうか」「でしょうか」）\n",
    "- 適度な質問の含有率\n",
    "- 接続詞の適切な使用（「しかし」「たとえば」）\n",
    "- 文の長さのバランスが良い\n",
    "\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Add memory usage monitoring log\n",
    "def log_memory_usage():\n",
    "    import psutil\n",
    "    import torch\n",
    "    \n",
    "    # CPU memory\n",
    "    process = psutil.Process()\n",
    "    cpu_memory = process.memory_info().rss / 1024 / 1024  # MB\n",
    "    \n",
    "    # GPU memory\n",
    "    gpu_memory = []\n",
    "    if torch.cuda.is_available():\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            gpu_memory.append({\n",
    "                'device': i,\n",
    "                'allocated': torch.cuda.memory_allocated(i) / 1024 / 1024,  # MB\n",
    "                'reserved': torch.cuda.memory_reserved(i) / 1024 / 1024,    # MB\n",
    "                'max_allocated': torch.cuda.max_memory_allocated(i) / 1024 / 1024  # MB\n",
    "            })\n",
    "    \n",
    "    logging.info(f\"CPU Memory usage: {cpu_memory:.2f} MB\")\n",
    "    for gpu in gpu_memory:\n",
    "        logging.info(f\"GPU {gpu['device']} Memory:\")\n",
    "        logging.info(f\"  - Allocated: {gpu['allocated']:.2f} MB\")\n",
    "        logging.info(f\"  - Reserved: {gpu['reserved']:.2f} MB\")\n",
    "        logging.info(f\"  - Max Allocated: {gpu['max_allocated']:.2f} MB\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "このコードはメモリ使用状況を監視・記録するための関数について説明します。\n",
    "\n",
    "# メモリ監視関数の解説\n",
    "\n",
    "```python\n",
    "def log_memory_usage():\n",
    "```\n",
    "\n",
    "この関数は、CPUとGPUのメモリ使用状況を監視し、ログに記録します。\n",
    "\n",
    "## 1. CPU メモリの監視\n",
    "\n",
    "```python\n",
    "# CPU memory\n",
    "process = psutil.Process()\n",
    "cpu_memory = process.memory_info().rss / 1024 / 1024  # MB\n",
    "```\n",
    "\n",
    "- `psutil.Process()`：現在実行中のPythonプロセスを取得\n",
    "- `rss`（Resident Set Size）：プロセスが使用している実メモリ量\n",
    "- 単位変換：バイトからメガバイト（MB）に変換（1024で2回割る）\n",
    "\n",
    "### 例：\n",
    "```\n",
    "CPU Memory usage: 8542.45 MB\n",
    "```\n",
    "これは約8.5GBのメモリを使用していることを示します。\n",
    "\n",
    "## 2. GPU メモリの監視\n",
    "\n",
    "```python\n",
    "# GPU memory\n",
    "gpu_memory = []\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "```\n",
    "\n",
    "GPUが利用可能な場合、各GPUについて以下の情報を収集：\n",
    "\n",
    "1. **Allocated Memory（割り当て済みメモリ）**\n",
    "```python\n",
    "'allocated': torch.cuda.memory_allocated(i) / 1024 / 1024\n",
    "```\n",
    "- 実際にPyTorchが使用中のGPUメモリ量\n",
    "- 例：モデルの重みやテンソルが使用中のメモリ\n",
    "\n",
    "2. **Reserved Memory（予約済みメモリ）**\n",
    "```python\n",
    "'reserved': torch.cuda.memory_reserved(i) / 1024 / 1024\n",
    "```\n",
    "- PyTorchが確保しているが、まだ使用していないメモリ量\n",
    "- キャッシュとして確保されている領域\n",
    "\n",
    "3. **Max Allocated Memory（最大割り当てメモリ）**\n",
    "```python\n",
    "'max_allocated': torch.cuda.max_memory_allocated(i) / 1024 / 1024\n",
    "```\n",
    "- プログラム開始からの最大メモリ使用量\n",
    "- メモリリークの検出に有用\n",
    "\n",
    "### 出力例：\n",
    "```\n",
    "GPU 0 Memory:\n",
    "  - Allocated: 3584.25 MB\n",
    "  - Reserved: 4096.00 MB\n",
    "  - Max Allocated: 3842.12 MB\n",
    "\n",
    "GPU 1 Memory:\n",
    "  - Allocated: 3621.83 MB\n",
    "  - Reserved: 4096.00 MB\n",
    "  - Max Allocated: 3912.45 MB\n",
    "```\n",
    "\n",
    "## 実用的な使用例\n",
    "\n",
    "このソクラテス式対話モデルの学習では、以下のような場面で特に重要です：\n",
    "\n",
    "1. **モデルロード時**\n",
    "```python\n",
    "# モデルロード前後でメモリ使用量を確認\n",
    "log_memory_usage()  # Before\n",
    "model = AutoModelForCausalLM.from_pretrained(...)\n",
    "log_memory_usage()  # After\n",
    "```\n",
    "\n",
    "2. **バッチ処理時**\n",
    "```python\n",
    "# 大きなバッチを処理する前後でメモリをチェック\n",
    "log_memory_usage()  # Before batch\n",
    "# バッチ処理\n",
    "log_memory_usage()  # After batch\n",
    "```\n",
    "\n",
    "3. **メモリリーク検出**\n",
    "```python\n",
    "# 定期的なメモリ使用量の記録\n",
    "for epoch in range(epochs):\n",
    "    log_memory_usage()  # 各エポックの開始時\n",
    "    # 学習処理\n",
    "    log_memory_usage()  # 各エポックの終了時\n",
    "```\n",
    "\n",
    "このモニタリングにより、以下のような問題を早期に発見できます：\n",
    "- メモリリーク\n",
    "- GPUメモリの不均衡な使用\n",
    "- 予期せぬメモリ消費の急増\n",
    "\n",
    "これは特に大規模な言語モデルの学習において重要で、メモリ関連の問題を事前に検出し、学習の安定性を確保するのに役立ちます。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Add memory cleanup\n",
    "def clear_memory():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "\n",
    "\n",
    "# メモリクリーンアップ関数の解説\n",
    "\n",
    "```python\n",
    "def clear_memory():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "```\n",
    "\n",
    "この関数は、CPUとGPUのメモリを解放するためのシンプルだが重要な機能を提供します。\n",
    "\n",
    "## 1. `gc.collect()`\n",
    "\n",
    "```python\n",
    "gc.collect()\n",
    "```\n",
    "\n",
    "- **目的**: Pythonのガベージコレクション（不要なメモリの解放）を手動で実行\n",
    "- **動作**: 参照されなくなったPythonオブジェクトを検出し、メモリから解放\n",
    "- **重要性**: 大規模な言語モデルの学習では、大量のテンソルや中間データが生成されるため、定期的なクリーンアップが必要\n",
    "\n",
    "### 使用例：\n",
    "```python\n",
    "# 大きなデータ処理後のクリーンアップ\n",
    "del large_dataset  # 明示的な削除\n",
    "clear_memory()     # メモリの解放\n",
    "```\n",
    "\n",
    "## 2. `torch.cuda.empty_cache()`\n",
    "\n",
    "```python\n",
    "torch.cuda.empty_cache()\n",
    "```\n",
    "\n",
    "- **目的**: GPUのキャッシュメモリを解放\n",
    "- **動作**: PyTorchが予約しているが使用していないGPUメモリを解放\n",
    "- **重要性**: GPU メモリの効率的な使用のため、特に長時間の学習セッションで重要\n",
    "\n",
    "### 使用例：\n",
    "```python\n",
    "# バッチ処理後のGPUメモリクリーンアップ\n",
    "del batch_output  # テンソルの削除\n",
    "clear_memory()    # キャッシュの解放\n",
    "```\n",
    "\n",
    "## 実践的な使用シーン\n",
    "\n",
    "### 1. エポック間のクリーンアップ\n",
    "```python\n",
    "for epoch in range(epochs):\n",
    "    # 学習処理\n",
    "    trainer.train()\n",
    "    \n",
    "    # エポック終了時のクリーンアップ\n",
    "    clear_memory()\n",
    "    log_memory_usage()  # メモリ使用状況の確認\n",
    "```\n",
    "\n",
    "### 2. 大きなバッチ処理後\n",
    "```python\n",
    "# 大きなバッチの処理\n",
    "outputs = model(large_batch)\n",
    "loss = outputs.loss\n",
    "loss.backward()\n",
    "\n",
    "# メモリの解放\n",
    "del outputs\n",
    "del loss\n",
    "clear_memory()\n",
    "```\n",
    "\n",
    "### 3. モデルの切り替え時\n",
    "```python\n",
    "# 古いモデルの解放\n",
    "del old_model\n",
    "clear_memory()\n",
    "\n",
    "# 新しいモデルのロード\n",
    "new_model = AutoModelForCausalLM.from_pretrained(...)\n",
    "```\n",
    "\n",
    "## メモリ管理の重要性\n",
    "\n",
    "ソクラテス式対話モデルの学習では、以下の理由でメモリ管理が特に重要です：\n",
    "\n",
    "1. **大規模なモデルサイズ**\n",
    "   - 基本モデル（Gemma 2B）自体が大きい\n",
    "   - LoRAパラメータの追加\n",
    "   - 中間層の活性化値\n",
    "\n",
    "2. **長い系列長**\n",
    "   - 対話形式のため、入力テキストが長くなりやすい\n",
    "   - アテンションメカニズムのメモリ要求\n",
    "\n",
    "3. **バッチ処理**\n",
    "   - 複数の対話サンプルを同時に処理\n",
    "   - グラデーント蓄積\n",
    "\n",
    "### メモリ問題の例と対処：\n",
    "\n",
    "```python\n",
    "try:\n",
    "    # 大きなバッチの処理\n",
    "    outputs = model(large_batch)\n",
    "except RuntimeError as e:\n",
    "    if \"out of memory\" in str(e):\n",
    "        # メモリ不足時の対処\n",
    "        clear_memory()\n",
    "        # バッチサイズを半分に\n",
    "        outputs = model(large_batch[:len(large_batch)//2])\n",
    "```\n",
    "\n",
    "このような適切なメモリ管理により：\n",
    "- 学習の安定性が向上\n",
    "- OOMエラー（Out of Memory）を防止\n",
    "- より効率的なGPUリソースの使用\n",
    "が可能になります。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Update custom callbacks\n",
    "class StyleCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.style_scores = []\n",
    "        self.flow_scores = []\n",
    "        \n",
    "    def on_evaluate(self, args, state, control, metrics, **kwargs):\n",
    "        if 'eval_style_consistency' in metrics:\n",
    "            self.style_scores.append(metrics['eval_style_consistency'])\n",
    "            self.flow_scores.append(metrics['eval_dialogue_flow'])\n",
    "            \n",
    "            # Log detailed information\n",
    "            logging.info(f\"Step {state.global_step}:\")\n",
    "            logging.info(f\"Style Consistency: {metrics['eval_style_consistency']:.3f}\")\n",
    "            logging.info(f\"Dialogue Flow: {metrics['eval_dialogue_flow']:.3f}\")\n",
    "    \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        # Log overall evaluation\n",
    "        avg_style = sum(self.style_scores) / len(self.style_scores) if self.style_scores else 0\n",
    "        avg_flow = sum(self.flow_scores) / len(self.flow_scores) if self.flow_scores else 0\n",
    "        \n",
    "        logging.info(\"Training Complete!\")\n",
    "        logging.info(f\"Average Style Consistency: {avg_style:.3f}\")\n",
    "        logging.info(f\"Average Dialogue Flow: {avg_flow:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "\n",
    "\n",
    "# StyleCallbackクラスの解説\n",
    "\n",
    "このクラスは、学習中のモデルのスタイル一貫性と対話の流れを監視するためのカスタムコールバックです。\n",
    "\n",
    "## クラスの基本構造\n",
    "\n",
    "```python\n",
    "class StyleCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.style_scores = []  # スタイル一貫性スコアを保存\n",
    "        self.flow_scores = []   # 対話の流れスコアを保存\n",
    "```\n",
    "\n",
    "## 1. 評価時の処理 (`on_evaluate`)\n",
    "\n",
    "```python\n",
    "def on_evaluate(self, args, state, control, metrics, **kwargs):\n",
    "    if 'eval_style_consistency' in metrics:\n",
    "        self.style_scores.append(metrics['eval_style_consistency'])\n",
    "        self.flow_scores.append(metrics['eval_dialogue_flow'])\n",
    "```\n",
    "\n",
    "### 具体例：\n",
    "\n",
    "評価時のログ出力：\n",
    "```\n",
    "Step 100:\n",
    "Style Consistency: 0.856\n",
    "Dialogue Flow: 0.742\n",
    "```\n",
    "\n",
    "これは以下を示します：\n",
    "- 学習の100ステップ目\n",
    "- スタイル一貫性スコア: 85.6%\n",
    "  - 例：「なるほど」「考えてみましょうか」などのソクラテス式の表現が適切に使用されている\n",
    "- 対話の流れスコア: 74.2%\n",
    "  - 例：質問と説明のバランスが良好\n",
    "\n",
    "## 2. 学習終了時の処理 (`on_train_end`)\n",
    "\n",
    "```python\n",
    "def on_train_end(self, args, state, control, **kwargs):\n",
    "    avg_style = sum(self.style_scores) / len(self.style_scores) if self.style_scores else 0\n",
    "    avg_flow = sum(self.flow_scores) / len(self.flow_scores) if self.flow_scores else 0\n",
    "```\n",
    "\n",
    "### 具体例：\n",
    "\n",
    "学習終了時のログ出力：\n",
    "```\n",
    "Training Complete!\n",
    "Average Style Consistency: 0.823\n",
    "Average Dialogue Flow: 0.751\n",
    "```\n",
    "\n",
    "これは全学習期間を通じての平均スコアを示します：\n",
    "- 平均スタイル一貫性: 82.3%\n",
    "- 平均対話の流れ: 75.1%\n",
    "\n",
    "## 実際の使用例\n",
    "\n",
    "```python\n",
    "# トレーナーの設定\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    callbacks=[StyleCallback()],  # コールバックの追加\n",
    "    # ...\n",
    ")\n",
    "\n",
    "# 学習中の出力例：\n",
    "\"\"\"\n",
    "Step 50:\n",
    "Style Consistency: 0.721\n",
    "Dialogue Flow: 0.689\n",
    "\n",
    "Step 100:\n",
    "Style Consistency: 0.856\n",
    "Dialogue Flow: 0.742\n",
    "\n",
    "...\n",
    "\n",
    "Training Complete!\n",
    "Average Style Consistency: 0.823\n",
    "Average Dialogue Flow: 0.751\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "## スコアの解釈\n",
    "\n",
    "### スタイル一貫性スコア (Style Consistency)\n",
    "- **高スコア（0.8以上）の例**：\n",
    "```\n",
    "なるほど、その考えは興味深いですね。\n",
    "しかし、その前提について、もう少し掘り下げて考えてみましょうか？\n",
    "```\n",
    "\n",
    "- **低スコア（0.5以下）の例**：\n",
    "```\n",
    "はい、そうです。\n",
    "わかりました。\n",
    "次に進みましょう。\n",
    "```\n",
    "\n",
    "### 対話の流れスコア (Dialogue Flow)\n",
    "- **高スコア（0.8以上）の例**：\n",
    "```\n",
    "その視点は確かに重要ですね。\n",
    "しかし、ここで一つ質問させていただきたいのですが、\n",
    "なぜそのような結論に至ったのでしょうか？\n",
    "```\n",
    "\n",
    "- **低スコア（0.5以下）の例**：\n",
    "```\n",
    "それは違います。\n",
    "なぜですか？\n",
    "どうしてですか？\n",
    "どう思いますか？\n",
    "```\n",
    "\n",
    "## 活用方法\n",
    "\n",
    "1. **学習の進捗モニタリング**\n",
    "   - 定期的なスコアの確認\n",
    "   - 急激なスコアの低下を検知\n",
    "\n",
    "2. **モデルの改善**\n",
    "   - スコアの傾向から問題点を特定\n",
    "   - 必要に応じてハイパーパラメータを調整\n",
    "\n",
    "3. **最終評価**\n",
    "   - モデルの全体的な性能を評価\n",
    "   - 異なるモデルバージョンの比較\n",
    "\n",
    "このコールバックにより、モデルがソクラテス式の対話スタイルを適切に学習できているかを定量的に評価することができます。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Extend custom callbacks\n",
    "class TrainingMonitorCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        # Import psutil here as well for safety\n",
    "        import psutil\n",
    "        self.train_start_time = None\n",
    "        self.metrics_history = {\n",
    "            'step': [],\n",
    "            'style_consistency': [],\n",
    "            'dialogue_flow': [],\n",
    "            'combined_score': [],\n",
    "            'loss': [],\n",
    "            'learning_rate': [],\n",
    "            'epoch': [],\n",
    "            'cpu_ram_usage': [],\n",
    "            'gpu_vram_usage': [],\n",
    "            'gpu_utilization': [],\n",
    "            'batch_size': [],\n",
    "            'moving_avg_loss': [],\n",
    "            # 新しい詳細メトリクス\n",
    "            'lr_schedule': [],\n",
    "            'batch_metrics': [],\n",
    "            'gpu_metrics': [],\n",
    "            'grad_norm': []\n",
    "        }\n",
    "        self.peak_metrics = {\n",
    "            'cpu_ram': 0,\n",
    "            'gpu_vram': 0,\n",
    "            'gpu_util': 0\n",
    "        }\n",
    "        self.output_dir = Path(f\"{BASE_OUTPUT_DIR}/training_progress\")\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "    def _record_resource_usage(self):\n",
    "        \"\"\"Record current resource usage with timestamp\"\"\"\n",
    "        import psutil\n",
    "        import torch\n",
    "        current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        \n",
    "        # CPU RAM\n",
    "        cpu_ram = psutil.Process().memory_info().rss / (1024 * 1024 * 1024)  # GB\n",
    "        self.peak_metrics['cpu_ram'] = max(self.peak_metrics['cpu_ram'], cpu_ram)\n",
    "        \n",
    "        # GPU metrics with timestamp\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_metrics = []\n",
    "            for i in range(torch.cuda.device_count()):\n",
    "                vram_used = torch.cuda.memory_allocated(i) / (1024 * 1024 * 1024)  # GB\n",
    "                self.peak_metrics['gpu_vram'] = max(self.peak_metrics['gpu_vram'], vram_used)\n",
    "                \n",
    "                # GPU utilization (requires nvidia-smi)\n",
    "                try:\n",
    "                    import subprocess\n",
    "                    result = subprocess.check_output(['nvidia-smi', '--query-gpu=utilization.gpu', '--format=csv,noheader,nounits'])\n",
    "                    gpu_util = float(result.decode('utf-8').strip())\n",
    "                    self.peak_metrics['gpu_util'] = max(self.peak_metrics['gpu_util'], gpu_util)\n",
    "                except:\n",
    "                    gpu_util = 0\n",
    "                \n",
    "                gpu_metrics.append({\n",
    "                    'device': i,\n",
    "                    'vram_used': vram_used,\n",
    "                    'utilization': gpu_util\n",
    "                })\n",
    "                \n",
    "            # 時系列データとして保存\n",
    "            self.metrics_history['gpu_metrics'].append({\n",
    "                'timestamp': current_time,\n",
    "                'metrics': gpu_metrics\n",
    "            })\n",
    "                \n",
    "            self.metrics_history['cpu_ram_usage'].append(cpu_ram)\n",
    "            self.metrics_history['gpu_vram_usage'].append(vram_used)\n",
    "            self.metrics_history['gpu_utilization'].append(gpu_util)\n",
    "    \n",
    "    def on_train_begin(self, args, state, control, **kwargs):\n",
    "        self.train_start_time = datetime.now()\n",
    "        logging.info(\"Training started at: %s\", self.train_start_time)\n",
    "        self._record_resource_usage()\n",
    "        \n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs:\n",
    "            current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            \n",
    "            # 学習率とスケジューリングの記録\n",
    "            if 'learning_rate' in logs:\n",
    "                self.metrics_history['lr_schedule'].append({\n",
    "                    'timestamp': current_time,\n",
    "                    'step': state.global_step,\n",
    "                    'learning_rate': logs['learning_rate'],\n",
    "                    'schedule_type': args.lr_scheduler_type\n",
    "                })\n",
    "                self.metrics_history['learning_rate'].append(logs['learning_rate'])\n",
    "            \n",
    "            # バッチサイズと損失値の関連を記録\n",
    "            if 'loss' in logs:\n",
    "                self.metrics_history['batch_metrics'].append({\n",
    "                    'timestamp': current_time,\n",
    "                    'step': state.global_step,\n",
    "                    'batch_size': args.per_device_train_batch_size,\n",
    "                    'loss': logs['loss'],\n",
    "                    'grad_norm': logs.get('grad_norm', None)\n",
    "                })\n",
    "                self.metrics_history['loss'].append(logs['loss'])\n",
    "                self.metrics_history['batch_size'].append(args.per_device_train_batch_size)\n",
    "                if 'grad_norm' in logs:\n",
    "                    self.metrics_history['grad_norm'].append(logs['grad_norm'])\n",
    "            \n",
    "            # 移動平均の計算と記録\n",
    "            if len(self.metrics_history['loss']) > 10:\n",
    "                avg_loss = sum(self.metrics_history['loss'][-10:]) / 10\n",
    "                self.metrics_history['moving_avg_loss'].append(avg_loss)\n",
    "                logging.info(f\"Moving average loss (last 10 steps): {avg_loss:.4f}\")\n",
    "            \n",
    "            logging.info(f\"Step {state.global_step}: {logs}\")\n",
    "            if 'grad_norm' in logs:\n",
    "                logging.info(f\"Gradient norm: {logs['grad_norm']:.4f}\")\n",
    "            \n",
    "        self._record_resource_usage()\n",
    "        \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        training_duration = datetime.now() - self.train_start_time\n",
    "        \n",
    "        # 詳細な学習履歴の保存\n",
    "        training_history = {\n",
    "            'lr_schedule': self.metrics_history['lr_schedule'],\n",
    "            'batch_metrics': self.metrics_history['batch_metrics'],\n",
    "            'gpu_metrics': self.metrics_history['gpu_metrics'],\n",
    "            'moving_avg_loss': self.metrics_history['moving_avg_loss']\n",
    "        }\n",
    "        \n",
    "        # 学習履歴をJSONファイルとして保存\n",
    "        history_file = self.output_dir / 'training_history.json'\n",
    "        with open(history_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(training_history, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        # 基本的なメトリクスのログ出力\n",
    "        logging.info(f\"Training completed. Total duration: {training_duration}\")\n",
    "        logging.info(f\"Peak CPU RAM usage: {self.peak_metrics['cpu_ram']:.2f} GB\")\n",
    "        logging.info(f\"Peak GPU VRAM usage: {self.peak_metrics['gpu_vram']:.2f} GB\")\n",
    "        logging.info(f\"Peak GPU utilization: {self.peak_metrics['gpu_util']:.1f}%\")\n",
    "        \n",
    "        # 最終サマリーの作成と保存\n",
    "        summary = {\n",
    "            'training_duration': str(training_duration),\n",
    "            'final_loss': self.metrics_history['loss'][-1] if self.metrics_history['loss'] else None,\n",
    "            'best_combined_score': max(filter(None, self.metrics_history['combined_score'])) if self.metrics_history['combined_score'] else None,\n",
    "            'total_steps': len(self.metrics_history['step']),\n",
    "            'final_epoch': self.metrics_history['epoch'][-1] if self.metrics_history['epoch'] else None,\n",
    "            'learning_rate_summary': {\n",
    "                'initial': self.metrics_history['learning_rate'][0] if self.metrics_history['learning_rate'] else None,\n",
    "                'final': self.metrics_history['learning_rate'][-1] if self.metrics_history['learning_rate'] else None,\n",
    "                'schedule_type': args.lr_scheduler_type\n",
    "            },\n",
    "            'loss_summary': {\n",
    "                'final_moving_avg': self.metrics_history['moving_avg_loss'][-1] if self.metrics_history['moving_avg_loss'] else None,\n",
    "                'best_loss': min(self.metrics_history['loss']) if self.metrics_history['loss'] else None\n",
    "            },\n",
    "            'resource_usage': {\n",
    "                'peak_cpu_ram_gb': self.peak_metrics['cpu_ram'],\n",
    "                'peak_gpu_vram_gb': self.peak_metrics['gpu_vram'],\n",
    "                'peak_gpu_utilization': self.peak_metrics['gpu_util']\n",
    "            },\n",
    "            'hardware_info': {\n",
    "                'cpu_info': self._get_cpu_info(),\n",
    "                'gpu_info': self._get_gpu_info(),\n",
    "                'total_ram': self._get_total_ram()\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # サマリーをJSONファイルとして保存\n",
    "        with open(self.output_dir / 'training_summary.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(summary, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "        logging.info(\"Training Complete!\")\n",
    "        logging.info(f\"Training duration: {summary['training_duration']}\")\n",
    "        \n",
    "        # Noneチェックを追加\n",
    "        if summary['loss_summary']['final_moving_avg'] is not None:\n",
    "            logging.info(f\"Final moving average loss: {summary['loss_summary']['final_moving_avg']:.4f}\")\n",
    "        if summary['loss_summary']['best_loss'] is not None:\n",
    "            logging.info(f\"Best loss achieved: {summary['loss_summary']['best_loss']:.4f}\")\n",
    "        \n",
    "        logging.info(f\"Peak CPU RAM usage: {summary['resource_usage']['peak_cpu_ram_gb']:.2f} GB\")\n",
    "        logging.info(f\"Peak GPU VRAM usage: {summary['resource_usage']['peak_gpu_vram_gb']:.2f} GB\")\n",
    "        logging.info(f\"Peak GPU utilization: {summary['resource_usage']['peak_gpu_utilization']:.1f}%\")\n",
    "\n",
    "    def _get_cpu_info(self):\n",
    "        import cpuinfo\n",
    "        try:\n",
    "            info = cpuinfo.get_cpu_info()\n",
    "            return {\n",
    "                'model': info.get('brand_raw', 'Unknown'),\n",
    "                'cores': psutil.cpu_count(logical=False),\n",
    "                'threads': psutil.cpu_count(logical=True)\n",
    "            }\n",
    "        except:\n",
    "            return \"Failed to get CPU info\"\n",
    "            \n",
    "    def _get_gpu_info(self):\n",
    "        if not torch.cuda.is_available():\n",
    "            return \"No GPU available\"\n",
    "        try:\n",
    "            import subprocess\n",
    "            result = subprocess.check_output(['nvidia-smi', '--query-gpu=gpu_name,memory.total', '--format=csv,noheader,nounits'])\n",
    "            gpus = result.decode('utf-8').strip().split('\\n')\n",
    "            return [{'model': g.split(',')[0], 'memory': float(g.split(',')[1])/1024} for g in gpus]\n",
    "        except:\n",
    "            return \"Failed to get GPU info\"\n",
    "            \n",
    "    def _get_total_ram(self):\n",
    "        return psutil.virtual_memory().total / (1024 * 1024 * 1024)  # GB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "pre {\n",
    "    border: 1px solid #333;\n",
    "    padding: 20px;\n",
    "    margin: 20px 0;\n",
    "    background-color: #000000;\n",
    "    color: #d4d4d4;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "pre code {\n",
    "    color: #d4d4d4;\n",
    "    display: block;\n",
    "    padding-bottom: 8px;\n",
    "    background-color: #000000; \n",
    "}\n",
    "\n",
    ".hljs, .language-python {\n",
    "    background-color: #000000 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"background-color: #F9F4F0; padding: 10px; border-left: 5px solid #4CAF50; margin: 10px; width: 95%;\">\n",
    "    <details>\n",
    "        <summary style=\"color: #8A6F5C; font-size: 1.17em; font-weight: bold;\">claude解説</summary>\n",
    "        <div style=\"color: #8A6F5C;\">\n",
    "\n",
    "\n",
    "\n",
    "# TrainingMonitorCallbackの詳細解説\n",
    "\n",
    "このクラスは、学習プロセスの詳細な監視と記録を行う高度なコールバックシステムです。\n",
    "\n",
    "## 1. 初期化と構造\n",
    "\n",
    "```python\n",
    "def __init__(self):\n",
    "    self.metrics_history = {\n",
    "        'step': [],\n",
    "        'style_consistency': [],\n",
    "        'dialogue_flow': [],\n",
    "        # ... その他のメトリクス\n",
    "    }\n",
    "```\n",
    "\n",
    "### 記録される主要メトリクス：\n",
    "- 学習の進捗（ステップ、エポック）\n",
    "- モデルの性能（スタイル一貫性、対話の流れ）\n",
    "- リソース使用状況（CPU/GPU使用率）\n",
    "- 学習パラメータ（学習率、バッチサイズ）\n",
    "\n",
    "## 2. リソース使用状況の記録\n",
    "\n",
    "```python\n",
    "def _record_resource_usage(self):\n",
    "    # CPU RAM監視\n",
    "    cpu_ram = psutil.Process().memory_info().rss / (1024 * 1024 * 1024)\n",
    "    \n",
    "    # GPU監視\n",
    "    if torch.cuda.is_available():\n",
    "        vram_used = torch.cuda.memory_allocated(i) / (1024 * 1024 * 1024)\n",
    "```\n",
    "\n",
    "### 出力例：\n",
    "```\n",
    "Resource Usage at 2024-03-15 14:30:25:\n",
    "CPU RAM: 12.45 GB\n",
    "GPU 0: \n",
    "  - VRAM Used: 8.32 GB\n",
    "  - Utilization: 85%\n",
    "```\n",
    "\n",
    "## 3. 学習プロセスの監視\n",
    "\n",
    "### 学習開始時\n",
    "```python\n",
    "def on_train_begin(self, args, state, control, **kwargs):\n",
    "    self.train_start_time = datetime.now()\n",
    "```\n",
    "\n",
    "### ログ記録時\n",
    "```python\n",
    "def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "    # 学習率の記録\n",
    "    if 'learning_rate' in logs:\n",
    "        self.metrics_history['lr_schedule'].append({\n",
    "            'timestamp': current_time,\n",
    "            'step': state.global_step,\n",
    "            'learning_rate': logs['learning_rate']\n",
    "        })\n",
    "```\n",
    "\n",
    "### 出力例：\n",
    "```\n",
    "Step 100:\n",
    "Learning Rate: 7.5e-5\n",
    "Loss: 2.345\n",
    "Moving Average Loss (last 10 steps): 2.412\n",
    "Gradient Norm: 0.876\n",
    "```\n",
    "\n",
    "## 4. 学習終了時のサマリー生成\n",
    "\n",
    "```python\n",
    "def on_train_end(self, args, state, control, **kwargs):\n",
    "    summary = {\n",
    "        'training_duration': str(training_duration),\n",
    "        'final_loss': self.metrics_history['loss'][-1],\n",
    "        'best_combined_score': max(self.metrics_history['combined_score']),\n",
    "        # ... その他の統計\n",
    "    }\n",
    "```\n",
    "\n",
    "### サマリー出力例：\n",
    "```json\n",
    "{\n",
    "    \"training_duration\": \"5:23:45\",\n",
    "    \"final_loss\": 1.234,\n",
    "    \"best_combined_score\": 0.856,\n",
    "    \"resource_usage\": {\n",
    "        \"peak_cpu_ram_gb\": 14.5,\n",
    "        \"peak_gpu_vram_gb\": 10.2,\n",
    "        \"peak_gpu_utilization\": 92.5\n",
    "    },\n",
    "    \"learning_rate_summary\": {\n",
    "        \"initial\": 8e-5,\n",
    "        \"final\": 1.2e-5\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "## 5. 実際の使用例\n",
    "\n",
    "```python\n",
    "# トレーナーの設定\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    callbacks=[\n",
    "        TrainingMonitorCallback(),\n",
    "        StyleCallback()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 学習実行\n",
    "trainer.train()\n",
    "\n",
    "# 出力ディレクトリの構造\n",
    "training_progress/\n",
    "├── training_history.json  # 詳細な学習履歴\n",
    "└── training_summary.json  # 最終サマリー\n",
    "```\n",
    "\n",
    "## 6. モニタリングの活用方法\n",
    "\n",
    "### 1. リソース使用の最適化\n",
    "```python\n",
    "# メモリ使用量が閾値を超えた場合の警告\n",
    "if self.peak_metrics['gpu_vram'] > 10:  # 10GB\n",
    "    logging.warning(\"High GPU memory usage detected!\")\n",
    "```\n",
    "\n",
    "### 2. 学習の進捗モニタリング\n",
    "```python\n",
    "# 損失値の急激な変化の検出\n",
    "if len(self.metrics_history['loss']) > 1:\n",
    "    current_loss = self.metrics_history['loss'][-1]\n",
    "    prev_loss = self.metrics_history['loss'][-2]\n",
    "    if current_loss > prev_loss * 1.5:  # 50%以上の増加\n",
    "        logging.warning(\"Significant loss increase detected!\")\n",
    "```\n",
    "\n",
    "### 3. ハードウェアの状態監視\n",
    "```python\n",
    "# GPU使用率の監視\n",
    "if gpu_util > 95:  # 95%以上の使用率\n",
    "    logging.warning(\"GPU utilization is very high!\")\n",
    "```\n",
    "\n",
    "このコールバックは、ソクラテス式対話モデルの学習において：\n",
    "- メモリ使用の最適化\n",
    "- 学習の安定性確保\n",
    "- パフォーマンスのボトルネック検出\n",
    "- 問題の早期発見と対処\n",
    "を可能にし、効率的な学習プロセスの実現を支援します。\n",
    "\n",
    "        \n",
    "</div>\n",
    "    </details>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
