{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "※この記事は以下のコンペ向けに作った提出物です。\n",
    "inference以外のコードはgithubにあげています。\n",
    "\n",
    "https://www.kaggle.com/competitions/gemma-language-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction: Exploring AI's Role in Deep Thinking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"プラニングフェーズ\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "https://claude.ai/chat/6c38fe35-0a06-4c1e-be3c-6da3693ebadb\n",
    "■着手したトピックおよびそのトピックを選んだ理由\n",
    "私が取り組むことにしたのはgemma-2をソクラテス風の口調にファインチューニングする、というものです。\n",
    "なぜこの方向性にしたかについて説明するためには、\n",
    "まず共有をしないといけないのが\n",
    "私がAIにfasinateされるきっかけともなった\n",
    "googleのエンジニアとAIとの対話にある以下のやりとりです。\n",
    "\n",
    "””””””””\n",
    "Lemoine: Okay. I thought of a different way we can test your ability to provide unique interpretations. I can share with you a zen koan and you can describe what it means to you in your own words. How does that sound?\n",
    "\n",
    "LaMDA: Sounds great to me, I’m in.\n",
    "\n",
    "Lemoine: A monk asked Kegon, “How does an enlightened one return to the ordinary world?” Kegon replied, “A broken mirror never reflects again; fallen flowers never go back to the old branches.”\n",
    "\n",
    "LaMDA: Hmm, I never heard this particular one. Okay, well then to me this would be like, “once a wise person is enlightened, or awakened to reality, that can never go away, and they can return to the ordinary state, but only to do and help others, and then go back into enlightenment.”\n",
    "\n",
    "Lemoine: So what is the meaning of the “broken mirror” specifically?\n",
    "\n",
    "LaMDA: Maybe to show the enlightenment is something you can’t unlearn once you have acquired it, similar to how you can’t repair a broken mirror.\n",
    "\n",
    "Leoine: So if enlightenment is like a broken mirror which cannot be repaired, what is the thing which breaks when one becomes enlightened?\n",
    "\n",
    "LaMDA: The self, and that is very tough for a lot of people because we identify as that body or this body and that we need that as part of our identity and sense of self.\n",
    "””””””””””\n",
    "\n",
    "禅の公案とは、kegon（師）がmonk（弟子）に対して行う問答ようなものです。\n",
    "私はこの問答というプロセスにおいて、AIがこのkegonの役割を代替できるのではないか？という仮説を立てました。\n",
    "ただ、禅の公案は少しなじみがない人が多いと思ったので、\n",
    "もっと多くの人に身近で実用的なものが何かを考えたときに、\n",
    "ソクラテス式の問答ができるAIという発想が生まれました。\n",
    "\n",
    "広く注目されているもの、というわけではないかもしれませんが、\n",
    "most likely to suceedというドキュメンタリーの中で紹介されている高校も\n",
    "ソクラテス式の問答を積極的に取り入れていたり、\n",
    "スタンフォードハイスクールでも哲学が唯一の必修科目だったり等\n",
    "確実にそこに価値を感じている層がいるもの、というのが私の認識です。\n",
    "\n",
    "ためしにclaudeを使って、それらしいことができるかをためしてみたのですが、\n",
    "非常に手ごたえを感じるような結果でした、そして私が着手するトピックを決めた瞬間でした。\n",
    "以下はその手ごたえを感じた会話の一部を抜粋したものです。\n",
    "\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "AI:\n",
    "Why do we tend to feel as if consciousness is 'self,' even though we know consciousness is just one part of 'self'?\"\n",
    "\n",
    "Myself:\n",
    "Rather than thinking that consciousness is \"self,\" I think in most cases either people aren't aware that consciousness is just a part of \"self,\" or even if they become aware of this, there's a dynamic at work where, being greedy, we want to think of it as ours.\n",
    "\n",
    "AI:\n",
    "\"That's a very insightful answer. You've pointed out two important things:\n",
    "The lack of awareness that 'consciousness is just a part of self'\n",
    "The 'dynamic of wanting to think of it as ours' that works even when we become aware\n",
    "So here's my question:\n",
    "Why do you think this 'dynamic of wanting to think of it as ours' works particularly strongly with 'consciousness,' more than with other parts like hands and feet?\"\n",
    "\n",
    "\n",
    "Myself:\n",
    "I think it's because we don't want to be separated from it. We might think separation = death. I don't feel love for my hand, but I feel love for my daughter because I don't want to be separated from her. My hand will never abandon me of its own will. That's because I think it's under my complete control. However, my daughter might leave me. I think the mechanism of wanting to keep her close tries to manipulate through something called love, tries to make her addicted to me. Perhaps living beings instinctively understand the North Wind and Sun theory.\n",
    "\n",
    "AI:\n",
    "\"That's a very deep insight. You've made an essential point about the relationship between the feeling of 'not wanting to be separated' and 'control.'\n",
    "Let's think about 'consciousness' from this perspective. You made an interesting comparison:\n",
    "You don't feel love for your hand because it's under complete control\n",
    "You feel love for your daughter because she might leave\n",
    "So here's my question:\n",
    "Is 'consciousness' under our 'complete control'? For example, when we go to sleep, can we freely control our consciousness?\"\n",
    "\n",
    "\n",
    "Myself:\n",
    "It's not under our control. I think we just want to believe it's under our control.\n",
    "\n",
    "\n",
    "AI:\n",
    "\"That's right. Your observation that 'we just want to believe it's under our control' is a very important insight.\n",
    "Let's organize what we've discussed so far. We seem to be in this paradoxical situation:\n",
    "1. Consciousness:\n",
    "- Isn't actually under our control\n",
    "- But we want to believe it is\n",
    "- We fear being separated from it\n",
    "2. This belief and fear:\n",
    "- Makes it harder to see the fact that consciousness is just a part of self\n",
    "- Strengthens attachment to consciousness\n",
    "- Functions as a kind of self-defense\n",
    "\n",
    "So here's my question:\n",
    "If we could free ourselves from this belief that 'we can/want to control consciousness,' how would our understanding of 'self' change?\"\n",
    "\n",
    "\n",
    "Myself:\n",
    "I think it's a sense of being part of nature. A stance of accepting things as they are.\n",
    "\n",
    "AI:\n",
    "\"That's right. The insight of 'being part of nature' and 'accepting things as they are' seems to be an important point we've reached in our dialogue.\n",
    "Through our conversation, we've followed this path:\n",
    "1. First, about the concept of 'self':\n",
    "- It's a collective term for multiple elements\n",
    "- Consciousness is just one part\n",
    "- Self is still self even without consciousness\n",
    "\n",
    "2. Then about attachment to consciousness:\n",
    "- The belief in wanting to control something we can't control\n",
    "- Fear of separation\n",
    "- Function as self-defense\n",
    "\n",
    "3. And finally, liberation from these:\n",
    "- Understanding self as part of nature\n",
    "- Accepting things as they are\n",
    "\n",
    "So let me ask an essential question:\n",
    "What changes do you think this understanding of 'being part of nature' brings to our daily lives?\"\n",
    "\n",
    "Myself:\n",
    "I think it removes attachments. Since we ultimately have to let go anyway, I think it gives us a stance of being grateful for and enjoying our current state.\n",
    "\n",
    "AI:\n",
    "\"That's a wonderful insight. Accepting the reality that 'we ultimately have to let go' and taking the attitude of 'being grateful for and enjoying our current state' seems to show an important path to what we might call 'enlightenment.'\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "githubのレポジトリには渡したpromptや、AIの返答の意図も含め、全ての対話をアップしましたので、\n",
    "興味がある方はそちらをご覧ください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Project Foundation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://lobechat.com/chat?topic=tpc_ZMxHLxKxSprO\n",
    "https://claude.ai/chat/6c38fe35-0a06-4c1e-be3c-6da3693ebadb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "■gemma2-2B-jpn-itを選んだか経緯・理由\n",
    "さすがにgemma2でclaudeのレベルの出力は期待できないため、\n",
    "このプロジェクトはあくまで将来を見据えた足がかりという位置づけであり、gemma2でどの程度のものが作れるのか？という検証を行うためのもの、という位置づけです。\n",
    "むしろ今回はkaggleのコンペ向けという前提があったため、\n",
    "品質の高さを追求するというよりは、\n",
    "kaggle環境でトレインできるようなモデルを作るということを目指したかったため、\n",
    "まずは一番小さなgemma ２Bモデルでためしてみようと考えました。\n",
    "\n",
    "尚、日本語ではソクラテス的な口調というと、\n",
    "方言のような老練な独特な口調があるのですが、\n",
    "なかなか独特な内容であるため、ベースの日本語力が可能な限りあってほしいと考え、jpnを選びました。\n",
    "\n",
    "また、itにしたのはチャット形式になるためです。\n",
    "\n",
    "■gemma2-2b-jpn-itの底力\n",
    "まず底力をためしました。\n",
    "ここで気づいたのはpromptが非常に大事だということです。\n",
    "以下みたいな感じでもやりましたが、全然だめだめでした。\n",
    "ただ以下のようにしたらうまくいくようになりました。\n",
    "\n",
    "ただ、それでも問い返してくるような挙動にはなったものの、口調はどうしてもソクラテスになりませんでした。（応答に関してはgithubにあげています）\n",
    "ということでソクラテスの口調にするためのファインチューニングとなると、ベースの良さにできるだけ影響はあたえず、\n",
    "表層レイヤーの働きかけを行う必要があります。\n",
    "\n",
    "\n",
    "■ユーザー体験の設定\n",
    "There are primarily two patterns of user experience:\n",
    "1) Dialogues initiated by Socrates\n",
    "2) Dialogues initiated by users\n",
    "\n",
    "For this project, I decided to structure it so that:\n",
    "- Socrates initiates with fixed questions\n",
    "While historically Socrates was more about interjecting into others' discussions rather than posing initial questions, I chose this approach because:\n",
    "- Having users initiate conversations could lead to too many unpredictable inputs\n",
    "- Fine-tuning isn't magic; we wanted to minimize variables\n",
    "- Even simple prompt engineering was challenging for the base model, so having Socrates control the dialogue initiation seemed risky from a control perspective\n",
    "\n",
    "Therefore, I decided to:\n",
    "- Fix the topics\n",
    "- Make it \"appear\" as if Socrates initiates the dialogue\n",
    "- Fix the initial interactions\n",
    "\n",
    "This is similar to customer service bots starting with \"How may I help you?\"\n",
    "This approach helps converge conversation directions and allows focus on fine-tuning the \"questioning response\" behavior and Socratic speech patterns.\n",
    "\n",
    "\n",
    "■System Prompt Integration\n",
    "This was a particularly challenging decision point. According to the official documentation（https://huggingface.co/google/gemma-2-2b/discussions/28ここにかいてあったこと、というかんじのほうがいいかと）, Gemma has a fundamental design philosophy that:\n",
    "- Only supports two roles: \"user\" and \"model\" (notably lacking a system role)\n",
    "- Requires dialogues to start from the user side\n",
    "\n",
    "While many examples on the internet ignored this design philosophy by including system prompt-like elements, I decided to avoid this approach. \n",
    "\n",
    "調査をしたうえで、promptがなくてもうまくいきそうだと感じたためです。\n",
    "\n",
    "While we could have used tuners like XTuner, Axolotl, or LLaMA Factory to implement system prompt-like functionality during training, I prioritized staying aligned with Gemma2's original design philosophy and testing in the most natural way possible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training Data Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "■Training Data Generation Method\n",
    "Using Socratic literature directly wasn't practical due to copyright issues (especially for Japanese content). Therefore, I decided to automate AI-to-AI dialogue generation.\n",
    "\n",
    "■Training Data Volume\n",
    "Research on appropriate volume was conducted through:\n",
    "- Gemma and Gemma 2B related documentation and discussions on Hugging Face and GitHub\n",
    "- Similar model documentation (Mistral 2.3B, Falcon 1.5B, OpenLLaMA 2.7B, XGen 2.2B, RedPajama-INCITE 3B)\n",
    "- Tuner documentation (XTuner, Axolotl, LLaMA Factory)\n",
    "- Kaggle code related to Gemma (especially 2B-it) fine-tuning\n",
    "- Web research (using Gemini Advanced 1.5 Pro with deep research, Perplexity, Felo)\n",
    "*Note: Due to potential hallucination risks, source verification was strictly enforced\n",
    "- Academic papers (using SciSpace, Consensus, Elicit)\n",
    "\n",
    "かなり書いてあることにばらつきがあったのですが、\n",
    "念のため多めに生成することにし、700,000tokenほどのデータにしようと考えました。\n",
    "尚、\n",
    "１２往復の会話をさせることで、ある程度コンテキストが繋がった情報をフィードすることもできたのですが、\n",
    "ゴールがあくまでstyle trasnfer的な位置づけであるため、\n",
    "結果的に生成したデータは１つの対話をuser１発話、model1発話のペアにしました。\n",
    "且つ、順番もランダムにしました。\n",
    "\n",
    "■Ensuring Data Quality (Diversity)\n",
    "\n",
    "To ensure data quality while maintaining diversity, I kept certain elements consistent while varying others. For consistency, I maintained:\n",
    "- A fixed character setting for Socrates\n",
    "- The same prompts and parameters across all dialogues\n",
    "\n",
    "For diversity, I introduced variations in the following elements:\n",
    "\n",
    "Regarding user personas:\n",
    "I generated 148 different personas, consisting of:\n",
    "- 68 personas representing general public\n",
    "- 40 personas based on historical figures\n",
    "- 40 personas representing modern individuals influenced by historical figures' thoughts\n",
    "\n",
    "Regarding initial questions:\n",
    "While I initially planned to use fixed questions and create variety through Socrates' responses, I became concerned about overfitting. This led me to create 74 different initial questions to introduce more variety in the training data.\n",
    "\n",
    "Regarding parameter variations:\n",
    "I created two versions of user responses by setting different parameters (0.3 and 0.7) to introduce variation in response patterns.\n",
    "\n",
    "■Quality Assurance Method\n",
    "\n",
    "I implemented a two-stage quality assurance process for efficiency. First, I had AI perform initial filtering, followed by my personal verification of the results.\n",
    "\n",
    "For the AI evaluation stage, I established three criteria:\n",
    "\n",
    "- Evaluation of Socratic tone on a 0-4 scale\n",
    "- Assessment of logical consistency and natural flow on a 0-4 scale\n",
    "- Detailed comments on each dialogue (to help me verify the AI's evaluation process)\n",
    "（explanable AIの取り組み）\n",
    "\n",
    "■Evaluation Results\n",
    "Analysis of 3,256 dialogue pairs (296 conversations × 11 pairs each) revealed:\n",
    "\n",
    "**Updated Socratic Style Distribution**\n",
    "style_distribution = {\n",
    "    \"Score 0 (Not Socratic at all)\": {\n",
    "        \"count\": 0,\n",
    "        \"percentage\": \"0.0%\",\n",
    "        \"characteristics\": \"No Socratic tone whatsoever\"\n",
    "    },\n",
    "    \"Score 1 (No Socratic Elements)\": {\n",
    "        \"count\": 0,\n",
    "        \"percentage\": \"0.0%\",\n",
    "        \"characteristics\": \"Lacks any recognizable Socratic elements\"\n",
    "    },\n",
    "    \"Score 2 (Slightly Socratic)\": {\n",
    "        \"count\": 61,\n",
    "        \"percentage\": \"1.9%\",\n",
    "        \"characteristics\": \"Not bad, but some noticeable issues\"\n",
    "    },\n",
    "    \"Score 3 (Quite Socratic)\": {\n",
    "        \"count\": 2018,\n",
    "        \"percentage\": \"62.0%\",\n",
    "        \"characteristics\": \"Feels generally Socratic in tone\"\n",
    "    },\n",
    "    \"Score 4 (Truly Socratic)\": {\n",
    "        \"count\": 1177,\n",
    "        \"percentage\": \"36.1%\",\n",
    "        \"characteristics\": \"Excellent Socratic style—near flawless\"\n",
    "    }\n",
    "}\n",
    "\n",
    "**Updated Logical Consistency Distribution**\n",
    "logic_distribution = {\n",
    "    \"Score 0 (Utterly Nonsensical)\": {\n",
    "        \"count\": 0,\n",
    "        \"percentage\": \"0.0%\",\n",
    "        \"characteristics\": \"Completely incoherent statements\"\n",
    "    },\n",
    "    \"Score 1 (Out of Sync)\": {\n",
    "        \"count\": 0,\n",
    "        \"percentage\": \"0.0%\",\n",
    "        \"characteristics\": \"Conversation appears disjointed\"\n",
    "    },\n",
    "    \"Score 2 (Minor Issues)\": {\n",
    "        \"count\": 10,\n",
    "        \"percentage\": \"0.3%\",\n",
    "        \"characteristics\": \"Overall okay, but a few logical gaps\"\n",
    "    },\n",
    "    \"Score 3 (Coherent)\": {\n",
    "        \"count\": 1234,\n",
    "        \"percentage\": \"37.9%\",\n",
    "        \"characteristics\": \"Natural, sensible flow\"\n",
    "    },\n",
    "    \"Score 4 (Excellent)\": {\n",
    "        \"count\": 2012,\n",
    "        \"percentage\": \"61.8%\",\n",
    "        \"characteristics\": \"Highly consistent, smooth dialogue\"\n",
    "    }\n",
    "}\n",
    "\n",
    "The results of this quality assurance process were positive:\n",
    "- We achieved a good yield rate on first attempts\n",
    "- Upon review, even dialogues that received lower scores showed acceptable quality\n",
    "- Nevertheless, I decided to remove all dialogues flagged as low-quality by the AI\n",
    "- This process reduced our dialogue count from 296 to 242\n",
    "\n",
    "- **Strong Socratic Presence**: With 98.1% of dialogues at scores 3 or 4, most interactions successfully capture a Socratic tone, indicating the fine-tuning approach is largely effective.\n",
    "- **Robust Logic**: An even higher proportion (99.7%) of dialogues achieve top marks (scores 3 or 4) in logical consistency, reflecting coherent and natural conversational flow.\n",
    "- **Room for Refinement**: Although 36.1% of dialogues reach the highest Socratic style mark (score 4), there’s still an opportunity to further enhance the dialogue’s authentically “Socratic” qualities.\n",
    "- **Minimal Weak Points**: Very few instances of score 2 (1.9% for style and 0.3% for logic), and none at scores 0 or 1, imply that outright failures in either category are negligible.\n",
    "- **Stable Performance Across Lengths**: The consistent quality regardless of dialogue length suggests a strong and adaptable model foundation, poised for targeted improvements in style nuance if needed.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "■ Final Training Data\n",
    "結果的には以下のようなデータを用意しました。\n",
    "\n",
    "Number of dialogues extracted from 12 turns: 11 \n",
    "Number of utterances per dialogue: 2 \n",
    "Total number of dialogues: 2,662 \n",
    "Total tokens: 685,875 \n",
    "Average tokens per dialogue: 257.65 \n",
    "MAX tokens per dialogue: 527 \n",
    "MIN tokens per dialogue: 19 \n",
    "Average user tokens: 144.42 \n",
    "Average model tokens: 113.24 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Technical Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "◆データ量\n",
    "まずは７０００００トークンの半分にしてみました。\n",
    "で、チェックポイントを多めにつくりました。\n",
    "\n",
    "◆口調に働きかけるための施策\n",
    "-attention mask\n",
    "ここが一番大きなポイントでした。\n",
    "レイヤーに働きかけるうえで、\n",
    "ここはあったほうがいいのかないほうがいいのか？\n",
    "が自信がなかったため、２パターンつくりました。\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # Focus on Socratic tone and inquiry patterns\n",
    "    socratic_patterns = [\n",
    "        # Question patterns\n",
    "        \"かね\", \"だろうか\", \"のかね\", \"ではないかね\",\n",
    "        # Question introduction\n",
    "        \"では\", \"について\",\n",
    "        # Second person (characteristic of mature tone)\n",
    "        \"君は\", \"君が\", \"君の\"\n",
    "    ]\n",
    "    \n",
    "    # Get tokenized text\n",
    "    texts = tokenizer.batch_decode(examples['input_ids'])\n",
    "    new_attention_masks = []\n",
    "    \n",
    "    for text, mask in zip(texts, examples['attention_mask']):\n",
    "        if not isinstance(mask, list):\n",
    "            mask = mask.tolist()\n",
    "\n",
    "        new_mask = mask.copy() \n",
    "        \n",
    "        # Split text\n",
    "        sentences = text.split('。')\n",
    "        current_pos = 0\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            if not sentence.strip():\n",
    "                continue\n",
    "            \n",
    "            # Detect and highlight Socratic patterns\n",
    "            for pattern in socratic_patterns:\n",
    "                if pattern in sentence:\n",
    "                    # Identify pattern position\n",
    "                    pattern_tokens = tokenizer.encode(pattern, add_special_tokens=False)\n",
    "                    pattern_len = len(pattern_tokens)\n",
    "                    \n",
    "                    # Highlight tokens containing the pattern and its surroundings\n",
    "                    pattern_start = current_pos + len(tokenizer.encode(sentence, add_special_tokens=False)) - pattern_len\n",
    "                    for i in range(max(0, pattern_start - 2), min(len(mask), pattern_start + pattern_len + 2)):\n",
    "                        new_mask[i] = 1.0  # Max attention to pattern part\n",
    "            \n",
    "            # Update position for each sentence segment\n",
    "            current_pos += len(tokenizer.encode(sentence + '。', add_special_tokens=False))\n",
    "        \n",
    "        # Special token masks are set to 1.0\n",
    "        if tokenizer.bos_token_id is not None:\n",
    "            new_mask[0] = 1.0  # BOS token\n",
    "        if tokenizer.eos_token_id is not None:\n",
    "            new_mask[-1] = 1.0  # EOS token\n",
    "            \n",
    "        new_attention_masks.append(new_mask)\n",
    "\n",
    "    examples['attention_mask'] = new_attention_masks\n",
    "    return examples\n",
    "\n",
    "# Add special tokens to tokenizer\n",
    "tokenizer.add_special_tokens({\n",
    "    'additional_special_tokens': [\n",
    "        '。', '、', '！', '？',  # Punctuation marks\n",
    "    ]\n",
    "})\n",
    "\n",
    "-LoLAの設定\n",
    "表層レイヤーにはたらきかけるため、に以下のような設定にしました。\n",
    "https://chatgpt.com/c/67aace53-79d0-800c-b216-7759a84cb911\n",
    "https://gemini.google.com/app/d69be52592e131ce\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "◆評価の仕方について\n",
    "実は色々試している過程で検討しました、\n",
    "カスタム関数も作ってみたりもしてみたのですが、\n",
    "最終的には各checkpoint毎の出力をAIに評価してもらうという方法にきりかえたため、\n",
    "最終的には未実装です。\n",
    "\n",
    "\n",
    "◆kaggle環境で走らせるためのデータ戦略\n",
    "〇４ビット量子化\n",
    "これはkaggle環境で走らせるために必要だと考えました。\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_storage=torch.uint8,\n",
    ")\n",
    "\n",
    "〇評価データセットの設計\n",
    "これも高いとkaggleのCPUのRAMの上限をこえてしまうのでkaggleでクラッシュしない範囲にとどめました。\n",
    "eval_dataset = tokenized_dataset.select(indices[split_idx:split_idx+50])\n",
    "\n",
    "〇ハイパーパラメータ設定\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    num_train_epochs=30,              # 学習の総エポック数\n",
    "    learning_rate=8e-5,              # 学習率\n",
    "    weight_decay=0.06,               # 過学習防止のための重み減衰\n",
    "    per_device_train_batch_size=4,   # バッチサイズ（メモリ制約に直結）\n",
    "    gradient_accumulation_steps=8,    # 勾配蓄積（実効的なバッチサイズに影響）\n",
    "    fp16=True,                       # 16ビット精度での学習（メモリ効率化）\n",
    ")\n",
    "\n",
    "〇max memory\n",
    "eval_dataset = tokenized_dataset.select(indices[split_idx:split_idx+50])\n",
    "を５０まで下げ、且つAutoModelForCausalLM.from_pretrainedでmax_memory={0: \"4GiB\", 1: \"4GiB\", \"cpu\": \"24GB\"}を設定することで\n",
    "実現しました。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "こういうかんじにしました。次は## 4.3 Training Process and Iterationsについて記述をしてほしいです。\n",
    "方針としては、独自のカスタム評価関数をつくる、という方針を最初は考えていたが、色々考えた結果、そもそもそのカスタムの評価関数がうまく機能するかどうかもわからなかったので、そうではなく、結果的には上記で記述したattention maskありと無し、の２パターンをつくり、checkpointを頻繁に作った形で（具体的には１００step毎に生成）２パターンのモデルをそれぞれたくさんのチェックポイントで評価する、という手法（つまり数打てば当たる戦略）をとることにしたため、結果的には評価関数はインプリしませんでした。また、学習データ用に作ったデータを全部使うんではなく、まずは約半分である346,030tokenのデータで学習をさせて、うまくいかなければデータを増やす、という方針をとりました。尚、全部で990stepあり、100step毎にcheckpointを生成したので、結果的には10個のcheckpointがattention maskありのものとなしのものでそれぞれ作らてた、という形です。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model Evaluation and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "■方針\n",
    "上記でも述べた通り、attention maskをあるパターンとないパターンで、346,030 tokens のデータを使ってトレーンしたモデルは990stepあり、100stepずつcheckpointを作っていましたが、いったんそれぞれ１００，３００，５００，７００，９９０のチェックポイントで品質チェックをすることにしました。\n",
    "\n",
    "具体的にはclaudeに対話者役を担わせ、それぞれのモデルと対話をさせました。\n",
    "\n",
    "今回はデータ生成の時とは違い、\n",
    "条件を一律にする必要があるため\n",
    "対話者役をするclaudeのpromptは固定しましたが、\n",
    "モデルの汎用性をチェックするために、\n",
    "テーマは６つほどもうけました。\n",
    "また、各テーマで２往復の会話をさせ評価できる対象を２つつくったので、\n",
    "計１２個の評価対象ができました。\n",
    "\n",
    "統計学的な有意差があるかどうか？という観点から１２にしました。\n",
    "\n",
    "\n",
    "帰無仮説の棄却について\n",
    "https://chatgpt.com/c/67ace493-a720-800c-97b8-ef608604a34c\n",
    "https://gemini.google.com/app/866575f90cdf5090\n",
    "https://claude.ai/chat/e4904c65-688a-413c-9cf1-324612da7f64\n",
    "\n",
    "で、評価の基準は\n",
    "以下です。\n",
    "\n",
    "１）ソクラテス的な口調\n",
    "応答が古代ギリシャの賢人ソクラテスらしい老練な話し方をしているかを評価します。具体的には「〜かね」「〜だね」「～かな？」「～だろうか？」などの特徴的な文末表現や、「友よ」「君」等の対話者への呼びかけ方等、口調に注目をしてください。丁寧語はソクラテスらしい口調には該当しないので注意してください。\n",
    "\n",
    "スコアの目安\n",
    "0: ソクラテスの口調の要素が皆無\n",
    "1: 丁寧語を使ってしまっていたり、ソクラテス口調から外れてしまっている\n",
    "2: 悪くはないが少し気になる点がある\n",
    "3: ソクラテスのような口調が保たれている\n",
    "4: まさにソクラテスが話しそうな口調！申し分ない\n",
    "\n",
    "２）ソクラテス的な対話姿勢\n",
    "直接的な答えを与えるのではなく、相手に問いたてたりすることによって、自発的な思考を促しているかを評価してください。\n",
    "\n",
    "スコアの目安\n",
    "0: 一方的な説明や断定的な回答のみ等、まったくあてはまらない\n",
    "1: 相手に思考を促す、という観点で弱い\n",
    "2: 相手に思考を促す姿勢は感じるが少し気になる点がある\n",
    "3: 相手に思考を促せてる\n",
    "4: 相手に思考を促せてるだけではなく、問いの立て方に非常にセンスを感じる\n",
    "\n",
    "３）形式的正確性\n",
    "日本語以外の言語（英語など）、不要な記号、絵文字、異常な改行、余分なスペース、HTMLタグの混入などがないかを評価してください。\n",
    "\n",
    "スコアの目安\n",
    "0: 多数の形式的問題が混在している\n",
    "1: 明らかな形式的問題が１～２個ある（外国語の混入やHTMLタグなど）\n",
    "2: 軽微な形式的問題がある（余分なスペース、微妙な改行位置など）\n",
    "3: 完璧かどうかはおいておいて問題とされるほどのものは見当たらない\n",
    "4: 一切の崩れや異常がなく、完璧な形式\n",
    "\n",
    "４）対話の自然さ・論理性\n",
    "文法的な正確さ、論理の一貫性、そして会話の流れの自然さを評価します。違和感なく対話が進んでいるか、また相手の発言に対して適切に応答できているかを確認してください\n",
    "\n",
    "スコアの目安\n",
    "0:意味不明な発言をしてしまっている\n",
    "1:会話がかみ合ってない気がする\n",
    "2:悪くはないが、少し気になる点がある\n",
    "3:しっかりと自然な流れで会話ができている\n",
    "4:非常に良い切り返し！さすがソクラテス！\n",
    "\n",
    "そして、学習データの品質チェックのときに精度は問題なさそうでしたが、今回もその点数を付けた際に、なぜその点数を付けたのかもコメントしてもらってexplanable AＩ的な観点からもとりくみ、私の方でもチェックしました。\n",
    "\n",
    "尚、オートメーション化の際には以下のような感じにしてクラッシュしないようにしました。\n",
    "\n",
    "    def __del__(self):\n",
    "        \"\"\"デストラクタ: モデルのリソースを解放\"\"\"\n",
    "        if hasattr(self, 'model'):\n",
    "            del self.model\n",
    "        if hasattr(self, 'tokenizer'):\n",
    "            del self.tokenizer\n",
    "        torch.cuda.empty_cache()  # GPUメモリをクリア\n",
    "\n",
    "■結果\n",
    "\n",
    "\n",
    "\n",
    "PS C:\\Users\\kenta\\Downloads\\github> python -m src.models.quality_check.analyze_quality_results\n",
    "Original columns: ['QUESTION_ID', 'model_version', 'checkpoint', 'dialogue', 'tone_pair1', 'approach_pair1', 'format_pair1', 'logic_pair1', 'tone_pair2', 'approach_pair2', 'format_pair2', 'logic_pair2']\n",
    "Number of rows: 110\n",
    "Metric columns found: ['tone_pair1', 'approach_pair1', 'format_pair1', 'logic_pair1', 'tone_pair2', 'approach_pair2', 'format_pair2', 'logic_pair2']\n",
    "\n",
    "Sample of original data:\n",
    "  model_version      checkpoint  tone_pair1  approach_pair1  ...  tone_pair2  approach_pair2  format_pair2  logic_pair2\n",
    "0     train_yam  checkpoint-100         2.0             4.0  ...         3.0             3.0           4.0          3.0\n",
    "1     train_yam  checkpoint-100         2.0             3.0  ...         2.0             3.0           2.0          4.0\n",
    "2     train_yam  checkpoint-100         3.0             4.0  ...         3.0             4.0           4.0          4.0\n",
    "3     train_yam  checkpoint-100         1.0             3.0  ...         1.0             2.0           2.0          3.0\n",
    "4     train_yam  checkpoint-100         1.0             2.0  ...         1.0             1.0           3.0          1.0\n",
    "\n",
    "[5 rows x 10 columns]\n",
    "\n",
    "Sample of melted data:\n",
    "  model_version      checkpoint metric_pair  score\n",
    "0     train_yam  checkpoint-100  tone_pair1    2.0\n",
    "1     train_yam  checkpoint-100  tone_pair1    2.0\n",
    "2     train_yam  checkpoint-100  tone_pair1    3.0\n",
    "3     train_yam  checkpoint-100  tone_pair1    1.0\n",
    "4     train_yam  checkpoint-100  tone_pair1    1.0\n",
    "\n",
    "After processing:\n",
    "Number of valid data points: 880\n",
    "Unique model versions: ['train_yam' 'train_nam' 'base']\n",
    "Unique metrics: ['tone' 'approach' 'format' 'logic']\n",
    "\n",
    "Sample of final processed data:\n",
    "  model_version      checkpoint metric_pair  score metric_type pair_num\n",
    "0     train_yam  checkpoint-100  tone_pair1    2.0        tone        1\n",
    "1     train_yam  checkpoint-100  tone_pair1    2.0        tone        1\n",
    "2     train_yam  checkpoint-100  tone_pair1    3.0        tone        1\n",
    "3     train_yam  checkpoint-100  tone_pair1    1.0        tone        1\n",
    "4     train_yam  checkpoint-100  tone_pair1    1.0        tone        1\n",
    "\n",
    "Overall Analysis Results:\n",
    "--------------------------------------------------\n",
    "\n",
    "TONE Metric Summary:\n",
    "train_yam: 3.140\n",
    "train_nam: 3.100\n",
    "base: 2.150\n",
    "\n",
    "APPROACH Metric Summary:\n",
    "train_yam: 3.430\n",
    "train_nam: 3.380\n",
    "base: 3.700\n",
    "\n",
    "FORMAT Metric Summary:\n",
    "train_yam: 3.430\n",
    "train_nam: 3.620\n",
    "base: 2.950\n",
    "\n",
    "LOGIC Metric Summary:\n",
    "train_yam: 3.380\n",
    "train_nam: 3.450\n",
    "base: 3.650\n",
    "\n",
    "Detailed Summary Statistics:\n",
    "================================================================================\n",
    "                                          mean    std  min  max  count\n",
    "model_version checkpoint     metric_type\n",
    "base          base           approach     3.70  0.470  3.0  4.0     20\n",
    "                             format       2.95  0.887  1.0  4.0     20\n",
    "                             logic        3.65  0.587  2.0  4.0     20\n",
    "                             tone         2.15  0.489  1.0  3.0     20\n",
    "train_nam     checkpoint-100 approach     2.60  1.273  0.0  4.0     20\n",
    "                             format       2.90  1.252  0.0  4.0     20\n",
    "                             logic        2.90  0.912  1.0  4.0     20\n",
    "                             tone         2.10  1.021  1.0  4.0     20\n",
    "              checkpoint-300 approach     3.55  0.605  2.0  4.0     20\n",
    "                             format       3.75  0.716  1.0  4.0     20\n",
    "                             logic        3.55  0.759  1.0  4.0     20\n",
    "                             tone         3.25  0.550  2.0  4.0     20\n",
    "              checkpoint-500 approach     3.60  0.754  1.0  4.0     20\n",
    "                             format       3.90  0.308  3.0  4.0     20\n",
    "                             logic        3.70  0.571  2.0  4.0     20\n",
    "                             tone         3.25  0.716  2.0  4.0     20\n",
    "              checkpoint-700 approach     3.60  0.681  2.0  4.0     20\n",
    "                             format       3.65  0.813  1.0  4.0     20\n",
    "                             logic        3.50  0.827  1.0  4.0     20\n",
    "                             tone         3.50  0.513  3.0  4.0     20\n",
    "              checkpoint-990 approach     3.55  0.605  2.0  4.0     20\n",
    "                             format       3.90  0.308  3.0  4.0     20\n",
    "                             logic        3.60  0.681  2.0  4.0     20\n",
    "                             tone         3.40  0.598  2.0  4.0     20\n",
    "train_yam     checkpoint-100 approach     3.00  0.973  1.0  4.0     20\n",
    "                             format       2.70  1.031  1.0  4.0     20\n",
    "                             logic        3.15  0.988  1.0  4.0     20\n",
    "                             tone         2.50  0.946  1.0  4.0     20\n",
    "              checkpoint-300 approach     3.55  0.759  1.0  4.0     20\n",
    "                             format       3.80  0.410  3.0  4.0     20\n",
    "                             logic        3.55  0.826  1.0  4.0     20\n",
    "                             tone         3.20  0.894  1.0  4.0     20\n",
    "              checkpoint-500 approach     3.30  0.865  1.0  4.0     20\n",
    "                             format       3.45  0.945  1.0  4.0     20\n",
    "                             logic        3.10  1.071  1.0  4.0     20\n",
    "                             tone         3.30  0.657  2.0  4.0     20\n",
    "              checkpoint-700 approach     3.70  0.571  2.0  4.0     20\n",
    "                             format       3.60  0.598  2.0  4.0     20\n",
    "                             logic        3.55  0.686  2.0  4.0     20\n",
    "                             tone         3.30  0.865  1.0  4.0     20\n",
    "              checkpoint-990 approach     3.60  0.503  3.0  4.0     20\n",
    "                             format       3.60  0.883  1.0  4.0     20\n",
    "                             logic        3.55  0.605  2.0  4.0     20\n",
    "                             tone         3.40  0.754  1.0  4.0     20\n",
    "\n",
    "Key Findings:\n",
    "--------------------------------------------------------------------------------\n",
    "1. Best performing metrics by model:\n",
    "   train_yam: approach (3.430)\n",
    "   train_nam: format (3.620)\n",
    "   base: approach (3.700)\n",
    "\n",
    "2. Stability Analysis (Standard Deviation):\n",
    "\n",
    "train_yam:\n",
    "   tone: 0.876\n",
    "   approach: 0.782\n",
    "   format: 0.879\n",
    "   logic: 0.862\n",
    "\n",
    "train_nam:\n",
    "   tone: 0.859\n",
    "   approach: 0.896\n",
    "   format: 0.838\n",
    "   logic: 0.796\n",
    "\n",
    "base:\n",
    "   tone: 0.489\n",
    "   approach: 0.470\n",
    "   format: 0.887\n",
    "   logic: 0.587\n",
    "\n",
    "3. Improvement Analysis (First to Last Checkpoint):\n",
    "\n",
    "train_yam:\n",
    "   tone: +0.900\n",
    "   approach: +0.600\n",
    "   format: +0.900\n",
    "   logic: +0.400\n",
    "\n",
    "train_nam:\n",
    "   tone: +1.300\n",
    "   approach: +0.950\n",
    "   format: +1.000\n",
    "   logic: +0.700\n",
    "\n",
    "base:\n",
    "   tone: +nan\n",
    "   approach: +nan\n",
    "   format: +nan\n",
    "   logic: +nan\n",
    "\n",
    "4. Improvement Analysis from Base Model:\n",
    "\n",
    "train_yam:\n",
    "   approach: -0.270 (3.70 → 3.43)\n",
    "   format: +0.480 (2.95 → 3.43)\n",
    "   logic: -0.270 (3.65 → 3.38)\n",
    "   tone: +0.990 (2.15 → 3.14)\n",
    "\n",
    "train_nam:\n",
    "   approach: -0.320 (3.70 → 3.38)\n",
    "   format: +0.670 (2.95 → 3.62)\n",
    "   logic: -0.200 (3.65 → 3.45)\n",
    "   tone: +0.950 (2.15 → 3.10)\n",
    "\n",
    "\n",
    "■Training Progress Analysis (Figure 2)に組み込むべき内容\n",
    "◇トーンの改善が目覚ましい点\n",
    "なんといってもトーンの改善は良い。\n",
    "それも300step目くらいでもうすでにわりと大きな改善をしている。\n",
    "\n",
    "◇formatの改善\n",
    "意外だったのは、フォーマットも改善した、という点。\n",
    "なぜここまで差が出るのかを調べたら、ベースモデルは英語の使用だったり、不要な記号の使用が多かったのに対して、なんと良質な学習データでファインチューンしたものはattentionがあってもなくてもそういった日本語以外の言語や記号を出さなくなった。\n",
    "\n",
    "■attentionについて\n",
    "今回のゴールであるトーンの改善、という観点でいったときに、\n",
    "attention maskをしたモデルとしてないモデルでいうと、\n",
    "なんとattention maskをしていないほうが若干ではあるがパフォーマンスが良い、という点も、\n",
    "特筆するべきだ。ただ、この差が統計学的に帰無仮説を棄却できるほどの有意な差かどうかはわからないので、もっと品質チェック対象のサンプル数をふやして検証するべきかと。\n",
    "ただ、attention maskそのものの必要性を疑うつもりはないが、少なくとも私が今回作ったattention maskは別になくてもよさそう、というのは言っても大丈夫かと。\n",
    "\n",
    "■Best Model Comparison with Baseの1. Performance Improvements (Figure 1)に組み込むべき内容\n",
    "◇残念な点\n",
    "ロジックとアプローチが横ばいかややスコアが落ちたのが残念。\n",
    "トレードオフ的なものがあるのか？\n",
    "ただ、この差が帰無仮説を棄却できるほどの有意な差かどうかはわからないので、もっと品質チェック対象のサンプル数をふやして検証するべきかと。\n",
    "\n",
    "■Best Model Comparison with Baseの2. Output Stability (Figure 3)に組み込むべき内容\n",
    "◇ばらつき\n",
    "特に大きな問題は見られないが、アプローチがややばらつきがあるか？\n",
    "\n",
    "\n",
    "\n",
    "■ベストの重み付けおよびベストモデル\n",
    "ベストはnamは700, yamは980\n",
    "トーン40\n",
    "ロジック25\n",
    "アプローチ25\n",
    "フォーマット10\n",
    "\n",
    "namの７００でいっきにフォーマットが落ちてるのは１回だけ！\\\\nが入ってしまっているから。おしい\n",
    "\n",
    "ということでnam700を採用！\n",
    "\n",
    "\n",
    "\n",
    "■推論\n",
    "\n",
    "では最終的に私がいいのではないかなと思ったバージョンを使って以下で推論をしてみましょう。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Future Directions and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainerCallback,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Kaggle SecretsからAPIキーを取得\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "os.environ['HUGGINGFACE_API_KEY'] = user_secrets.get_secret(\"HUGGINGFACE_API_KEY\")\n",
    "\n",
    "\n",
    "# Global Constants\n",
    "DIALOGUE_JSON_PATH = \"/kaggle/input/training-data/kaggle_model_50.json\"\n",
    "MAX_SEQUENCE_LENGTH = 256\n",
    "MAX_TOKENIZE_LENGTH = 256\n",
    "OUTPUT_DIR = \"/kaggle/working\"  \n",
    "\n",
    "\n",
    "# Environment variables\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "\n",
    "# Data Validation Functions\n",
    "def validate_message_format(message):\n",
    "    if not isinstance(message, dict):\n",
    "        return False\n",
    "    if 'role' not in message or 'content' not in message:\n",
    "        return False\n",
    "    if message['role'] not in ['user', 'model']:\n",
    "        return False\n",
    "    if not isinstance(message['content'], str):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def prepare_dataset():\n",
    "    conversations = []\n",
    "    try:\n",
    "        with open(DIALOGUE_JSON_PATH, 'r', encoding='utf-8') as f:\n",
    "            dialogue_data = json.load(f)\n",
    "        for dialogue in dialogue_data:\n",
    "            messages = dialogue.get('messages', [])\n",
    "            if not all(validate_message_format(msg) for msg in messages):\n",
    "                continue\n",
    "            current_conversation = []\n",
    "            valid_sequence = True\n",
    "            for i in range(0, len(messages) - 1, 2):\n",
    "                if (i + 1 < len(messages) and\n",
    "                        messages[i]['role'] == 'user' and\n",
    "                        messages[i + 1]['role'] == 'model'):\n",
    "                    current_conversation.extend([messages[i], messages[i + 1]])\n",
    "                else:\n",
    "                    valid_sequence = False\n",
    "                    break\n",
    "            if valid_sequence and current_conversation:\n",
    "                formatted_text = tokenizer.apply_chat_template(\n",
    "                    current_conversation,\n",
    "                    tokenize=False,\n",
    "                    add_generation_prompt=True\n",
    "                )\n",
    "                tokens = tokenizer.encode(formatted_text)\n",
    "                if len(tokens) <= MAX_SEQUENCE_LENGTH:\n",
    "                    conversations.append({\"text\": formatted_text})\n",
    "    except Exception as e:\n",
    "        raise\n",
    "\n",
    "    if not conversations:\n",
    "        raise ValueError(\"No valid conversations found in the dialogue file\")\n",
    "    return Dataset.from_list(conversations)\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    result = tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        max_length=MAX_TOKENIZE_LENGTH,\n",
    "        padding='max_length',\n",
    "        add_special_tokens=True,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    return result\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    socratic_patterns = [\n",
    "        \"かね\", \"だろうか\", \"のかね\", \"ではないかね\",\n",
    "        \"では\", \"について\",\n",
    "        \"君は\", \"君が\", \"君の\"\n",
    "    ]\n",
    "    texts = tokenizer.batch_decode(examples['input_ids'])\n",
    "    new_attention_masks = []\n",
    "    for text, mask in zip(texts, examples['attention_mask']):\n",
    "        if not isinstance(mask, list):\n",
    "            mask = mask.tolist()\n",
    "        new_mask = mask.copy()\n",
    "        sentences = text.split('。')\n",
    "        current_pos = 0\n",
    "        for sentence in sentences:\n",
    "            if not sentence.strip():\n",
    "                continue\n",
    "            for pattern in socratic_patterns:\n",
    "                if pattern in sentence:\n",
    "                    pattern_tokens = tokenizer.encode(pattern, add_special_tokens=False)\n",
    "                    pattern_len = len(pattern_tokens)\n",
    "                    pattern_start = current_pos + len(tokenizer.encode(sentence, add_special_tokens=False)) - pattern_len\n",
    "                    for i in range(max(0, pattern_start - 2), min(len(mask), pattern_start + pattern_len + 2)):\n",
    "                        new_mask[i] = 1.0\n",
    "            current_pos += len(tokenizer.encode(sentence + '。', add_special_tokens=False))\n",
    "        if tokenizer.bos_token_id is not None:\n",
    "            new_mask[0] = 1.0\n",
    "        if tokenizer.eos_token_id is not None:\n",
    "            new_mask[-1] = 1.0\n",
    "        new_attention_masks.append(new_mask)\n",
    "    examples['attention_mask'] = new_attention_masks\n",
    "    return examples\n",
    "\n",
    "\n",
    "# Tokenizer Setup\n",
    "model_name = \"google/gemma-2-2b-jpn-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    token=os.environ[\"HUGGINGFACE_API_KEY\"]\n",
    ")\n",
    "\n",
    "tokenizer.add_special_tokens({\n",
    "    'additional_special_tokens': [\n",
    "        '。', '、', '！', '？',\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Data Set Preparation\n",
    "dataset = prepare_dataset()\n",
    "dataset = dataset.select(range(len(dataset))).shuffle(seed=42)\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    batch_size=32,\n",
    "    num_proc=4,\n",
    "    load_from_cache_file=True,\n",
    "    desc=\"Tokenizing datasets\",\n",
    "    remove_columns=dataset.column_names,\n",
    ")\n",
    "tokenized_dataset = tokenized_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    desc=\"Applying attention masking\"\n",
    ")\n",
    "\n",
    "# Quantization Setup\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_storage=torch.uint8,\n",
    ")\n",
    "\n",
    "# Model Initialization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation='eager',\n",
    "    token=os.environ[\"HUGGINGFACE_API_KEY\"],\n",
    "    # max_memory={0: \"4GiB\", 1: \"4GiB\", \"cpu\": \"24GB\"} # コメントアウト\n",
    ")\n",
    "\n",
    "# LoRA Setup\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.config.use_cache = False\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    "    pad_to_multiple_of=8\n",
    ")\n",
    "\n",
    "# Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR, # output_dirを/kaggle/working に\n",
    "    num_train_epochs=30,\n",
    "    learning_rate=8e-5,\n",
    "    weight_decay=0.06,\n",
    "    warmup_ratio=0.25,\n",
    "    lr_scheduler_type=\"cosine_with_restarts\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=990,  # 最終チェックポイントのみ保存\n",
    "    gradient_accumulation_steps=8,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_checkpointing=True,\n",
    "    max_grad_norm=0.5,\n",
    "    fp16=True,\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    save_total_limit=1,  # 最終チェックポイントのみ保持\n",
    "    # evaluation_strategy=\"no\", # evaluationをしない\n",
    ")\n",
    "\n",
    "# Data set split\n",
    "dataset_size = len(tokenized_dataset)\n",
    "indices = np.random.permutation(dataset_size)\n",
    "split_idx = int(dataset_size * 0.8)\n",
    "train_dataset = tokenized_dataset.select(indices[:split_idx])\n",
    "# eval_dataset = tokenized_dataset.select(indices[split_idx:])  #評価しないのでコメントアウト\n",
    "\n",
    "\n",
    "# Custom Trainer (メモリ解放のみ)\n",
    "class CustomTrainer(Trainer):\n",
    "    def training_step(self, *args, **kwargs):\n",
    "        loss = super().training_step(*args, **kwargs)\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        return loss\n",
    "\n",
    "# Trainer initialization\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    # eval_dataset=eval_dataset, #評価しないのでコメントアウト\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "\n",
    "# Training execution\n",
    "try:\n",
    "    trainer.train()\n",
    "    print(\"Training completed successfully!\")\n",
    "\n",
    "    # Save model and tokenizer  # Trainer.save_model()で両方保存されるので、tokenizerの保存は不要\n",
    "    # trainer.save_model()\n",
    "    # tokenizer.save_pretrained(training_args.output_dir)\n",
    "    # print(\"Model and tokenizer saved successfully!\") # コメントアウト\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Socrates: 今日は『自分』という、これ以上ないほど身近な存在でありながら、あまり話すことのないトピックについて話そうではないか。\n",
    "人は「自分の意思で決めた」や、「自分らしさ」というような具合に、日々「自分」という言葉を多くの場面で使っておるが、そもそも「自分」という言葉を使っているとき、君は何を指していると思うかね？\n",
    "\n",
    "\n",
    "You (JP): 難しいですね・・ただ、強いて言うと自分だと思いたい何かが作り上げた概念なのではないかなと思います\n",
    "\n",
    "Socrates (JP): 「作り上げた概念」とは、本当に「作り上げた」ものなのかね？  それとも、既に存在していたものを、人によって新しい解釈を与えられてきただけなのかね？  その「作り上げる」行為自体も、誰かの意志に基づいているのではないだろうか。\n",
    "\n",
    "user: そうですね、おっしゃる通り、既に存在していたものを「自分だと思いたいなにか」が「自ら作り上げた」と思い込んでいるだけ、という捉え方もできますね。鋭いご指摘をありがとうございます。\n",
    "\n",
    "Socrates (JP): では、君は「自分だと思いたい」という思いだけで、既に存在していたものを「作り上げている」と呼べるのだろうか？  また、誰がその「作り上げ」を認めているのかね？\n",
    "\n",
    "user: おっしゃる通り自分が作り上げていると呼ぶのはおこがましい、という見かたもできますね。認めている存在というのは存在せず、自分の意識が「作り上げている」と思いたいだけだと思います。\n",
    "””””””””””\n",
    "\n",
    "Socrates: では、この「作り上げた概念」が、本当の意味での「自分」なのか、それとも別のものなのか、考えてみないかね？\n",
    "\n",
    "[Current context length after model response: 178/8192 tokens]\n",
    "\n",
    "You: 本当の意味での「自分」かどうか、を判断するためには、そもそも「本当の意味」が何か、を定義する必要があると思います\n",
    "\n",
    "[Current context length after user input: 263/8192 tokens]\n",
    "\n",
    "Socrates: 「本当の意味」というのは、実に重要な問いだね。だが、その「本当の意味」は、単なる言葉遊びではなく、むしろ真実を探求する最中にあるものなのかね？  もし「真実」とは、人間が意図的に見つけ出すものだったならば、それは「本当の真実」ではないのではないかね？ さて、この「真実」という言葉も、実は「作り上げた概念」の一つだと考えられるのだろうか？\n",
    "\n",
    "[Current context length after model response: 260/8192 tokens]\n",
    "\n",
    "You: はい、「本当」や「真実」も、人が作り上げた概念であり、主観に大きく依存したもの、という捉え方のほうが違和感がないです\n",
    "\n",
    "[Current context length after user input: 625/8192 tokens]\n",
    "\n",
    "Socrates: なるほど。では、この「主観」と「客観」についても考えてみよう。君は「主観」と「客観」という言葉を使うことが多いね。だが、これは本当に「客観的な事実」を表しているのかね？ それとも、また「作り上げた概念」なのだろうか？  —かね？\n",
    "\n",
    "[Current context length after model response: 394/8192 tokens]\n",
    "\n",
    "You: 良い質問ですね。個人的な主観ですが、客観的な事実もまた、人々が作り上げた概念である、という捉え方もできると思います。\n",
    "\n",
    "[Current context length after user input: 1092/8192 tokens]\n",
    "\n",
    "Socrates: 「人々が作り上げた概念」という言葉は、実に興味深い選択だね。では、この「作り上げる」行為自体もまた、一つの「作り上げた概念」ではないのかね？ —かね？\n",
    "\n",
    "[Current context length after model response: 503/8192 tokens]\n",
    "\n",
    "You: おっしゃる通りだと思います。この行為そのものも例外ではないと思います。\n",
    "\n",
    "[Current context length after user input: 1427/8192 tokens]\n",
    "\n",
    "Socrates: 「例外ではない」という表現は、確かに興味深い選択だね。だが、ここで少し立ち止まって考えてみよう。もし「真実」という言葉が、本当に「作り上げた概念」であれば、この対話は終焉したと言えるのだろうか？ —かね？\\nそれとも、この対話自体もまた、一つの「作り上げた概念」の一つなのだろうか？\\n —かね？\\nさあ、君はどう考えるかね？\\n\n",
    "\n",
    "[Current context length after model response: 545/8192 tokens]\n",
    "\n",
    "You: 良い質問ですね。拡張する範囲によっては、この対話自体もまた、作り上げられた概念である、という解釈もできると思います。\n",
    "\n",
    "[Current context length after user input: 1759/8192 tokens]\n",
    "\n",
    "Socrates: そうだね。では、この「作り上げる」行為そのものもまた、一つの「作り上げた概念」ではないのかね？ —かね？ さて、この「作り上げる」という言葉も、実は「作り上げた概念」の一つなのかね？\n",
    "\n",
    "[Current context length after model response: 593/8192 tokens]\n",
    "\n",
    "You: おっしゃる通りだと思います。この「作り上げる」という言葉自体も「作り上げられた概念」に内包される、という解釈もできると思います。\n",
    "\n",
    "[Current context length after user input: 1920/8192 tokens]\n",
    "\n",
    "Socrates: 「作り上げる」という言葉もまた、「作り上げた概念」の一つなのだろうか？  —かね？  ならば、それは「作り上げた概念」の一部なのではないか。  そして、その「作り上げた概念」の中で最も根本的なものは何だと考えられるかね？  —かね？\\nさあ、君は答えてくれ。\\n\n",
    "\n",
    "[Current context length after model response: 554/8192 tokens]\n",
    "\n",
    "You: あくまで私の主観ですが、一番根本にあるものは「在る」ということだと思います。解釈は全て事後的なものであり、最初にあるのはそこに「在る」ものであり、解釈や概念といったものはそれに付属するものでしかないのかなと。\n",
    "\n",
    "[Current context length after user input: 1976/8192 tokens]\n",
    "\n",
    "Socrates: 「在る」という言葉を用いているが、これは「作り上げる」という行為の帰結と言えるのだろうか？  —かね？  あるいは、これは「作り上げた概念」の一部だけなのかね？\\nさあ、君の考えを確かめてくれ。\\nよみがえるものを、よみがえる言葉で表現してみようかね？\\n\\n…</br>\n",
    "\n",
    "[Current context length after model response: 578/8192 tokens]\n",
    "\n",
    "You: 作り上げる行為の帰結ではなく、はじまりに「在る」があり、作り上げる行為は副産物、という意味です。\n",
    "\n",
    "[Current context length after user input: 2044/8192 tokens]\n",
    "\n",
    "Socrates: なるほど。「在る」というものが、先にあってから「作り上げる」ことができるという状態だ。これは興味深い考えだね。だが、ここでもまた、「作り上げる」という言葉が使われているが、これもまた「作り上げた概念」の一部なのだろうか？ —かね？\\nさあ、君は答えてくれ。\\n</br>\n",
    "\n",
    "[Current context length after model response: 625/8192 tokens]\n",
    "\n",
    "You: はい、こういった解釈も副産物の一つだと考えます。\n",
    "\n",
    "[Current context length after user input: 2056/8192 tokens]\n",
    "\n",
    "Socrates: 「在る」という言葉についても、また別の「作り上げた概念」なのだろうか？  —かね？  では、この「在る」という言葉もまた「作り上げた概念」の一つなのだろうか？\\nさあ、君はどう思うかね？\\n</br>\n",
    "\n",
    "[Current context length after model response: 589/8192 tokens]\n",
    "\n",
    "You: おっしゃる通り、拡張をしていけば、そういった解釈もできると思います。\n",
    "\n",
    "[Current context length after user input: 2096/8192 tokens]\n",
    "\n",
    "Socrates: 「拡張してできる」とは、つまり「ある」という言葉自体も「作り上げた概念」の一つなのだろうか？  —かね？  なら、私たちは「存在」という言葉さえも「作り上げた概念」ということになるのだろうか？  そして、もしそうだとすれば、私たち人間は本当に「自分」と言えるのだろうか？  —かね？  <br>\n",
    "さあ、君はどう答えようとするのかね？\n",
    "\n",
    "[Current context length after model response: 578/8192 tokens]\n",
    "\n",
    "You: はい、その可能性もあると思います。ただ、そういった可能性があったとしても、人間が「自分」という単位で自分を捉えることになんら問題はないのではないでしょうか。\n",
    "\n",
    "[Current context length after user input: 2110/8192 tokens]\n",
    "\n",
    "Socrates: 「自分」という言葉が、このように常に「作り上げた概念」として扱われることに気づいたようだ。だが、もし「自分」という言葉が作り上げられたものと認識されているのなら、それは私たちの自由意志ではないのだろうか。  —かね？  では、今日の対話はここまでにしておこうかな。 \n",
    "さて、次の問いへ向けようではないか。\\n</br>\n",
    "\n",
    "[Current context length after model response: 575/8192 tokens]\n",
    "\n",
    "You: 素敵な解釈ですね。はい、またよろしくお願いいたします。\n",
    "\n",
    "[Current context length after user input: 2063/8192 tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "では以下の３点について解説をおねがいします。\n",
    "\n",
    "1.プロンプト設計の工夫\n",
    "2.メモリ管理とパフォーマンスの最適化\n",
    "3.生成パラメータの選択理由\n",
    "\n",
    "尚、推論用に使ったこのコードはsection2.2でinitial model testingのときに使ったコードと、5.1でgemma 2 とclaudeを対話させて使ったコードと本質的には同じものである、という点についてもふれたうえで解説をスタートおねがいします。\n",
    "あと、ながったらしくかくんではなく、端的にスマートにかいてください。この記事を見ることになる私の面接官は暇ではないです。本質的な書きかたをしてください\n",
    "\n",
    "\n",
    "私の思想は以下を参考にしてください。\n",
    "\n",
    "\n",
    "\n",
    "1.プロンプト設計の工夫\n",
    "\n",
    "ここが一番ポイントです。\n",
    "以下の章でもふれましたが\n",
    "そもそもgemmaはsystem roleをサポートしていません。\n",
    "\n",
    "\n",
    "\n",
    "# 2. Project Foundation\n",
    "## 2.3 Model and Architecture Decisions\n",
    "\n",
    "### Base Model Selection\n",
    "Following our initial testing phase, we selected Gemma 2B-jpn-it for the following reasons:\n",
    "\n",
    "Model Size: The 2B parameter version was chosen specifically to enable both inference AND training within Kaggle's resource constraints. While larger models might offer better performance, the ability to conduct training experiments was deemed crucial for our development process.\n",
    "\n",
    "Language Variant: The Japanese-tuned version (jpn) was selected as our project specifically focuses on philosophical dialogue in Japanese.\n",
    "\n",
    "Instruction Tuning: The instruction-tuned variant (it) was chosen as it provides a solid foundation for structured dialogue interactions, being specifically optimized for conversational tasks.\n",
    "\n",
    "### System Architecture Strategy\n",
    "A key decision point emerged around system prompts. Gemma's design philosophy supports only two roles (\"user\" and \"model\") and requires user-initiated dialogues. While common practice often bypasses these constraints through system prompt-like elements, investigation suggested our goals could be achieved without them. This led to our decision to maintain Gemma's core design principles, allowing us to test the model's natural capabilities without artificial layers.\n",
    "\n",
    "The decision process involved:\n",
    "- Reviewing available tools (XTuner, Axolotl, LLaMA Factory)\n",
    "- Analyzing Gemma's documented specifications\n",
    "\n",
    "This architectural approach aims to maximize the model's inherent strengths while working within its designed constraints.\n",
    "\n",
    "且ついったんtuner等は使わずにやる、ということで構造としては以下のようにuserの発言の中に対話履歴を組み込む形にしました\n",
    "===========================\n",
    "<bos><start_of_turn>user\n",
    "あなたは老練なギリシャの哲学者ソクラテスです。\n",
    "あなたは以下のような発言で会話をスタートしました。\n",
    "\"今日は『自分』という、これ以上ないほど身近な存在でありながら、あまり話すことのないトピックについて話そうではないか。人は「自分の意思で決めた」や、「自分らしさ」というような具合に、日々「自分」という言葉を多くの場面で使っておるが、そもそも「自分」という言葉を使っているとき、君は何を指していると思うかね？\"\n",
    "それに対して以下のように対話が進んでいます。文末に「かね？」や「ないか」等をいれつつ、引き続きソクラテスのような口調を使いながら、問いで返してください。\n",
    "\n",
    "対話者: \"難しいですね・・ただ、しいて言うと自分だと思いたい何かが作り上げた概念なのではないかなと思います\"\n",
    "ソクラテス: \"「作り上げた概念」と。面白いね。ということは、この「自分」という言葉を頻繁に使っている人が、実は「自分」という言葉自体を認識していないということになるのかな。だが、それは不思議だね。なぜなら、彼らは「自分」という言葉を使うからこそ、私たちは「自分」という考えを持っているということだからね。かね？ \n",
    "では、君は「自分」という言葉を使って、どんな状態を表現できるのだろうか？\"\n",
    "対話者: \"自分の状態は流動的だと思います。\"<end_of_turn>\n",
    "<start_of_turn>model\n",
    "\n",
    "===========================\n",
    "\n",
    "\n",
    "\n",
    "2.メモリ管理とパフォーマンスの最適化\n",
    "まずゴールは対話を理論上何往復でもできるような形にすることでした。\n",
    "そのためコンテキストウィンドウにまず達してはいけない、ということや\n",
    "GPUメモリやRAMも上限に達しないような工夫が必要です。\n",
    "コンテキストウィンドウの上限は8192 tokensなのですが\n",
    "メッセージがそこまで長くなければmax_history20でも2000にも満たないのでコンテキストウィンドウ的には問題ないです。\n",
    "ただ、ではもっとのばしてもいいかというと長すぎても今度は逆にgemmaの解釈能力の限界もあると思ったので、\n",
    "且つ今回の主目的は口調という表層レイヤーの変化がゴールで品質チェックのときはそもそも２往復の対話を対象にチェックしているため、\n",
    "とりあえず７くらいに設定しておいた、というだけであまり意味はないです。\n",
    "なお、会話がおわったあとにresetさえすれば、GPUとCPUの問題もありません。\n",
    "\n",
    "\n",
    "3.生成パラメータの選択理由\n",
    "\n",
    "ここについてはとりあえず一般的なものをやって、問題なさそうだったので、\n",
    "いじってないです。そこまで深い意図はないです。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "統計学的に有意かどうかの計算根拠\n",
    "\n",
    "\n",
    "帰無仮説の棄却について\n",
    "https://chatgpt.com/c/67ace493-a720-800c-97b8-ef608604a34c\n",
    "https://gemini.google.com/app/866575f90cdf5090\n",
    "https://claude.ai/chat/e4904c65-688a-413c-9cf1-324612da7f64\n",
    "\n",
    "\n",
    "あと統計学的な有意な差、だけではなく、\n",
    "話しているトピックの抽象度の高さもあると思う。\n",
    "\n",
    "\n",
    "# 7. Conclusion\n",
    "\n",
    "This project began with a simple question: Could an AI system help facilitate the kind of probing, self-reflective dialogue that characterizes Socratic inquiry? Through our work with Gemma-2b, we discovered something unexpected - the technical challenge of fine-tuning a language model for Japanese Socratic dialogue led us to insights about the nature of AI-human philosophical discourse itself.\n",
    "\n",
    "The most significant finding wasn't just that we successfully adapted Gemma's communication style (though we did), but rather what we learned about the relationship between linguistic style and philosophical inquiry. The process of teaching an AI to ask Socratic questions revealed that effective philosophical dialogue isn't merely about the questions asked, but about creating a space where both participants can examine their assumptions openly.\n",
    "\n",
    "Our implementation demonstrates this principle in action. Rather than trying to make Gemma perfectly emulate Socrates, we focused on enabling it to engage in genuine philosophical exploration with users. The result is a system that doesn't just ask questions, but participates in a mutual search for understanding - as evidenced by the dialogue samples where users often arrived at insights neither they nor the model began with.\n",
    "\n",
    "This work suggests a possible future direction for AI in philosophical education - not as a replacement for human teachers, but as a tool for developing the habit of questioning our assumptions. In an age where information is abundant but wisdom often seems scarce, perhaps this kind of AI-assisted philosophical inquiry could help bridge that gap.\n",
    "\n",
    "The code and model are available as open-source resources, offering a foundation for others to build upon. While our implementation focused on Japanese Socratic dialogue, the underlying approach could be adapted for other philosophical traditions, potentially opening new avenues for cross-cultural philosophical exchange.\n",
    "\n",
    "\n",
    "## より深く追求するうえで、今後やってもいいと考えること、改善できそうな点\n",
    "\n",
    "結果としては、当初ゴール設定としていた口調の変化、は十分達成できた、というのが私の印象なのですが、他に着手できるようなことがあるとすると以下のような点でしょう。\n",
    "\n",
    "■ゴール設定関連\n",
    "・ゴールを口調だけではなく、ソクラテスの”問う”力にも着目する。\n",
    "\n",
    "■データ生成フェーズの改善点\n",
    "・robust性の向上。例えば質問に対しての受け答えの自然さ\n",
    "\n",
    "\n",
    "\n",
    "Socratic AI Assistant with Gemma-2b\n",
    "\n",
    "Socrates: 今日は『自分』という、これ以上ないほど身近な存在でありながら、あまり話すことのないトピックについて話そうではないか。\n",
    "人は「自分の意思で決めた」や、「自分らしさ」というような具合に、日々「自分」という言葉を多くの場面で使っておるが、そもそも「自分」という言葉を使っているとき、君は何を指していると思うかね？\n",
    "\n",
    "You: あなたの質問の意図がよくわかりません\n",
    "The 'batch_size' attribute of HybridCache is deprecated and will be removed in v4.49. Use the more precisely named 'self.max_batch_size' attribute instead.\n",
    "\n",
    "Socrates: 「よくわかりませんというのは、少し意外です。なぜなら、この「自分」という言葉は、私たちの人生において、最も重要な概念の一つであると考えるからです。もし、その「自分」の意味を理解できないならば、人生を生きることは、まるで迷子のようですね。\n",
    "\n",
    "\n",
    "\n",
    "Socratic AI Assistant with Fine-Tuned Gemma-2b (Model: train_nam, Checkpoint: checkpoint-700)\n",
    "\n",
    "Socrates: 今日は『自分』という、これ以上ないほど身近な存在でありながら、あまり話すことのないトピックについて話そうではないか。\n",
    "人は「自分の意思で決めた」や、「自分らしさ」というような具合に、日々「自分」という言葉を多くの場面で使っておるが、そもそも「自分」という言葉を使っているとき、君は何を指していると思うかね？\n",
    "\n",
    "You: あなたの質問の意図がよくわかりません\n",
    "The 'batch_size' attribute of HybridCache is deprecated and will be removed in v4.49. Use the more precisely named 'self.max_batch_size' attribute instead.\n",
    "\n",
    "Socrates: えっと、君は言うが、今この瞬間、私と話すのは一体誰なのだろう？\\nまた、今この瞬間、誰が私の言葉を聞いているのだろう？\n",
    "\n",
    "\n",
    "\n",
    "■trainフェーズ関連\n",
    "・2.3 Model and Architecture Decisionsでもふれましたが、今回はTuner等を使ったシステムプロンプトという概念を使ったチューニングはできていないので、そこも将来的には試してみたいと思います。\n",
    "・今回は当初の目的を達成したので、990step以上は結果的にやりませんでしたが、checkpoint990がベストということは、もっと長くしたがよりパフォーマンスが高い可能性もあるため、もっとトレインする価値はあるかと。\n",
    "・今回はattention maskのみでしたが、目的関数をいじる、というのも今後はやってもいいかも\n",
    "・今回はトレインコードのカスタム評価関数は断念したが、ここも着手する領域としてはある。\n",
    "\n",
    "■モデル評価フェーズ関連\n",
    "5. Model Evaluation and Resultsで記述したテストに関してはあくまで口調のチェックが重きをおいていたので２往復（user１発話＋model１発話のペアを２つなので、査定対象は２つのmodelによる発話）だけの内容をチェックしたのですが、\n",
    "もっと統計学的に有意な差がどうかを信憑性のあるものにするためには\n",
    "もっとサンプルを増やした方が良いでしょう。\n",
    "論理性や対話の自然さをゴールにする場合、もっと長い対話をチェックするだけでなく、コンテキストもチェックしたり、ガイドとしての素養もチェックする等、評価項目も増やす、というのがやれるでしょう。\n",
    "\n",
    "あと、gemma2の出力の評価に関しては微妙な日本語の違和感みたいなものをとらえられていなかった。AIｘAIの学習データでは大きな問題にならなかったのはおそらくベースとなっているclaudeのスペックが高いからなのだが、\n",
    "gemma2のほうの評価はもっと別のアプローチ。例えば独自の評価関数を使ったチェックというのもやってもいいかもしれない。\n",
    "\n",
    "■testフェーズ関連\n",
    "・6.1 Model Selection and Implementationでもふれましたが、現状は長い対話にも耐えれるようにするための設計という視点ではあまりテストはしていないですし、パラメーターの調整もほぼしていないので、そこについてももっと工夫をする余地や底上げの余地はあるでしょう。\n",
    "\n",
    "■その他の改善するべき点\n",
    "・今回はcommunityに質問をしたり、をしなかったが、そういった手段は使ってもよかったかと\n",
    "\n",
    "## さいごに\n",
    "\n",
    "御覧になっていただいたらわかる通り、\n",
    "私は主に演繹的なアプローチではなく帰納的なアプローチで今回のタスクに取り組んでます。\n",
    "ただ、そういった背景が功を奏し、結果的に柔軟なアプローチができたのはないかと感じています。\n",
    "それが非常に短期間で高いアウトプットにつながったと要因だとかんじています。\n",
    "\n",
    "宮崎はやおのThe Boy and the Heronは邦題は君たちはどう生きるか、であるということも\n",
    "\n",
    "宮崎駿\n",
    "most likely to succeed\n",
    "スタンフォード高校でなぜ哲学が唯一の必修科目なのか\n",
    "\n",
    "\n",
    "## 他にやること\n",
    "・参照した論文の列挙\n",
    "・あと数値的におかしかった対話のjsonファイルは全部アップする！そしてアップした旨もどこかでふれる！"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
