
3. Scaling Parameter-Constrained Language Models with Quality Data


Introduction
Recent advancements in language model (LM) development have been shaped by scaling laws that relate training loss, dataset size, and model parameters. While these laws typically emphasize data quantity and model size, this work emphasizes the critical importance of data quality for model generalization, especially in scenarios limited by the number of model parameters. The focus shifts from just increasing data and model size to prioritizing data quality, specifically effective training tokens.

Key points included:

Challenge to the emphasis on data volume and model size in LM development.
Highlight on the decisive role of data quality (effective training tokens) in sub-billion parameter models.
Qualitative measures of data refinement techniques like data sampling and text synthesis on a pretraining corpus (RefinedWeb) to assess the impact on model performance.
Evaluation of data refinement techniques through experiments on eight benchmarks across various model sizes (25 million to 1.5 billion parameters).
Introduction of effective token size in scaling law formulation to better address data quality nuances.
Emphasis on high-quality data's pivotal role in training efficient and powerful language models.
Background
The Chinchilla scaling law offers a way to predict model training loss based on training tokens and model parameters, initially developed for optimizing compute settings during pretraining. Recent research underscores the critical role of data quality in model pretraining, prompting a reevaluation of scaling law formulations.

Non-Transformative Refinements:

Involves selective curation of data without altering core characteristics.
Data deduplication is key to prevent model generalization issues by removing duplicate documents, enhancing data quality, and improving model performance.
Data selection involves choosing an optimal subset from a larger corpus to boost model performance, reduce costs, and maintain evaluation integrity.
Transformative Refinements:

Includes generating synthetic data through instructional prompts to address data gaps and introduce new learning scenarios.
Integration of synthetic data in pretraining has notably enhanced model robustness and generalization.
Data refinements play a pivotal role in molding training landscapes for machine learning models, directly impacting training token distribution, quality, efficiency, and effectiveness in alignment with scaling laws.

Formulating Data Quality
The researchers use two metrics to evaluate data quality in pretraining sets, focusing on diversity and syntheticity. Here's a breakdown of how these metrics are formulated:

Diversity
Compression Ratio: Utilized to measure diversity by comparing the original file size to the compressed file size.
Higher compression ratios indicate more redundancy and lower diversity in the text data.
Diversity (Dr(D)) is defined based on this ratio, with higher values indicating more diverse text.
Token-Type Ratio: Calculated by dividing the count of unique tokens by the total number of tokens in a text.
MATTR (Moving Average Token Type Ratios): Introduced to capture lexical dynamics across varying text lengths.
N-gram Diversity: Expands the token-type ratio concept to consider longer sequences, addressing the limitation of individual word repetition within text segments.
Self-Repetition Metric: Assesses the tendency of language models to repeat long ngrams across different outputs, indicating redundancy in longer sequences.
Syntheticity
Perplexity Metric: Estimates the syntheticity of data points in the dataset by calculating how well a teacher model predicts a sequence of subword tokens.
Lower perplexity values suggest higher predictability and, by extension, higher syntheticity.
Teacher Model: Chosen for its robust performance across benchmarks and reliability for general evaluations.
Inverse Exponential Formula: Defines syntheticity as inversely proportional to perplexity, reflecting how the model's learned patterns represent the data point.
Token Prediction Likelihood: Quantifies the predictability of tokens given the model's knowledge state, indicating the typicality of the sequence within the teacher model's context.
Scaling Law with Data Quality
The researchers propose modifying the Chinchilla scaling law's third approach to model the losses when training large language models by adjusting the functional form. They introduce a formulation where (E) represents the baseline loss, similar to the entropy of natural text under an ideal generative process, establishing the theoretical minimum loss achievable with data (D) and model parameter (N).

In this study, the focus is on modeling zero-shot accuracy in common sense reasoning to evaluate how much reasoning ability a specific dataset (D) could potentially impart. To incorporate data quality into this framework, a quality term (Q) is introduced to adjust the number of training tokens (Dq), combining two equations.

The quality term (Q) is utilized to provide a quality-adjusted count of training tokens (Dq).
Scaling factors (c1) and (c2) are introduced to adjust (Dq) based on the syntheticity and diversity of the training tokens.
The revision of the scaling law shifts the prediction from average zero-shot accuracy across eight reasoning tasks, instead of focusing on loss. This revision integrates the quality-adjusted number of training tokens (Dq) into the accuracy function to offer a more nuanced understanding of how data quality influences model training and performance.

Data Refinement: A Case Study
In this section, the researchers delve into two key data refinement techniques, data selection, and data synthesis, crucial for improving data quality, particularly in pretraining datasets. These techniques play a vital role in enhancing text diversity, syntheticity, and downstream performance, a fact supported by various studies.

Key points included:

Two primary data refinement techniques are data selection and data synthesis, pivotal for increasing data quality.
Pretraining datasets benefit significantly from these techniques, impacting text diversity, and downstream model performance.
The authors present a comparative analysis in a figure to illustrate the relationship between effective token counts (Dq) and the total number of tokens (D). The analysis clearly indicates that data synthesis has a more profound effect on boosting the effective token count compared to data selection or using original datasets.

Key insights:

Data synthesis has a more substantial impact on enhancing effective token count compared to data selection.
The value of data synthesis in optimizing data quality for training models is highlighted, emphasizing its importance in the process.
Data Selection
The data selection process in the paper involves two main strategies: Coreset Selection and Text Deduplication, aimed at improving dataset quality and model training efficiency.

Coreset Selection:

Involves importance sampling to transform data into n-gram based feature vectors.
Compares feature distributions between raw and target datasets, assigning importance weights.
Enhances dataset syntheticity, influencing the syntheticity factor in the scaling law.
Allows easy exploration of the impact of in-domain data on data quality and losses.
Text Deduplication:

A method to remove redundant data for a balanced dataset.
Modulates dataset diversity and quality, crucial for robust model training.
Controls the dataset quality parameter by eliminating excessive redundancy to prevent overfitting.
Synthetic Data
The researchers employ a teacher model trained on a varied dataset to create synthetic data, enhancing the dataset's diversity. This approach introduces intricate token patterns, potentially boosting model performance by presenting complex scenarios missing in the initial dataset.

Utilization of a teacher model trained on a diverse dataset
Synthetic data generation aimed at broadening dataset diversity
Introduction of complex token patterns for improved model performance
Addressing scenarios not well-represented in the original dataset
Experimental Setup
The experimental configuration features:

Pretraining the decoder-only transformer using causal language modeling objectives on selected datasets with randomly initialized model weights.
Evaluating language models of various sizes, ranging from 25M to 1.5B parameters, to understand the impact of model capacity on results.
Conducting pretraining on a distributed computing setup with 32 GPUs across 4 nodes, each equipped with an H100 graphics card.
Benchmarking models across eight common sense reasoning tasks in a zero-shot setting, including ARC-easy, ARC-challenge, BoolQ, and PIQA.
Training models with token counts varying from 2 billion to 10 billion tokens using equivalent computational resources.
Running models for 100,000 steps with a context length of 2048 and a batch size of 16 to ensure thorough training.
Training models for different epochs based on dataset size, with the largest dataset undergoing about 9.5 epochs and the smallest about 48.1 epochs.
Constructing datasets from multiple sources: random data (8B tokens), selected data (7B tokens), and synthetic data (2B tokens) to ensure varied training data.
Curating selected data based on the evaluation set of the eight tasks using importance sampling, while generating synthetic data through instructional prompts for paraphrasing pretraining documents.
Noting that random data had high diversity but low syntheticity, while synthetic data had the lowest diversity but the highest syntheticity score.
Discussions
The researchers conducted over 200 training runs to re-estimate all the constants, as presented in a table. In this section, they delve into the estimation of constants that are connected to accuracy and other scaling parameters in Equation 4. Specifically, they focus on the scaling factor Q and its application in pretraining scenarios.

Over 200 training runs were performed to re-estimate constants
Constants related to accuracy and scaling parameters in Equation 4 were discussed
Emphasis on the scaling factor Q and its relevance in pretraining scenarios
Correlation Strength of Estimated Constants
The section discusses the correlation strength of estimated constants in a scaling law equation and a proposed scaling factor, emphasizing the robustness of the formulation and factors influencing model accuracy:

The estimated constants in the scaling law equation and the proposed scaling factor show a correlation strength of +0.83 across all model sizes and data samples.
The formulation's robustness is attributed to using a data-agnostic compression ratio and a capable language model as a teacher.
Effective tokens significantly impact model accuracy, with an increase in effective tokens correlating with higher accuracy.
The influence of the scaling factor Q varies across different models, with a more pronounced impact on data quality in smaller model sizes (25M to 500M).
As the value of the scaling factor Q increases, the impact of data quality levels off, leading to effective tokens being predominantly determined by the number of tokens.
In examining the interplay between the scaling factor Q, diversity, and syntheticity, it is observed that there is an inverse relationship between diversity and syntheticity, as synthetic data generated by language models tends to be less diverse.
Less Diverse Data and Scaling Factor Q
Less diverse data influences the scaling factor Q, with more synthetic data leading to an elevation in Q. However, a point of convergence exists where the impact of scaling factor Q on accuracy improvement becomes minimal when diversity and syntheticity curves align. This insight indicates the interplay between data diversity, syntheticity, and the scaling factor in enhancing model accuracy.

Data Quality and Token Quantity Bound: The relationship between data quality and token quantity is critical. For smaller models, introducing synthetic data can be advantageous as it results in less diverse but more synthetic data with a higher scaling factor Q.
Balancing Text Syntheticity and Token Count: While increasing text syntheticity can enhance data quality, the total token count plays a dominant role in improving model accuracy, especially for larger models that require substantial data volume (e.g., exceeding 1.5B in experiments).
Practical Implications for Training Practitioners: Training practitioners should be mindful of the balance between data diversity, syntheticity, and token quantity, recognizing the varying impacts on model accuracy based on model size and data requirements.
Conclusion and Future Works
The authors revisited traditional scaling laws in language modeling, emphasizing the crucial impact of data quality on model generalization. They introduced the concept of effective training tokens to enhance model performance, especially for models with limited parameters, to better understand the influence of data quality on model scaling.

Key points included:

Data quality significantly affects model generalization in language modeling.
Effective training tokens play a crucial role in improving model performance, especially for models with constrained parameters.
The research findings underscore the importance of data quality and open avenues for developing more efficient and compact language models suitable for on-device applications.
Limitations
The authors present a revised scaling law that incorporates effective training tokens to enhance data quality understanding. However, a key limitation is the need for a significant number of sample points to accurately estimate the constants within this law. The precision of these constants is critical as they directly impact the model's performance predictions and generalizations. Some key points regarding the limitations include:

Obtaining a sufficient number of diverse and representative sample points to robustly estimate these constants poses a challenge.
This challenge is especially prominent in cases involving rare or complex data characteristics, where access to an ample and varied set of training examples is restricted.
In scenarios where there is a scarcity of suitable training data, the reliability of the scaling law might be compromised.
Addressing this limitation may require further research efforts and possibly the exploration of more advanced sampling techniques to improve the reliability of the estimates.
Ethics Statement
The authors delve into the impact of data quality on language model performance by introducing the concept of effective training tokens. Their experiments, spanning diverse sampled and synthetic data, maintain rigorous standards for reproducibility and reliability:

Experiments analyze the influence of data quality on language model performance.
Introduce the concept of effective training tokens to enhance training.
The research employs established datasets but emphasizes the need for ethical caution when extending the findings to sensitive or private data:

Emphasize strict ethical considerations and robust privacy safeguards for sensitive data.
Proposed methodologies for improving data quality, such as text diversity and fidelity assessments, necessitate careful application to prevent unintended biases or ethical issues:

Advocates judicious use of data quality enhancement techniques to avoid biases.
Balancing advancements in model efficiency with ethical implications is crucial to account for increased computational demands and environmental impact:

Emphasize the importance of considering broader implications of model efficiency advancements.
Highlight the need to balance technological progress with environmental concerns.
A Details of Data Synthesis
The data synthesis process involves utilizing an instruction prompt with a Llama-3-70B model to rewrite documents sourced from pretraining data. Key points include:

Data synthesis involves rewriting provided documents using the Llama-3-70B model.
The data is sourced from JSONL files organized by group numbers and shards.
The model is configured to process sequences up to 8196 tokens.
Computational precision is optimized by enabling BF16 and disabling FP16 for specific hardware.
A batch size of 8 per device is used for efficient processing and resource utilization.
The instruction prompt guides the creation of common sense reasoning problem-answer pairs.
If creating a problem is impossible, the text is rewritten in a clear and concise textbook style.
Only the relevant response is provided without additional information.
Context for the reasoning problem is included to ensure understanding for all readers.
B Details of Data Selection
The researchers followed the data selection process outlined in Xie et al. and delved into the feature extraction from documents. To manage memory constraints, the RefinedWeb dataset was split into 16 distinct shards.

Each of the 16 data portions underwent selective sampling tailored to specific requirements.
The sampling procedure typically takes around 1.5 days to complete across various methodologies.
Notably, changes in the tokenizer's vocabulary minimally impact the sampling speed.
The vocabulary size seems to play a more substantial role in affecting the sentence compression ratio rather than the processing time.
Computing Text Syntheticity
Computing text syntheticity involves calculating the perplexity for each document using a language model with a context length of 1024 tokens. Here are the key points included:

The perplexity score is determined for each document to assess its syntheticity.
A language model is employed with a context length of 1024 tokens for processing all documents.
The average perplexity score obtained across the documents acts as the measure of syntheticity.
To handle the computational intensity of calculating perplexity using language models, the researchers adopted a strategic approach:

To manage computational resources, 25% of complete documents were sampled from each dataset.
This sampling strategy ensures a substantial amount of data, varying from around 100 million to several billion subword tokens.
The aim is to facilitate a robust and efficient analysis of syntheticity while dealing with large volumes of text data effectively.
D Scaling Law Constant Estimation
In this section, the authors introduce a scaling law for language modeling systems represented as Ĝ(N, D), where N is the model size and D is the dataset size. The goal is to estimate the accuracy of language models with the help of various constants like E, A, α, B, β, c1, and c2.

Key Points:

The estimation of scaling law constants involved analyzing a dataset comprising 210 data points, each representing different model and dataset sizes along with their training losses and accuracy scores.
This estimation process considered refining the training data by integrating factors like diversity and syntheticity into the dataset size, exploring different dataset size transformations for effective integration.
Model accuracy was determined for each refinement in the dataset, enabling robust parameter estimation through nonlinear curve fitting to align scaling law predictions with observed training losses.
Goodness of fit assessment during curve fitting utilized the R-squared value to measure predictability of model inputs in the observed data variance.
Iterative refinement and evaluation processes aimed at achieving the best fit between predicted and observed accuracies, stopping at 200 iterations.
Refinement of constants E, A, α, B, β, c1, and c2 through this process improved the scaling law's predictive ability for training losses in diverse settings, facilitating efficient resource allocation and model design in language modeling.
The refined constants offered a more accurate representation of how training loss scales concerning changes in model and dataset sizes, integrating factors like diversity and syntheticity through c1 and c2.
E Deriving Effective Token D q Equation
The researchers derived a formula to calculate the number of effective tokens based on the loss function:

The formula involved manipulating the loss function, denoted as L.
Steps included moving variables, isolating terms, and applying mathematical operations to determine D effective tokens.
Feature Extraction Process Details:

Due to memory limitations, the researchers divided the RefinedWeb data into 16 parts and extracted subsets from each based on target data.
This extraction process, across different approaches, takes approximately 1.5 days on average.
Despite changes in tokenizers' vocabulary, there were no significant differences in sampling speed, as the vocabulary also influences sentence compression ratio.