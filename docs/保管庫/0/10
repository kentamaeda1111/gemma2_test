10. DataComp-LM: In search of the next generation of training sets for language models
J. Li, Alex Chengyu Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Yitzhak Gadre, Hritik Bansal, Etash Guha, Sedrick Scott Keh, Kushal Arora, Saurabh Garg, Rui Xu, Niklas Muennighoff, Reinhard Heckel, Jean Mercat, Mayee Chen, Suchin Gururangan, Mitchell Wortsman, Alon Albalak, Yonatan Bitton, Marianna Nezhurina, Amro Abbas, Cheng-Yu Hsieh, Dhruba Ghosh, Josh Gardner, Maciej Kilian, Hanlin Zhang, Rulin Shao, Sarah I. Pratt, Subrata Sanyal, Gabriel Ilharco, Giannis Daras, Kalyani Marathe, Aaron Gokaslan, Jieyu Zhang, Khyathi Raghavi Chandu, Thao Nguyen, Igor Vasiljevic, Sham M. Kakade, Shuran Song, Sujay Sanghavi, Fartash Faghri, Sewoong Oh, Luke Zettlemoyer, Kyle Lo, Alaaeldin El-Nouby, Hadi Pouransari, Alexander Toshev, Stephanie Wang, Dirk Groeneveld, Luca Soldaini, Pang Wei Koh, Jenia Jitsev, Thomas Kollar, Alexandros G. Dimakis, Yair Carmon, Achal Dave, Ludwig Schmidt, Venkatesh Shanka



Using DataComp-LM for Dataset Development and Model Training
The researchers leveraged DataComp-LM to create DCLM-BASELINE, a high-quality dataset used for training models that strike a balance between computational requirements and performance. They conducted comparisons on two sets of tasks:

CORE Tasks:

The DCLM-BASELINE dataset (orange) exhibited better performance compared to close-source models (crosses) and other open-source datasets and models (circles).
MMLU 5-shot Tasks:

DCLM-BASELINE also showed favorable performance on these tasks.
The models utilized in the comparison were not specified in this section.

Introduction
The researchers highlight the significance of large training datasets in advancing language modeling. They emphasize the need for improving training datasets along with scaling models to achieve efficient generalization across various tasks. Several challenges exist in this domain, such as the lack of controlled comparisons due to variations in model architectures, hyperparameters, and compute resources used in evaluations.

Key points included:

The importance of controlled comparisons in assessing data curation strategies for language models.
Challenges due to the scarcity of details about training sets in current research.
Introduction of DataComp for Language Models (DCLM) as a benchmark for training data curation.
Description of DCLM-POOL, a vast public corpus used for language model training.
Investigation of scaling trends for dataset design across different compute and data scales.
Findings on the impact of filtering models on performance and the role of human judgments in identifying quality training data.
Development of DCLM-BASELINE as a new state-of-the-art public training set for language models.
Achievement of competitive MMLU performance with the 7B parameter model using DCLM-BASELINE.
This section underscores the importance of systematic data curation in training high-performing language models and introduces a comprehensive benchmark, DCLM, to facilitate further research in this area.

Related Work
The authors survey related work on data curation for language models, emphasizing the importance of refining datasets for model training. Key points included:

Researchers often rely on web crawls to gather large datasets, which may contain low-quality content necessitating curation.
Previous efforts focused on improving model performance through methods like language filtering, heuristic-based filtering, and data deduplication.
The authors conducted a significant investigation into data curation, resulting in the creation of the DCLM-BASELINE dataset.
Various open-source datasets have been curated to match the increasing scale of language models, with token counts ranging from billions to trillions.
The authors provide performance comparisons with different datasets and delve into the FineWeb's LightEval evaluation framework.
They release a vast pool of raw text data derived from web crawls and introduce the DCLM-BASELINE dataset, which outperforms previous datasets.
Data-centric benchmarks like dataset distillation, curriculum learning, and transfer learning have been pivotal in assessing data enhancements for language models.
The DCLM benchmark, with a massive token pool and various models, serves as a significant data-centric benchmark for language models.
DataComp for Language Models (DCLM) Benchmark
This section outlines the essential components of the DCLM benchmark. The summary covers:

Introduction to DCLM-POOL, the raw text corpus forming the foundation of the benchmark.
Development of the DCLM workflow, involving selecting a competition scale, curating datasets by filtering DCLM-POOL, potentially incorporating other sources, training models with fixed hyperparameters, and evaluating models to assess dataset performance.
DCLM-POOL
The DCLM-POOL corpus is an extensive unfiltered web-text dataset created from all Common Crawl data up to 2023. Here are the key points from this section:

DCLM-POOL comprises 200 billion documents, equivalent to 370 terabytes after gzip compression, providing a massive dataset for research.
Text from HTML is re-extracted using resiliparse instead of relying on Common Crawl's pre-extracted text, ensuring data freshness.
Five competition scales within DCLM enable research across various computational settings, each specifying model size, training tokens, and pool size for filtering.
Estimates for training compute requirements like Train FLOPs (calculated as 6N D) and GPU hours (using the OpenLM framework) are provided to guide researchers.
Tools based on Lee et al. are offered for decontamination analysis to help researchers assess dataset overlap with provided test sets.
Submission guidelines require disclosure of decontamination efforts and advocate against the use of highly contaminated data to maintain research integrity.
Evaluation of contamination effects on models is carried out using the decontamination tools on DCLM-POOL in Section 4.6, with core evaluations targeting a score of 7 billion-1x for high-performing submissions.
Scale
The researchers created different competition scales with varying compute constraints to make their DeepCrowd Large-scale Modeling (DCLM) accessible and to study scaling trends. Key points included:

Five competition scales were established: 400M-1x, 1B-1x, 1B-5x, 7B-1x, and 7B-2x, each specifying model parameters and a Chinchilla multiplier.
Training tokens for each scale were determined as 20 times the number of parameters multiplied by the Chinchilla multiplier.
Performance of data curation methods was evaluated across different scales, revealing a high correlation between smaller and larger scale results (r = 0.885 and r = 0.919 for 400M-1x, 1B-1x, and 7B-1x).
Concerns about changing rankings with increased compute scale were addressed by assessing method performance across different scales.
Further insights on scale ablations and dataset improvements relative to training hyperparameters can be found in Appendix H.
Benchmark Tracks: Filtering and Mixing
The section introduces two tracks for participants after selecting a scale:

Filtering Track:
Participants propose algorithms to select training data from candidate pools.
Initial pools are random document subsets of DCLM-POOL for each scale.
Pool sizes are restricted by scale to promote scalable filtering strategies and mimic real-world constraints on data download and storage.
Mixing Track:
Submissions involve combining documents from multiple sources.
Participants can synthesize documents from various sources like DCLM-POOL, custom crawls, Stack Overflow, and Wikipedia.
Key Points:

Participants choose between filtering and mixing tracks after selecting a scale.
Filtering track focuses on selecting training data from candidate pools with restricted pool sizes based on scale for scalability.
Mixing track involves combining documents from diverse sources like DCLM-POOL, custom crawls, Stack Overflow, and Wikipedia.
Detailed rules for each track are provided in Appendix C.
Appendix D explains the open-source tooling for executing filtering and mixing operations.
Training
The authors maintain a consistent training configuration across scales to focus on dataset interventions. Here's how they set up their training process:

The training recipe remains fixed for each scale to isolate the impact of dataset interventions.
They use a decoder-only Transformer model like GPT-2 or Llama, implemented in OpenLM.
Unified data processing utilities are offered to support the training process effectively.
Additional training specifics can be found in Appendix F for more in-depth details.
Evaluation
The evaluation suite for the research, based on LLM-Foundry, covers 53 tasks across various domains like question answering, coding, text-book knowledge, and common-sense reasoning. The evaluation focuses on data curation algorithms and utilizes three key performance metrics:

MMLU 5-shot accuracy is used to compare models like GPT-4 and Llama 3 70B.
CORE centered accuracy is calculated over a subset of 22 tasks such as HellaSwag and ARC-E, providing a low-variance signal even at small scales. The accuracy is rescaled per task to represent random guessing (0) to perfect accuracy (1) linearly.
EXTENDED centered accuracy is the average of centered performance across all 53 tasks.
For more detailed metrics, refer to Appendix G in the paper.

Building high-quality training datasets with DCLM
The section outlines how the DCLM workflow contributes to creating high-quality datasets and assesses the impact of data curation techniques. Here's a breakdown of the key points:

Converting Common Crawl: The process involves converting Common Crawl into the DCLM-BASELINE dataset.

Ablation Experiments: The researchers conduct ablation experiments for each phase of dataset creation to analyze the effectiveness of each step.

Evaluation of Open-Source Datasets: The study begins by assessing open-source datasets as a foundational element (Section 4.1).

Experimentation with Dataset Construction Phases:

Text Extraction (Section 4.2): Different methods are explored for extracting text from the datasets.
Deduplication (Section 4.3): Various techniques for removing duplicate entries are examined.
Model-Based Filtering (Section 4.4): Experimentation with filtering techniques based on models is undertaken.
Integration of High-Quality Sources (Section 4.5): The researchers explore incorporating high-quality sources to enhance the dataset.

Contamination Analysis (Section 4.6): An analysis is provided to assess and mitigate contamination in the dataset.

Scaling the Approach: In Section 5, the approach is scaled up to train a 7B model for 2T tokens, demonstrating the scalability and applicability of the methodology.

Evaluating Existing Training Datasets
The researchers start by assessing various popular open-source datasets such as C4, RefinedWeb, RedPajama, and Dolma-V1 for tasks like rule implementation (e.g., spam removal) and content deduplication. Notably, RefinedWeb is derived solely from Common Crawl, whereas RedPajama and Dolma-V1 incorporate curated sources like Wikipedia in addition to Common Crawl.

Comparison of these datasets highlights the differing strengths in filtering approaches, a factor the researchers investigate further in their experiments.
The evaluation indicates that the filtering employed by RefinedWeb stands out, leading to its selection for use in DCLM-BASELINE and other experimental setups.
The comparison of these datasets provides insights into their filtering effectiveness, with RefinedWeb's filters proving to be particularly effective, influencing the choice for subsequent experimental setups.

Text Extraction
Text extraction, an initial processing step in pulling content from raw HTML, is compared across three approaches in this study: resiliparse, trafilatura (utilized by RefinedWeb), and WET files from Common Crawl containing pre-extracted text. The comparison reveals the following:

Resiliparse and trafilatura enhance CORE by a minimum of 2.5 points compared to WET extraction.
Open source datasets like C4, RedPajama, and Dolma-V1, which predominantly use WET extraction, exhibit inferior performance.
Resiliparse, being 8 times faster than trafilatura, is more efficient for large-scale processing despite both approaches showing similar downstream performance.
Furthermore:

Appendix J provides additional in-depth analysis.
Resiliparse is chosen for text extraction in DCLM-POOL and subsequent experiments due to its efficiency.
This section underscores the importance of the text extraction method in processing raw HTML content and its impact on downstream performance metrics.

Deduplication
Web-crawled datasets often contain numerous duplicate or near-duplicate data strings, impacting performance and data diversity. The researchers employed two main approaches for deduplication:

MinHash and Near-Duplicate Bloom Filtering:

MinHash, integrated into a suffix array pipeline.
Near-duplicate Bloom filtering, modifying an exact deduplication scheme.
Both approaches yield similar downstream performance, with around a 0.2 CORE percentage point difference at the 7B-2x scale.
Scalability:

The modified Bloom filter method demonstrates superior scalability for datasets exceeding 10TB.
Implementation:

Bloom filter utilized for DCLM-BASELINE.
MinHash employed for other experiments.
This section highlights the effectiveness of different deduplication techniques, showcasing the scalability advantage of the Bloom filter approach for handling large datasets exceeding 10TB.

Model-based quality filtering
The section investigates the use of learnable models for quality filtering, comparing various strategies such as PageRank score filtering, Semantic Deduplication (SemDedup), linear classifiers based on pre-trained BGE text embeddings, AskLLM for document relevance, Perplexity filtering, Top-k average logits, and fastText binary classifiers.

Experimental Setup:

Training data consists of around 400k documents split equally between positive and negative classes.
Different positive data options are explored while negative data is sampled from RefinedWeb.
Utilizes a 154M parameter causal Transformer trained on a mix of English Wikipedia, RedPajama v1 books subset, and peS2o for perplexity filtering and top-k average logits strategies.
Results:

Comparison in Table shows fastText-based filtering outperforms other approaches.
Study on how fastText training recipes impact its effectiveness as a data filtering network.
Text Classifier Variants:

Explore different variants of fastText, considering reference positive data from sources like Wikipedia, OpenWebText2, RedPajama-books, and novel sources like OpenHermes 2.5 and r/ExplainLikeImFive subreddit.
fastText OH-2.5 + ELI5 approach yields a 3.5 percentage point lift on CORE compared to conventional choices.
Threshold Analysis:

Strict threshold selection (top 10% examples) shows improvement compared to more lenient top 15% and top 20% thresholds.
Appendix P demonstrates compatibility of the OH-2.5 data with modern fine-tuning paradigms.
Additional Observations:

Study on the relationship between dataset filtering behavior and human judgment in Appendix M.
Decontamination
The researchers conducted an analysis to investigate the impact of contamination in pretraining data on model evaluation results, focusing on MMLU as the evaluation metric. Here are the key points included:

The experiment aimed to detect and remove questions from the MMLU evaluation set that were present in the DCLM-BASELINE pretraining data.
Training documents containing the last sentence of a question from MMLU along with one of the corresponding options were flagged for removal.
The flagged examples had the matched question and option strings removed to reduce contamination.
After training a 7B-2x model with the cleaned DCLM-BASELINE data, it was observed that performance did not decrease, indicating that the gains in model performance were not due to increased presence of MMLU in the dataset.
Removal of contaminated samples did not result in a performance decrease, as shown by comparing results between models with and without the contaminated data removed.
The contamination removal strategy was also applied to other datasets like Dolma-V1.7 and FineWeb-Edu to compare contamination levels with DCLM-BASELINE.
The analysis revealed that DCLM-BASELINE had similar contamination statistics to other high-performing datasets.
Furthermore, the researchers scaled up the DCLM-BASELINE model to a trillion token scale by combining it with additional datasets like StarCoder and ProofPile2.
Results showed that the 7B model trained on the 4.1T token dataset outperformed models trained on public datasets and approached performance levels of models trained on more tokens.
The model achieved strong instruction-tuning performance, maintaining benchmark performance after tuning on IT datasets, with an AlpacaEval2.0 LC Win-rate surpassing certain closed-data models.
Conclusion & Limitations
The authors introduced the DCLM testbed, showcasing advancements in creating new state-of-the-art training sets. However, the exploration of the dataset design space has constraints. Some key points and limitations are as follows:

Due to compute limitations, the authors could only analyze design dimensions individually and not test all approaches on larger scales.
Various unexplored variations of DCLM-BASELINE exist, such as the impact of sharded deduplication and different training approaches for filtering models in terms of architecture and data.
Most experiments were conducted using only one tokenizer (GPT-NeoX), potentially missing out on better performance with other tokenizers for multilingual tasks or math challenges.
The study did not adequately explore the effects of run-to-run variations from different random seeds.
While the current models based on DCLM-BASELINE perform well in common language understanding tasks, they fall short in code and math tasks, indicating avenues for improvement.
Future directions involve integrating domain-specific training sets for code and math to enhance performance and extending DCLM to encompass these areas.
Important performance dimensions like fairness, multilinguality, safety, toxicity, and privacy filtering are not fully addressed in the evaluation suite, suggesting areas for expansion.
The scalability of the models trained as part of DCLM to larger scales remains untested, considering that current state-of-the-art models are significantly larger.
A limitation of the DCLM-BASELINE approach could be its stringent filtering ratio, hinting at a potential area for refinement in future research.
Language Detection
Language detection methods primarily utilize a fastText classifier trained to recognize 157 languages, although other classifiers like a naive Bayes classifier have been used.

Curation Options:
Datasets can be collected by filtering web pages based on country domains or selecting URLs correlated with specific languages.
Heuristics for Data Cleaning:
Different heuristic-based systems identify and remove undesirable data from web-scraped text, such as boilerplate HTML and error messages.
Heuristics categories include item count, repetition count, existence, ratio, and statistics.
Quality Filtering:
Filtering for high-quality data, often done using classifiers trained on high vs. low-quality datasets, helps in data curation.
Pretrained Language Models:
Use of pretrained language models for identifying and curating high-quality data through prompting for perceived quality dimensions has been proposed.
Data Deduplication:
Deduplication methods vary from URL-based deduplication to hash-based methods like Bloom filters and MinHashLSH.
Data Mixing:
Determining the source distribution in a mixed-domain dataset involves heuristics, empirical methods, or principled approaches like Group DRO and multi-armed bandits.
Synthetic Data Generation:
Synthetic data generation, including methods like Phi models and WRAP, has gained traction for data curation tasks and pretraining language models.
Novel Approaches:
Recent approaches like MixMax and BiMix aim to optimize data mixing methods and scaling laws for data quantity and source mixing.
Application Beyond Performance:
Various methods exist for purposes beyond performance improvement, such as removing copyrighted content, toxic speech, and private information, crucial for real-world data curation.
C Benchmark Rules
This section outlines the specific guidelines for submissions in the two DCLM tracks, emphasizing the benchmark rules in the context of the research paper.

Detailed guidelines are provided for submissions in the DCLM tracks.
The section specifically focuses on the benchmark rules for the participants.
Participants are expected to adhere to these rules for their submissions in the competition.
The rules are crucial for ensuring a fair and standardized evaluation of participants' contributions in the DCLM tracks.
C.1 General Rules
The general rules apply to both the filtering and mixing tracks:

Submissions must include detailed documentation outlining their key components.
To promote reproducibility, submissions to the leaderboard must provide either the dataset used or fully functional code for reproduction, which should be freely accessible. Submissions lacking this may still be accepted but will be marked as such on the leaderboard.
Tokenization of data must be done using the prescribed script that both tokenizes the data and executes a global shuffle.
Submissions are not allowed to modify the training or evaluation code.
It is strictly prohibited to use evaluation data (test data from the evaluation tasks) for any purposes other than evaluation and decontamination.
C.2 Filtering track
The entries in the filtering track form the dataset by processing a subset of DCLM-POOL for the chosen compute scale without incorporating external data. This requirement has two main purposes:

Standardization of Initial Data:

Fixing the initial dataset ensures a level playing field for comparison of core curation techniques.
The size and quality of the initial data significantly impact processing costs and dataset quality.
Encouraging Innovation:

By using a specific subset of DCLM-POOL for each compute scale, the goal is to promote methods that are relevant even at large scales.
For instance, using a pool with 16T tokens for a compute scale requiring 8B tokens pushes participants to develop efficient filtering strategies that are scalable.
Criteria and allowances for submissions in this track include:

HTML Extraction Modification:

DCLM-POOL is created by text extraction from Common Crawl archives using resiliparse, but participants can experiment with text extraction by specifying the Common Crawl archives.
Participants can either start with the pre-parsed DCLM-POOL data or directly use the Common Crawl WARC archives.
Use of External Models:

The processing pipeline for DCLM-POOL can utilize models for tasks like quality filtering or paraphrasing.
Models trained on external data are allowed for this purpose, except for evaluation data to prevent the introduction of external data through a backdoor.
Submissions attempting to include external data covertly, like memorizing and regenerating DCLM-POOL documents, will not be accepted.
C.3 Mixing Track
In the mixing track of the paper, participants have the flexibility to utilize any data source for their submissions, as long as the data adheres to certain criteria:

Data must be freely accessible.
Evaluation data should not be included in the chosen sources.
Submissions Guidelines: When submitting to the mixing track, the entries should provide clear documentation regarding:

The data sources utilized.
The weighting assigned to each data source.
The proportion of tokens allocated for training, which remains consistent across every benchmark scale, in relation to the total custom pool size.
D Tooling
The authors utilize various tools and processes in their data processing pipeline, which involves downloading WARC files from Common Crawl and processing them using resiliparse through the Ray data processing framework. Here's a breakdown of the key aspects of the tooling used:

Download:
Warc files are downloaded from Common Crawl for pool construction.
Data processing involves streaming from S3 to EC2 using the Ray data processing framework.
Processing:
A processing pipeline is defined to clean, modify, and filter the raw text pool.
The pipeline involves sharding the pool and processing it in parallel.
Different types of mappers are used in the processing pipeline:
Filters: Retain or discard documents based on filtering criteria like length.
Enrichers: Add information to metadata.
Modifiers: Change text content and split documents.
Corpora-Level Operations:
Besides document-level processing, the tooling supports corpora-level operations.
Users can execute operations like deduplication of text spans across documents.
Contamination Analysis:
Tools from previous studies are adapted to evaluate contamination in the training set with evaluation sets.
Evaluation includes measuring token overlap between training and evaluation samples.
Tokenization and Shuffling:
Standardized code is used for tokenization and shuffling to create trainable dataset artifacts.
GPT-NeoX tokenizer is utilized for tokenization.
Tokenization code scales from single-node to multi-node setups for different dataset sizes.
Training Setup:
Training code is based on OpenLM, with configuration files provided for different scales.
Scripts are available to train models using each configuration, generating detailed model descriptions.
Evaluation:
Evaluation pipeline is based on tasks from LLMfoundry.
Tools evaluate models on various tasks, producing detailed evaluation results for each task and aggregate metrics.
Reproducibility:
All results, including data processing, model training, evaluations, and plots, can be reproduced using the open-source framework provided by the authors.
Compute requirements are detailed in the appendix for reference.
E DCLM-POOL
The DCLM-POOL dataset was created by extracting text from 5.1 million Common Crawl WARC dumps spanning from 2008 to 2022, excluding data from 2023 onwards to prevent contamination by language model generated text in the future. The dataset is released on HuggingFace under a CC-BY-4 license in .jsonl format similar to Dolma-V1 and RedPajama datasets. Here are the key points related to DCLM-POOL:

Dataset Details:
Consists of 5.1 million gzip compressed .jsonl files, totaling 340TB on disk.
Fields provided in the .jsonl files can be found in a designated table.
Usage and Licensing:
Usage of the dataset is subject to CommonCrawl's Terms of Use.
Respect for robots.txt is maintained in DCLM-POOL, allowing content creators to opt out of Common Crawl and DCLM-POOL.
Privacy and Data Handling:
Contains some personally identifiable information (PII) data due to its origin in Common Crawl.
Common Crawl honors deletion requests and regularly redacts data.
Data Integrity and Updates:
Maintains a one-to-one mapping between raw Common Crawl WARC files and DCLM-POOL .jsonl files to facilitate updates based on redactions.
No special treatment for PII or sensitive content is applied to preserve the raw data's representativeness.
For a detailed discussion on personally identifiable information (PII) and consent related to the dataset, refer to the corresponding section in the provided Appendix S.

F Training Details
The training setup in this study closely resembles previous works by Wortsman et al. and Gadre et al., utilizing the OpenLM framework for decoder-only, pre-normalization Transformers based on architectures like GPT-2 and Llama.

OpenLM: Implemented in PyTorch, the OpenLM code-base supports FSDP modules for distributed training.
Architecture Details:
Normalization: LayerNorm without bias parameters is used, qk-LayerNorm for query and key stability, SwiGLU MLPs, and a depth-scaled initialization scheme.
Sequence Length: Pre-training involves sequences of length 2048.
Training Batch Handling: Batches pack multiple sequences to fill entire contexts, using an EOS token to split documents. Causal attention can span across documents.
Training Data and Tokenization:
Dataset: Over 270 data distributions are trained on, primarily from Common Crawl, with tokenization using GPT-NeoX for a 50k vocabulary.
Optimization Approach: Standard next-token prediction objective is employed, with the utilization of z-loss for stable output logit magnitudes.
Hyperparameters:
Tables detailing hyperparameters for different model scales, optimized based on perplexity from validation sets including tokens from recent arXiv papers and news articles.
Exploration of alternative hyperparameters for the 1B-1x scale, with the best results from specific hyperparameters.
Higher learning rates and lower weight decays for the 7B-1x and 7B-2x scales determined by a hyperparameter sweep.
Training Configurations:
A cooldown of 3e-5 is used for all experiments.
Variations in learning rates based on experiments, such as lower learning rates for specific setups.
G Evaluation Details
The authors provided details on the evaluation tasks conducted to assess their models in LLM Foundry and discussed the LightEval evaluation pipeline utilized in the FineWeb-Edu evaluations. Key points included:

Evaluation tasks were outlined for assessing the models in LLM Foundry.
The LightEval evaluation pipeline was highlighted for the evaluations in FineWeb-Edu.
Both evaluation processes were crucial for testing and measuring the performance of the models developed in the study.
G.1 Evaluation Tasks
The researchers categorized their evaluations into two main types: CORE and EXTENDED tasks. Here's a breakdown of the evaluation tasks:

CORE Tasks (22 tasks):
Chosen for their consistency in indicating learning progress even with limited data.
Designed to provide a stable measure of learning performance.
EXTENDED Tasks (53 tasks):
Aimed at evaluating a wide range of model capabilities.
Intended to assess the model's performance across diverse scenarios and challenges.
CORE tasks
The paper discusses various CORE tasks designed to evaluate model performance and capabilities across different domains and types of reasoning. Here are some key points included:

AGI Eval LSAT-AR Dataset (3-shot):

Tests model knowledge in the legal domain and analytical reasoning capabilities.
ARC Easy and ARC Challenge Datasets (10-shot):

Multiple choice questions from grade 3-9 science exams.
Easy dataset requires basic science knowledge, while challenge questions involve procedural reasoning.
Tasks from Big-Bench (all 10-shot):

Various datasets like QA Wikidata, Dyck languages, Operators, Repeat Copy Logic, CS Algorithms, Language Identification, etc.
Evaluated tasks range from factual statement completion to language identification.
BoolQ (10-shot) and CommonsenseQA (10-shot):

Evaluated on binary question answering and multiple-choice questions to assess commonsense knowledge application.
COPA (0-shot) and CoQA (0-shot):

Test causal reasoning and conversational question answering abilities of models.
HellaSwag (0-shot and 10-shot):

Requires understanding of implicit context and common knowledge for commonsense reasoning.
Jeopardy (10-shot) and LAMBADA (0-shot):

Evaluate knowledge across a variety of topics and narrative context understanding.
OpenBookQA (0-shot) and PIQA (10-shot):

Test multi-step reasoning, commonsense, and physical reasoning abilities of models.
SQuAD (10-shot) and Winograd tasks:

Assess question answering and pronoun resolution skills based on contextual understanding and commonsense knowledge.
Tasks from AGI Eval Suite (all 3-shot):

Include tasks like LSAT-LR, LSAT-RC, SAT-En, and SAT-Math to evaluate legal domain knowledge, logical reasoning, and language/math capabilities.
AQuA (3-shot) and BBQ (3-shot):

Test algebra questions and detect model biases across various social dimensions.
Additional tasks from Big-Bench (all 10-shot):

Cover challenging tasks like GPQA, GSM8k, LogiQA, Math QA, MMLU, and PubMedQA to evaluate domain-specific knowledge and problem-solving capabilities.
Other tasks like Simple arithmetic, Social Interaction QA, SVAMP, Trivia QA, Winogender, LightEval, and HumanEval also form part of the evaluation suite.

G.2 LightEval
The researchers conducted a study using the LightEval evaluation framework to assess accuracy, particularly for 0-shot MMLU (Massively Multi-Label Learning with noisy labels) on 1B models. Here's a summary of the key points from this section:

Evaluation Study:

Utilized LightEval framework for evaluation.
Achieved scores above random (25%) for 0-shot MMLU on 1B models by considering log-probabilities of whole answer passages.
Examined correlation between LightEval and LLM Foundry evaluation scores, especially on multiple choice tasks like MMLU.
Identified differences between LightEval and LLM Foundry in evaluating MMLU, with LightEval considering complete answer sequences' log probabilities while LLM Foundry focuses on single letters' log probabilities.
Comparative Analysis:

Reproduced MMLU scores from the FineWeb-Edu blog.
LightEval demonstrated MMLU scores above random for 1B models, whereas LLM Foundry indicated accuracies around 0.25 for all 1B models.
At larger scales, LightEval scores for models were closely grouped together, potentially complicating model comparisons and introducing noise.
Model Comparisons:

Models like Gemma-7B, Llama3-8B, and Mistral-7B showcased similar scores in LightEval but exhibited varied scores in LLM Foundry.
Comparison between FineWeb-Edu 7B-2x and DCLM 7B-2x showed comparable performance in LightEval, yet DCLM-7B outperformed by nearly 10 points in LLM Foundry.
Framework Evaluation:

LightEval appears promising for evaluating smaller models, while frameworks like LLM Foundry may offer clearer insights for larger models.
Future Considerations:

The study focused on MMLU as a representative task, highlighting a limitation.
Future investigations will explore comparisons with various tasks and frameworks like Eleuther LLM Harness for a more comprehensive evaluation.
Hyperparameter Study
The authors address concerns about how training hyperparameters can affect conclusions on the optimal dataset for training by demonstrating in Table the preservation of dataset orderings across various weight decay and learning rate combinations.

Furthermore, they highlight that performance improvements from selecting optimal hyperparameters and designing datasets tend to be independent and complementary to each other, as shown in Table.

The section also delves into the detailed implementations of several model-based quality filters discussed earlier, with a specific focus on the fastText classifiers, chosen as the primary method for DCLM-BASELINE. Key points included:

Preservation of dataset orderings across different hyperparameter combinations
Complementary nature of performance gains from hyperparameter optimization and dataset design
Detailed implementation descriptions of model-based quality filters with a focus on fastText classifiers
I.1 fastText classifiers
The authors utilized the supervised fastText package by Joulin et al. to train models that classify between "high-quality" reference data (with positive labels) and web-crawled data (with negative labels). Here's a breakdown of the process:

Training Process:

Trained fastText classifiers to distinguish between positive and negative labeled data.
Applied classifiers to score documents for filtering based on predicted probabilities of positive labels.
Adjusted a hyperparameter by expanding feature space to include unigrams and bigrams for better model performance.
Data Sources for Positive Labels:

Utilized various sources for positively labeled reference data, including Wikipedia, OpenWebText2, GPT-3 Approx, and OH-2.5 + ELI5.
Processed Wikipedia data by filtering the English pages from the en.wikipedia.org domain and removed specific section titles to ensure core content reliance.
Used OpenWebText2 dataset as is and combined Wikipedia, OpenWebText2, and books sourced from RedPajama for the GPT-3 Approx mix.
Sampled instruction and question-answer formatted data for OH-2.5 + ELI5 mix from OH-2.5 and r/ExplainLikeImFive subreddit.
Data Preparation for ELI5:

Curated training examples by combining top-scoring answers with questions from the r/ExplainLikeImFive subreddit.
Filtered examples based on post and comment scores, retaining only those meeting specific score thresholds and comment quantity criteria.
I.2 Other Quality Filtering Baselines
The researchers explored various quality filtering methods beyond fastText but found them less effective. Some of the methods examined are as follows:

PageRank:
PageRank metrics from Common Crawl's webgraph dataset were used to identify central web text for higher quality data.
Hosts with PageRank scores were partitioned into quintiles, but no quintile outperformed the overall dataset.
AskLLM:
AskLLM method involved using instruction-tuned models to evaluate the informativeness of a document for pre-training language models.
Different models, sequence lengths, and prompts were tested, with Mistral-7B-Instruct-v0 configuration showing the best results.
However, this method was not as effective as fastText experiments and was costly to scale up.
Semantic Deduplication:
This approach involved embedding documents with pre-trained language models, clustering them using k-means, and retaining only one document from closely related groups for dataset diversity.
Despite following best practices, this method negatively affected the trained model and was computationally intensive at scale.
The researchers decided to rely on other deduplication methods due to scalability issues, leaving this area for future research.
J Text Extraction Comparison
The section provides a detailed comparison between "resiliparse," the text extractor chosen by the researchers, and two alternative extractors (WET files and trafilatura) used in other datasets. The comparison includes both quantitative and qualitative aspects, offering insights into the performance and efficacy of each text extraction method.

Quantitative and qualitative comparisons:

The researchers analyze how well resiliparse performs compared to WET files and trafilatura.
They delve into numerical data to show the extraction capabilities of each method.
Qualitative aspects are also considered, highlighting the quality and accuracy of the extracted text by each extractor.
Resiliparse:

Detailed evaluation of resiliparse's strengths and weaknesses.
Discussion on how resiliparse stands out in text extraction compared to the other methods.
Specific examples or metrics may be provided to illustrate the efficiency of resiliparse.
WET files and Trafilatura:

Comparison of the advantages and limitations of WET files and trafilatura.
Potential areas where these extractors excel or fall short in text extraction tasks.
Insights into the challenges or benefits of using WET files and trafilatura in comparison to resiliparse.
J.1 Profiling
The authors conducted profiling by analyzing basic summary statistics for each extractor using a sample of 10 WARC files, equivalent to 900K individual pages. Key points included:

Resiliparse and trafilatura produce documents that are at least 2 times shorter on average compared to WET files.
WET files contain numerous additional lines that are deemed less valuable for pre-training, like navigation bars and copyright statements.
The comparison between trafilatura and resiliparse revealed:

Resiliparse retains about 10% more text, which could include useful content like section titles and dates in articles.
Resiliparse significantly outperforms trafilatura in terms of processing speed, being approximately 8 times faster.
Appendix J.2 contains examples illustrating the differences in text trimming between the two extractors, showcasing how trafilatura is more stringent in removing unnecessary lines compared to resiliparse.

Resiliparse
The section provides insights into the differences between en-dashes and em-dashes in the context of English grammar. Here's a summary of the key points:

An en dash (-) is larger than a hyphen but smaller than an em dash (-).
The names come from an obscure typographical measurement system.
Em dashes can serve various purposes like replacing colons or parentheses, indicating a sudden change in thought or tone.
En dashes are used for showing numerical ranges (e.g., in dates, ages, pages), and for compound adjectives where the first part consists of multiple words.
Em dashes, like extended hyphens, are utilized to denote breaks in thought or shifts in tone.
For more detailed information on the distinctions between a hyphen, en dash, and em dash, the section refers readers to a provided blog post for further clarification.

K Deduplication
The section investigates different deduplication techniques through a series of experiments and ablations to select the optimal deduplication pipeline for generating DCLM-BASELINE and other DCLM scales. The content is structured as follows:

Deduplication Methods Considered:
The authors explore various methodologies for deduplication.
Ablations:
The ablation studies are outlined, highlighting the key factors that led to the selection of the deduplication pipeline used in creating DCLM-BASELINE and other DCLM scales.
K.1 Deduplication Methods
The authors discuss various methods for deduplication in text datasets, focusing on a two-stage pipeline that targets near duplicates at both inter-document and intra-document levels. Some key points include:

MinHash: Utilized to group sets based on Jaccard similarity; used in prior work and configurations based on Lee et al. and Penedo et al.
Suffix Arrays: Enables efficient identification and removal of substrings by sorting suffixes; used with specific hyperparameters similar to Lee et al.
Bloom Filters: Space-efficient structures for set membership queries; employed for near-duplicate removal and modified for document and paragraph level deduplication.
Paragraph + Document BFF Algorithm: Modified Bloom filter-based algorithm involving tokenization, paragraph processing, and removal based on defined hyperparameters like n-gram sizes and thresholds.
The Bloom filter method is highlighted as more efficient than MinHash and Suffix Array pipelines, with details on hyperparameter choices elucidated:

False Positive Rate: Crucial parameter influencing memory footprint, discussed in relation to the Bloom filter size.
Min_Ngram_Size: Determined based on the characteristics of documents, balancing token sizes for effective deduplication.
Threshold: Setting that affects the removal of documents based on the fraction of n-grams contained in the set, with threshold selection influencing false positive rates for duplicates.
K.2.1 Deduplication Ablations: Pipeline at 1B-1x Scale
The researchers conducted experiments to analyze the deduplication pipeline at the 1B-1x scale using a pool of 76B tokens subsampled from Common Crawl:

The pipeline involved a series of deduplication steps, reducing the token pool to 28B tokens for evaluation.
The study aimed to determine the impact of each step in multi-step deduplication pipelines and identify the most effective pipeline for scaling to larger pool sizes.
Results indicated that the Suffix Array deduplication method outperformed MinHash alone.
BFF showed comparable performance to the more complex Exact+MinHash+SuffixArray pipeline.
Sharding the dataset into smaller chunks was explored for parallelizing and scaling deduplication processes.
Key Points:
Evaluation metrics like CORE score and token removal percentage were used.
Different deduplication pipelines were tested for efficiency and scalability.
Results emphasized the effectiveness of the Suffix Array method and the potential of BFF as a scalable alternative.
Sharding datasets improved performance and enabled more efficient deduplication processes.
Hyperparameters like min_ngram_size, shards, and threshold were adjusted to optimize performance.
In further experiments at larger scales, the researchers assessed the performance of BFF against MinHash+SuffixArray pipelines:

BFF with specific hyperparameter settings demonstrated comparable performance to traditional pipelines.
Varying hyperparameters like min_ngram_size influenced performance metrics.
The study highlighted the impact of hyperparameters on token yield and evaluation outcomes.
Adjustments in hyperparameters like n-gram sizes and thresholds affected document statistics and removal rates.
Key Takeaways:
Optimal hyperparameter configurations were identified for maximizing deduplication performance.
The study emphasized the importance of balancing hyperparameters for efficient and effective deduplication.
Sharding datasets and adjusting thresholds were crucial for improving performance at larger scales.
Results provided insights into the influence of hyperparameters on dataset statistics and deduplication efficiency.
K.2.4 Global MinHash on Open Datasets
The researchers conducted Global MinHash on open datasets to assess remaining duplicates post data processing. Key points include:

Evaluation of DCLM-BASELINE, RefinedWeb dataset from HuggingFace, pipeline emulation, and Dolma V1 using MinHash with 14 buckets and bucket size of 9.
Observations:
Bloom Filter-deduplicated pools still contain many "fuzzy duplicates" per MinHash/Jaccard Similarity.
MinHash is nearly idempotent, but deduplication over shards doesn't eliminate a significant duplicate portion.
100-shard Bloom filter deduplication in DCLM-BASELINE retains duplicates without impairing downstream performance.
Discussion challenges the belief that any duplicates affect downstream performance, suggesting either only vast duplicates or aggressive single-shard deduplication harm performance, prompting further research.
L Mixing sources:
The researchers investigated the impact of mixing their dataset with other sources on overall performance and found that integrating their dataset with common sources did not enhance performance due to stringent filtering applied to their Common Crawl portion. To explore if better filtering in other sources could improve performance similarly, they conducted an experiment:

Applied the fastText classifier used in their DCLM-BASELINE to filter RedPajama sources.
Selected only the highest scored sources after filtering.
Added this new data to their pretraining dataset and trained models at the 1B-1x scale.
The findings, as presented in a table, revealed that despite more uniform mixing procedures across different sources, incorporating additional sources still resulted in decreased performance. The researchers deferred further analysis on potential mixtures of their datasets with other sources for future research and highlighted the mixing track as a pathway for researchers to explore such avenues.

M Human Judgment
The section discusses the use of human annotators for data annotation, highlighting both the advantages and challenges associated with this approach:

Human annotators are considered the gold standard for data annotation despite potential biases and noise.
An experiment involved 16 AI graduate students and professors annotating 500 documents without a quality filter.
Comparison of different quality filters showed varied performance, with human-correlated filters performing poorly as quality filters.
Correlations between quality filters and downstream tasks were examined, revealing mixed results with low correlations observed in most cases.
Findings suggest human judgment may not consistently identify the most useful documents for language model training.
Data collection involved sampling 499 documents, using rule-based filters and deduplication.
Annotators assessed examples for inclusion in a language model pretraining corpus based on specific criteria.
Cooking Instructions
The recipe begins by heating butter and oil in a wok or saucepan. Here are the steps to follow:

Saute Chinese five-spice powder and shallots until tender.
Add garlic and continue sauteing.
Increase heat, add fresh and reconstituted-dried shiitake mushrooms with mushroom water.
Cook until mushrooms are tender, then add soy sauce and rice wine.
Increase heat, stir for another minute and add remaining mushroom-soaking water.
Add noodles and heat through in the sauce.
Garnish with green onions and sesame seeds if desired, then serve immediately.
Value of Travel Guides
Travel guides, whether in electronic or traditional form, offer extensive information, answering various questions for travelers:

Provide language basics for local interactions.
Offer insights on accommodations, attractions, and dining options.
Explain the history and ambiance of specific regions.
Doc4 (Good)
The section debates the necessity of buying a travel book versus obtaining similar information from other sources by outlining the pros and cons of purchasing a travel book.

Pros of Purchasing a Travel Book:
Detailed information: Travel books often provide in-depth details about destinations, attractions, and practical tips.
Reliable recommendations: They offer trusted recommendations on accommodations, restaurants, activities, and more.
Convenience: Having a physical book can be more convenient than relying on digital sources, especially in areas with limited connectivity.
Cons of Purchasing a Travel Book:
Outdated information: Travel books may contain outdated details due to the time lag between publication and travel.
Limited scope: They might not cover niche or off-the-beaten-path locations that are available on newer online platforms.
Cost: Travel books can be expensive, and some travelers prefer free or more cost-effective alternatives online.
This section aims to help readers weigh the benefits and drawbacks of investing in a travel book for trip planning, acknowledging the availability of alternative sources for travel information.

Advantages of a Travel Book
Travel books, whether in paperback or e-book format, serve as valuable companions during travel, offering several benefits:

Insight into Culture and Customs: Travel books provide valuable insights into the customs and culture of a specific destination, helping travelers familiarize themselves with local traditions.

Comfort and Adaptation: By browsing a travel book, individuals can adapt to the environment of the place they are visiting, making their stay more comfortable and enjoyable for extended periods.

They Come In Handy
The travel guide mentioned in the paper is available in different formats to cater to various preferences and needs. These include e-books, paperbacks, and digital file formats. Here's why these guides are essential for travelers:

Accessibility: Travelers can easily access these guides, making it convenient to obtain detailed information relevant to the specific region they are visiting.
Comprehensive Assistance: The travel guides are designed to provide all the necessary details and information that travelers may require during their journey.
Variety of Formats: By offering guides in multiple formats, travelers can choose the version that best suits their preferences and travel style.
Region Compatibility: The content of the guides is tailored to be compatible with the region the traveler is exploring, ensuring relevant and localized information.
N Decontamination
In this section, the researchers analyzed contamination in the data by comparing token overlaps between different sets. Here are the key points included:

Instead of the MMLU-specific decontamination method used earlier, a more general approach based on token overlaps was followed.
Specifying a universally applicable decontamination rule is challenging due to subjectivity in defining contamination in text data and diverse formats across tasks.
Following Touvron et al., contaminated tokens existing in overlapping 10-grams or longer between DCLM-BASELINE and downstream tasks were identified.
Percentage of samples with varying levels of contamination (clean or dirty based on token overlap criteria) in each evaluation set was measured.
Performance of the model trained on DCLM-BASELINE was compared between the full evaluation set and subsets categorized as "not dirty" or "clean" based on contamination levels.
Findings showed minimal performance differences between the full dataset and "not dirty" samples; for highly contaminated datasets like BoolQ and SQuAD, the model performed slightly better on "not dirty" samples.
Analysis illustrated that the difference in performance between the full set and "clean" subset was also small for most datasets.
Determining the correct threshold for defining contaminated samples is challenging; a 20% token overlap might yield false positives, while 80% might miss some contaminated samples.
Instruction Tuning
Instruction tuning is a crucial step that allows users to interact effectively with pretrained language models, such as DCLM-BASELINE. Here's a summary of the key points in this section:

The researchers instruction-tuned the DCLM-BASELINE (7B) with the OpenHermes-2.5 (OH-2.5) dataset by training the model to predict responses based on instructions.
Training involved using the Adam optimizer for 10 epochs with specific hyperparameters like a warmup ratio and a cosine learning rate schedule.
A hyperparameter search was conducted over three learning rates, and the best-performing results were reported using evaluation metrics like AlpacaEval 2.0 length-controlled win-rate.
The model's performance was benchmarked against baselines like LlaMA-2-Chat, Zephyr-7B, and Mistral-7B, showing that DCLM-BASELINE finetuned with OH-2.5 outperformed other instruct models.
Performance analysis was conducted on subsets of evaluation tasks categorized as "dirty" and "clean," showcasing the model's abilities under different contamination levels.
Despite a slight lag behind Mistral-7B-OH-2.5, DCLM-BASELINE showed competitiveness, which was attributed to longer generations and fewer tokens seen during pretraining.
To enhance instruction-following capabilities further, a custom dataset, DCLM-IT, was curated by combining various instruction-tuning datasets, leading to significant data instances and tokens.
The researchers performed instruction-tuning exercises with specific cooldowns, creating a "model soup" with varying token weights to improve competitiveness with models of similar scale.
The final model soup was trained for 100B tokens, presenting results in a comprehensive manner for evaluation.
P.1 Instruction Tuning the Scaled-Up Model
The researchers illustrate how instruction tuning a model with 80B additional tokens enhances performance on various benchmarks, surpassing similar 7B models like Gemma-7B. Here are the key points outlined in this section:

Instruction tuning benefits the model significantly on extended evaluations while only slightly impacting core and MMLU evaluations.
Results indicate a substantial performance increase of 2.5% to 52.5% on GSM8k, aligning with comparable models that incorporate IT data during pretraining.
Continual learning results are presented for adapting a 7B model to a context length of 8192, following a specific recipe.
The continual learning process involves warming up to a maximum learning rate of 10^-4 over 2000 steps and then annealing with a cosine schedule to 10^-5, maintaining other hyperparameters from pretraining.
The global batch size remains consistent at 2^22 tokens per optimization step, with a varied sequence length curriculum ranging from 64 to 8192 in length.
Training during this stage involves approximately 120B randomly sampled tokens distributed across different sequence lengths.
Various tools and datasets are used for instruction tuning and ablations, each governed by distinct licenses such as MIT, Apache 2.0, Open Data Commons License Attribution family, and Creative Commons Attribution Non-Commercial 4.0, among others.
The authors emphasize adhering to the respective licenses and terms of use associated with the datasets and tools utilized in these processes.
R.3 Libraries
The main libraries used in the benchmark pipeline include:

DCLM-POOL is based on Common Crawl, capturing the internet at a specific time, which can introduce noise like broken links, placeholder text, and duplicate data.

The dataset is self-contained for this benchmark but includes URLs linking to external resources on the internet with no guarantee of their permanence or consistency.

Confidentiality: While the dataset contains publicly accessible internet data, it may unintentionally include confidential information that was made public.

Offensive Content: The dataset may contain offensive, hateful, or toxic material due to its nature of web-scraped text; efforts are made to apply content-filtering from RefinedWeb to improve safety.

People-related Data: As a snapshot of the internet, the dataset may include information shared intentionally or unintentionally about individuals.

Identification of Subpopulations: The dataset does not explicitly identify subpopulations based on age, gender, or other factors in its metadata.

Possibility of Identifying Individuals: Given the prevalence of names and identifiers in web data, some content may be traceable back to specific individuals.

Sensitive Data: The dataset may reveal sensitive information such as racial origins, political opinions, or religious beliefs as individuals often publish such data on public sites willingly.

Efforts are encouraged to filter potentially biased or toxic content from the dataset to ensure privacy and avoid the inclusion of sensitive information in training models.

S.3 Collection Process
The data acquisition process involved scraping public internet data by Common Crawl, utilizing various mechanisms and procedures for data collection and validation:

Common Crawl directly scraped data from the public internet.
Python-based processing scripts parsed the archives, filtering and deduplicating content.
Model-based features were computed after data processing.
Hundreds of AWS CPU nodes were used for data parsing, while GPU clusters ran model-based features.
The sampling strategy for the dataset, if sampled from a larger set, was deterministic or probabilistic.
Users were provided with a mechanism to request exclusion of specific URLs from the datasets.
Previous research highlighted the presence of hate speech and explicit content in web-based data even after filtering, emphasizing the risk of bias propagation and harmful stereotypes.
Future studies are encouraged to utilize the datasets to explore techniques for improving web-scale datasets.
Key Points
Data was scraped from the public internet by Common Crawl.
Python scripts were used to filter and process the data, with model-based features computed thereafter.
Different hardware configurations were employed for data parsing and feature computation.
Users could request specific URLs to be excluded from the datasets.
Concerns about bias and harmful stereotypes in language model training from web-based datasets were raised.
Encouragement for future research to enhance web-scale dataset creation using the provided datasets.
Q38 Any other comments?
The authors acknowledge the vital role of tools developed by the open-source community in creating DCLM-POOL, DCLM-BASELINE, the DCLM tooling, and their trained models. These tools were essential for the research and would not have been possible without the contributions from the open-source community.

DCLM-POOL, DCLM-BASELINE, DCLM tooling, and trained models are heavily dependent on tools developed by the open-source community.
The authors express gratitude for the support and contributions that made their research and tool development feasible.
S.5 Uses
The full dataset and its subsets have been utilized to train numerous language models of different scales and compute budgets, evaluated on a testbed of 53 zero- and few-shot downstream tasks.

The dataset has been used in training language models for diverse tasks.
Evaluation of models was conducted on a testbed of 53 zero- and few-shot downstream tasks.
A leaderboard linked to DCLM allows review of submissions and publications utilizing the data at https://datacomp.ai/dclm/leaderboard.
The dataset has the potential for various applications beyond language models, including spell-checking, translation, interactive agents, sociological studies analyzing biases and trends in human communication, as well as studying human behavior online.

Suitable for pretraining large language models.
Useful for sociological studies examining biases and trends in human communication and behavior online.
Precautions need to be taken as the dataset inherently contains biases and stereotypes from the internet, making it unsuitable for use in production systems involving sensitive areas like race, gender, ethnicity, and decision-making about individuals.

Not recommended for applications involving decision-making about individuals.
Intended exclusively for research purposes to analyze dataset curation and study the impact of different curation methods on downstream models.
The dataset, DCLM-POOL, and related subsets are intended for research purposes only, not for applications involving decision-making about individuals due to inherent biases and stereotypes.

Meant for academic research to establish benchmarks across various dimensions.
Essential for enhancing datasets' effectiveness, minimizing redundant efforts, and ensuring safety.