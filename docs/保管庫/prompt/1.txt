ではまずは## 1. Training Data Volume and Dialogue Lengthのパート１に着手したい。
まず前提の共有なのですが、今回目指そうとしたのはあくまでソクラテス風の口調のチャットボットです。”問う力”というのは非常に深遠な能力であり、ソクラテス式問答に熟練したチャットボットはgemma2の２ｂでは敷居が高すぎると判断したためです。つまり”問う力”という深いレイヤーへの働きかけではなく、口調というわりと浅いレイヤーのチューニングがゴールである、という前提を絶対忘れないでください。で、その前提でどういう学習データにしようかを考えた結果、以下のような構成にしました。
### Decision
The training dataset was structured with the following specifications:
- Total dialogues: 2,662
- Total tokens: 752,369
- Average tokens per dialogue: 282.6
- Token range: 44-552 tokens
- Average user utterance: 169.4 tokens
- Average model response: 113.2 tokens

ポイントになってくる点は２つで、
１）学習データ全体の規模が妥当かどうか、という点、
２）一つの対話の長さはどの程度が良いか？という点です。

それぞれの点について、私の判断の妥当性・正当性を持たせる論文はありそうか、を以下の論文から具体的な記述をまずは抽出してDESIGN_DECISIONS.mdの## 1. Training Data Volume and Dialogue Lengthの箇所に記入してください。

