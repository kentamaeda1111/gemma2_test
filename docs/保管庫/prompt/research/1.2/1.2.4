4. StyleDGPT: Stylized Response Generation with Pre-trained Language Models




This paper introduces STYLEDGPT, a model for generating stylized responses in open-domain dialogue systems. The core problem addressed is the scarcity of parallel data (paired conversations with specified styles) for training such models. STYLEDGPT leverages pre-trained language models (specifically DialoGPT) to overcome this limitation.

Here's a breakdown of the key aspects:

1. The Problem:

Stylized Response Generation: The goal is to generate responses that are not only contextually relevant but also adhere to a specific style (e.g., formal, informal, humorous, scientific). Traditional approaches struggle due to a lack of parallel data.

2. The Approach (STYLEDGPT):

Pre-trained Language Model: STYLEDGPT utilizes DialoGPT, a large language model pre-trained on a massive conversational dataset. This provides a strong foundation for generating coherent and natural-sounding responses.

Style Corpus: A separate corpus of text representing the target style is used (e.g., scientific papers for a scientific style, Sherlock Holmes novels for a Holmesian style). This corpus doesn't need to be paired with the dialogue data.

Two-Level Loss Function: The core innovation is a two-level loss function designed to steer the response generation towards the target style:

Word-Level Loss (Lw): This loss uses KL divergence to measure the difference between the probability distribution of words in the generated response and the probability distribution of words in the style corpus. It encourages the model to use words more characteristic of the target style.

Sentence-Level Loss (Ls): This loss uses a style classifier to judge whether the generated sentence matches the target style. It maximizes the probability of the generated sentence being classified as belonging to the target style. A "Gumbel trick" is employed to overcome the non-differentiability of discrete tokens in backpropagation.

Relevance Loss (LNLL): A standard negative log-likelihood loss (NLL) from DialoGPT is included to ensure the generated response remains relevant to the conversation context.

Sample-and-Rank Strategy: Multiple candidate responses are generated, then ranked based on both relevance and style consistency, before selecting the final response.

3. Experiments and Evaluation:

Datasets: Two tasks were used: arXiv-style response generation (target style: scientific writing) and Holmes-style response generation (target style: Sherlock Holmes' narrative style). Both tasks used a large Reddit conversational dataset as the dialogue corpus.

Evaluation Metrics: Both automatic metrics (BLEU, ROUGE, Distinct-n, style intensity, lexical and syntactic consistency) and human evaluation (style consistency, fluency, relevance, informativeness) were employed.

Baselines: Several state-of-the-art models for stylized response generation were used as baselines for comparison.

4. Results:

STYLEDGPT significantly outperformed the baselines on both automatic and human evaluation metrics, demonstrating its effectiveness in generating contextually relevant and stylistically consistent responses. The ablation study showed that all three components of the loss function (word-level, sentence-level, and relevance) are essential for optimal performance.

5. Key Contributions:

Addressing stylized response generation using pre-trained language models, a novel approach.

The design of the two-level loss function to effectively guide style transfer while maintaining context relevance.

Empirical demonstration of the method's effectiveness on public datasets.

In essence, STYLEDGPT presents a robust and effective approach to stylized response generation by cleverly leveraging the capabilities of pre-trained language models and addressing the key challenge of limited parallel data. The two-level loss function is a particularly noteworthy contribution.