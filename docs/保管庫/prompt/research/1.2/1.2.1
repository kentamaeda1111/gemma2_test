
1. Customizing Large Language Model Generation Style using Parameter-Efficient Finetuning
Xinyue Liu, Harshita Diddee, Daphne Ippolito


Introduction
Language models play a crucial role in various writing applications, but the reliance on a few models may limit creativity and introduce biases. This paper aims to customize language model generations to individual writers' styles, particularly those with existing prior work. Key points in the introduction include:

The focus is on creating customized language models that mimic the writing style of specific users while understanding natural language instructions.
The authors propose a method to tailor language models to reflect individual writing styles, allowing users to choose if customization includes learning specific words.
While traditional methods involved full model fine-tuning or prompt engineering, the paper explores parameter-efficient fine-tuning techniques like LoRA for altering generation styles effectively.
The novel approach, StyleTunedLM, utilizes LoRA for efficient model fine-tuning and outperforms prompt engineering and few-shot learning methods in capturing the training data style effectively.
The authors address challenges such as maintaining instruction-following ability post-fine-tuning and learning style characteristics without content words, aiming to preserve the model's capabilities learned during training. They provide detailed finetuning instructions in the appendix.
Additional Information:

Baselines for comparison involve using excerpts from the target author or instructions to generate text in a specific style.
The paper explores techniques like masking out named entities during fine-tuning to control the model's learning of specific word classes.
A new method is introduced to merge LoRA modules to enhance instruction-following and style-following functionalities in a single model, addressing tasks requiring a broader understanding of user instructions.
Experimental Design
The experimental configuration features:

Evaluation of corpora from authors not present in the training data, using works of ten authors from Project Gutenberg.
Introduction of each author in detail in section A.1.
Random division of all available books from each author into training, validation, and test datasets.
Use of training and validation sets for model fine-tuning and selection, while the test set is reserved for generative tasks in evaluation.
Separate assignment of a book to only one dataset among training, validation, or test to avoid overlap.
Evaluation of in-style generation on a dataset consisting of 100 prompts, with 50 generated using GPT-4 and 50 randomly selected from the test set.
Extraction of five sentences per author, using the first 6-8 words of each sentence to create a prompt for evaluation.
Evaluating Generation Style
The evaluation of generation style in the paper is conducted across three dimensions: perplexity on withheld text, style-embedding alignment, and linguistic alignment.

Perplexity:

Measures the model's ability to generate text consistent with a target author.
Comparison of perplexity values of StyleTunedLLMs and the pre-trained Llama-2-7b model on validation sets for each author.
Style-embedding Alignment:

Text excerpts from 10 authors are embedded using a Sentence-Transformer.
Average embeddings of text excerpts for each author are computed and compared with model outputs to assess stylistic similarity.
Training of a style attribution model using pairwise loss and BERT classifier for author classification.
Linguistic Alignment:

Evaluation across lexical, syntactic, and surface linguistic levels.
Lexical assesses word choice, syntactic reviews sentence structure complexity, and surface examines statistical features.
Metrics include Mean Squared Error for lexical and surface analysis, and Jensen-Shannon Divergence for syntactic analysis.
The methods used provide a comprehensive validation of the model's style alignment, assessing unique stylistic features of authors' writing styles at different linguistic levels.

Experiment Results
The experiment results reveal key insights from the study:

Perplexity Analysis:

Finetuned models consistently demonstrate lower perplexity on validation sets compared to the base LLaMA-2-7b.
Across all authors, there is an average reduction of 7.0% in perplexity.
The most significant improvement (13.6%) is observed for the 18th-century writer SR, whose language differs substantially from modern English.
Style-Embedding Alignment:

Average cosine similarity between generated text and author embeddings is illustrated in a figure, showing that the proposed method achieves the highest average similarities across all authors.
In contrast, the comparison method displays inconsistencies and struggles with complex styles.
Detailed author-specific performance metrics and confusion matrices from the classifier are provided for further analysis.
Classifier Accuracy Comparison:

A table presents the classifier accuracy of each method, highlighting the superior capability of the proposed method in accurately capturing and distinguishing authors' writing styles.
Evaluation of Masking:

An evaluation is conducted to assess whether masking influences the model's style, providing insights into how this approach impacts the model's performance and style detection accuracy.
Conclusion
The authors introduce StyleTunedLM, a new method using parameter-efficient finetuning to customize large language models to individual users' stylistic preferences economically. Key points included:

StyleTunedLM effectively aligns model outputs with specific stylistic features of different authors.
It outperforms conventional techniques like few-shot learning and prompt engineering.
The study delves into the impact of training data size, content control via masking, and instruction-following capability using merged LoRA modules.
Future research should analyze writings from outside the pretraining corpus to assess generalizability.
Enhancing the fusion of style and instruction-following modules is crucial for refining methods to balance and specify each component's influence for improved performance and utility.
Limitations
The study mainly focuses on authors well-represented in the pretraining dataset, limiting the generalizability of the findings. Although the proposed method shows strong performance in learning authors' stylistic nuances, it may not work as well in low-resource settings with limited training data.

Key points included:

The model's effectiveness in capturing unique stylistic elements, especially in short essays, is uncertain in low-resource scenarios.
More research is needed to assess style alignment using data from diverse and underrepresented authors.
Ethics Statement
The researchers acknowledge that although their method effectively captures and reproduces stylistic details, it raises significant ethical considerations that could lead to privacy violations and unauthorized identity usage. Moreover, there is a concern that the technology could potentially be misapplied for creating deceptive content like scams or fake news, thereby propagating misinformation and societal harm. To mitigate these risks, strict protocols and verification procedures need to be established. The objective is to proactively address these ethical dilemmas to promote the responsible and positive utilization of their methodology.

Key points included:

The method's effectiveness in replicating stylistic nuances
Ethical concerns related to potential misuse for impersonation and privacy breaches
Risks of customization for harmful purposes like generating fake news and scams
Importance of implementing stringent guidelines and verification processes
Aim to ensure responsible and beneficial use of the technology.
C Finetuning Details
The researchers performed experiments using two A6000 GPUs, maintaining consistent hyperparameters for a fair comparison:

Learning Rate: 5 ร 10^-5
Number of Epochs: 3
Per-GPU Batch Size: 4
Input Max Token Length: 256
In the finetuning process, the model was tasked with generating continuations containing 256 tokens for each input prompt.

D.1 Qualitative Analysis
The researchers analyze sample generations in the styles of Virginia Woolf (VW) and P. G. Wodehouse (PGW) using the same input prompt randomly selected from the evaluation dataset. This analysis involves the following key points:

Authors' Writing Styles Comparison:

Virginia Woolf is known for rich, introspective, and figurative language.
P. G. Wodehouse's style is characterized by light-hearted, whimsical, and humorous tones.
Evaluation of Generated Texts:

StyleTunedLM aligns closely with both authors' styles compared to the baselines.
For Virginia Woolf, it effectively captures the reflective tone and vivid descriptions.
In the context of P. G. Wodehouse, it captures the playful and comedic tone, using formal yet amusing language.
Baseline Performance:

Fewshot fails to convey the depth of VW's metaphors and misses PGW's light-hearted tone.
Instruct provides rich descriptions for VW but can be overly complex and inconsistent in maintaining humor for PGW.
Overall Assessment:

StyleTunedLM offers more consistent and accurate representations of both authors' styles, making it superior to other methods.
The presented examples from the generated texts showcase the ability of StyleTunedLM to capture the essence of VW and PGW's unique writing styles effectively, demonstrating the superior performance of the proposed method in stylistic text generation tasks.

StyleTunedLM
The section contains a narrative excerpt that describes a personal transformation or realization experienced by the narrator. It conveys a sense of change and introspection regarding one's perception of the world and oneself. Here are the key points included:

The narrator expresses a shift in perspective without feeling unwell or unhappy, but rather experiencing a profound sense of strangeness in perceiving the world differently.
There is a comparison drawn to a past experience of seeing oneself from an external viewpoint, such as after a significant event like a marriage, where the perception of oneself is altered.
Despite the expression of love and admiration towards someone (possibly the reader), there is a subtle hint of a change in feelings or perceptions, indicating a nuanced emotional journey.
The narrative ends with a reflective statement acknowledging the present moment, suggesting a sense of acceptance or realization about the current circumstances.
This section of the text captures a moment of inner reflection and transformation, offering insights into how an individual's perspective can evolve and shift over time.

D.2 t-SNE Analysis
The authors conducted t-SNE visualizations using pairwise loss on both the training and test datasets, as well as the generated StyleTunedLM. Here's a breakdown of their findings:

Training Dataset Visualization:

Distinct clusters observed in the training dataset indicate that the style attribution model successfully learned to differentiate between the style embeddings of various authors.
Testing Dataset and Generation Visualization:

Clusters in the testing dataset and the generated data exhibit some overlap, but they consistently share similar clustering patterns.
This suggests that the model is capable of effectively assessing the similarity of embeddings, even across different datasets.
D.3 Style-embedding Alignment Analysis
The researchers used cosine similarity to analyze style-embedding alignment among different authors. Here's a breakdown of their findings:

Method Performance:

The method consistently achieves the highest scores for most authors.
It effectively captures nuanced features like Nathaniel Hawthorne's complex symbolism and intricate sentence structures.
Comparison with Other Approaches:

5shot and 10shot approaches show moderate performance in the analysis.
The instruct approach frequently underperforms, especially in learning complex stylistic elements.
D.4 Linguistic Alignment Analysis
The section presents results from a linguistic alignment analysis using MSE for lexical and surface measurements, and JSD for syntactic measurements, as outlined in ยง3.1. The analysis demonstrates that the StyleTunedLM model generally outperforms or competes well at three linguistic levels:

Lexical and Surface Alignment:

StyleTunedLM demonstrates superior or competitive performance in both lexical and surface aspects.
The model achieves notably low syntactic errors for Nathaniel Hawthorne (NH) with a 0.010 error and reduces surface errors to 6.690, showcasing its ability to capture the author's stylistic nuances effectively.
Syntactic Alignment:

The model also enhances syntactic alignment for Mark Twain (MT) with an error as low as 0.047, and significantly reduces surface errors to 1.849.
These results indicate that StyleTunedLM is effective in minimizing deviations from the target author's style, thereby improving the alignment with the writing style of different authors.
D.5 Masking during Training
The researchers presented experiment results on the effects of masking during training, showcasing how it impacts the generation of text by models.

Experimental Results: The results, detailed in a table, illustrate the influence of masking on all authors.
Examples Generated: Two sets of examples were provided, one with masking during training and the other without, demonstrating the difference in outputs.
Highlighting Names: In the examples, names directly following prompts were highlighted in bold, while names existing in the training data were marked in italics.
Effect of Masking: Without masking, the model tended to reproduce commonly seen names from the training data, like "Bingo" and "Aunt Agatha," in the generated text.
Benefits of Masking: On the other hand, when masking was applied, the model avoided fixating on specific names from the training data, resulting in more diverse outputs.
Influence of Temperature: Variances in the content generated with and without masking were partly due to a high temperature setting (0.9) during text generation, enhancing creativity and reducing predictability.
Contextual Continuation: When a different name was predicted due to masking, the model continued the narrative based on this new context, leading to distinct storylines and divergences in the generated text.
D.6 LoRA Module Merging for Enabling Instruction-Following Ability
The section presents the performance of models with different merging ratios across multiple benchmarks. The findings indicate that:

Scores exhibit minor fluctuations but generally remain stable regardless of the merging ratios used.
This stability suggests that LoRA module merging is feasible without causing detrimental effects on performance.
The section also includes snippets of a conversation, which seem unrelated to the main discussion of LoRA module merging. The snippets involve dialogues between characters like Aunt Agatha, Jeeves, and others, creating a contrast with the technical content of the section.

Supplementary Materials Availability Statement
The authors will provide the following supplementary materials for download:

Author datasets containing train, validation, and test splits of books chunked into 256 tokens.
Code necessary to re-run the LoRA finetuning for each author.
Finetuned LoRA weight modules for all experiments.
Finetuned BERT and Sentence-BERT models used for evaluation.
Instructions for loading all checkpoints into HuggingFace for inference.