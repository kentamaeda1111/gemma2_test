


5. CoRA: Optimizing Low-Rank Adaptation with Common Subspace of Large Language Models
Xiaojun Xiao, Sen Shen, Qiming Bao, Hongfei Rong, Kairui Liu, Zhongsheng Wang, Jiamou Liu 






Introduction
The researchers aim to optimize large language models (LLMs) for downstream tasks by reducing computational expenses through Parameter-Efficient Fine-Tuning (PEFT) methods. Notable among these methods is Adapter Tuning, which involves techniques like Low-Rank Adaptation (LoRA). These methods introduce small, trainable matrices to adjust pre-trained model weight matrices, allowing quick adaptation to new tasks while maintaining model stability.

PEFT techniques, like Adapter Tuning, significantly reduce the number of parameters needing training, enabling effective adaptation without the computational cost of retraining the entire model.
Methods such as DyLoRA and QA-LoRA have been developed to enhance the efficiency of LoRA, focusing on learning efficiency and hardware adaptability.
The objective is to reduce computational resource usage further and develop a universal method for broader application in future model training.
By utilizing Singular Value Decomposition (SVD) for dimensionality reduction, a common basis matrix replaces the original B matrices in LoRA, achieving parameter reduction and training optimization.
The researchers propose two scenarios: replacing and freezing the common basis matrix to reduce parameter count by 50% and using an alternative matrix for optimized initialization to achieve favorable training outcomes within the same computational budget.
Contributions:

Evaluating the common basis matrix across different model scales suggests the potential existence of a universal subspace within models.
Freezing the common basis matrix after replacement sometimes outperforms traditional LoRA methods with only half the parameters, conserving computational resources.
Using the common basis matrix as an enhanced initial state for the B matrix of LoRA leads to up to a 4% performance improvement with better fluency, relevance, and accuracy.
Parameter-Efficient Fine-Tuning
The exponential growth in the number of parameters in Transformer-based pre-trained language models has significantly improved NLP tasks but poses challenges due to computational demands. Parameter-Efficient Fine-Tuning (PEFT) methods have emerged as crucial solutions to address these challenges, aiming to reduce resource requirements while maintaining model performance.

Key points included:

Large language models benefit from fine-tuning for specific downstream datasets, enhancing task-specific processing capabilities.
PEFT methods like Prompt Tuning and Adapter Tuning streamline fine-tuning by employing task-specific prompts and minimal specialized parameters, respectively, to optimize performance.
Prefix Tuning enhances model efficiency by optimizing task-specific vectors as input, ensuring effective token processing.
These techniques enable efficient utilization of advanced NLP large models, particularly in resource-constrained environments.
Despite advancements, fine-tuning extensive models still demands significant resources, which can lead to prolonged computation times in low-resource settings.
Low-Rank Adaption of LLMs
The Low-Rank Adaptation (LoRA) technique enhances pre-trained deep learning models by incorporating low-rank up-projection and down-projection matrices, targeting a minimal subset of parameters for adjustment while maintaining the model's integrity and prioritizing efficiency and flexibility. Key points included:

LoRA facilitates model adaptation to new tasks or datasets efficiently, reducing training and inference times without compromising performance.
Challenges with LoRA include balancing model accuracy with limited computational resources, especially in edge computing devices.
Variants of LoRA like QLoRA employ 4-bit quantized backpropagation and Low-Rank Adapters to train large models efficiently, introducing technological advancements like the 4-bit NormalFloat data type and Paged Optimizers for optimized memory utilization.
QA-LoRA enhances memory and computational efficiency during fine-tuning using quantization-aware techniques, providing significant improvements in computational efficiency, especially in resource-limited environments.
DyLoRA offers flexibility in training across a range of ranks, accelerating the training process significantly and maintaining high performance across various pre-trained models like RoBERTa and GPT.
The authors' novel approach restructures LoRA by drastically reducing the number of trainable parameters, ensuring model functionality and efficiency even in strict resource-limited environments, offering a solution to traditional fine-tuning methods' high resource demands.
Method
The experiment demonstrated that replacing and freezing the B matrix in LoRA with a common basis matrix led to performance comparable to fine-tuning the original LoRA parameters. This approach surpassed the efficiency of halving the LoRA parameters. Key points included:

Illustrating that using the common basis matrix in place of the B matrix in LoRA could achieve performance comparable to fine-tuning the original parameters.
Demonstrating that freezing the common basis matrix in LoRA could outperform the strategy of halving the LoRA parameters.
Showing that initializing the B matrix with the extracted common basis matrix resulted in training effectiveness exceeding the outcomes of fine-tuning the original LoRA parameters.
Workflow of The Entire Framework
The workflow of the entire framework is detailed, starting with the experiment based on Llama2-13B and the attention head mechanism suitable for the related experiments. Here are the key points included:

After fine-tuning downstream tasks, the Q, K, and V matrices from different models are combined into a matrix W0 to adjust the pre-trained Transformer model effectively.
LoRA reduces the parameters of critical weight matrices Q, K, and V into two lower-rank matrices, A and B, to enhance computational efficiency.
By focusing on adjusting A and B matrices instead of the original Q, K, and V matrices during finetuning, LoRA maintains model expressiveness while improving parameter adjustment efficiency.
Matrix B is replaced with zeros at the start of training to keep the initial impact neutral, ensuring stable pre-trained weights for smoother fine-tuning transitions.
A matrix is randomly initialized to allow parameter adjustments during training to adapt to new tasks, while B is replaced due to its impact on downstream tasks.
Matrix B is derived from an analysis of various downstream tasks, integrated into a single matrix W0, adapted through matrix dimensionality reduction using SVD to enhance parameter reduction efficiency.
The workflow involves obtaining U, Î£, and V matrices through W0 decomposition, reducing dimensionality for common space representation.
Two approaches are adopted after replacing matrix B: freezing matrix B, assumed well-trained for downstream tasks, and involving matrix A to capture new task information, and involving both A and B in parameter iteration to improve overall model performance.
Methods For Matrix Compression Extraction
Principal Component Analysis (PCA) and Singular Value Decomposition (SVD) are crucial for feature extraction and data dimensionality reduction. The methodologies' analysis sheds light on their strengths and performance boundaries:

PCA identifies principal components by eigendecomposition of the covariance matrix, simplifying data complexity while preserving information.
The matrix P comprises eigenvectors, with columns representing eigenvectors, selecting the first r eigenvectors to simplify data complexity effectively.
SVD, a versatile decomposition method, compresses data by retaining a limited number of the largest singular values and vectors for dimensionality reduction:

Formula (3)(4)(5) represent the mathematical expressions of SVD in the matrix dimension reduction task.
Experiments confirmed the effectiveness of SVD, applied in the study for dimensionality reduction.
Comparing PCA and SVD for dimensionality reduction in the Llama2 model's Transformer part:

PCA requires around 3000 principal components for a 100% explained variance rate, while SVD achieves this with just 130 singular values.
Based on results, SVD was chosen for CoRA lightweight fine-tuning due to its efficiency in reducing components and ensuring a complete variance explanation:

SVD significantly reduces the required components, reduces computational costs, and accelerates model training and prediction processes, making it advantageous for large or complex datasets.
Selection of Replacement Matrices
The researchers randomly selected 1, 2, 3, 4, or all 5 models from a pool of 5 to determine an exceptional matrix (B matrix) for their experiments. The selection aimed for fairness and randomness to achieve unbiased results. Models from different downstream tasks were chosen to explore the potential of a common basis matrix extraction.

Key points included:

Stability in performance across datasets (yahma and text-to-code) was observed regardless of B matrix updates.
Large models seem to retain a common knowledge space (common basis matrix), contributing to stable results irrespective of the number of matrices extracted.
Updating the B matrix appeared to improve results compared to training the A matrix alone, possibly due to updating a larger parameter set.
Further experiments are needed to confirm if better performance can be achieved post a suitable initialization of the B matrix.
Any extracted matrix could be used for subsequent experiments due to the presence of a common basis matrix, although some matrices may be extracted less optimally due to unknown factors.
For result accuracy, the matrix from the 5 models with the best and most stable performance was chosen for later experiments.
Experimental Settings
The experimental configuration features:

Frozen Large Language Model: The Llama2 13Bhf was chosen as the primary model for the experiments.

Datasets:

Utilized two datasets:
Yahma from Hugging Face
Custom Text-to-Code Dataset
These datasets were used to train downstream tasks and improve the model's ability to follow instructions.
Collation Datasets: The incoming GPT-4 data was collated to enhance the model's performance.

Evaluation Metrics: Robust validation was done using:

ROUGE-1, ROUGE-2, and ROUGE-L for text similarity assessment at different levels.
Meteor Score for nuanced translation quality evaluation.
SacreBLEU Score for standardized machine translation evaluation.
BERT Score for semantic similarity evaluation by comparing word vector cosine similarity between candidate and reference texts.
LoRA Setting:

PEFT Strategy: LoRA from the PEFT library was a foundational strategy.
Comparative Experiments:
Conducted experiments with LoRA at r = 48.
Trained a baseline LoRA at r = 24 for comparison with the freezing strategy.
GPU
The training process for the proposed method utilized NVIDIAÂ® A100 GPUs with 80GB of memory. The GPU setup included:

Model Selection: Five models from Hugging Face were randomly chosen to extract the common basis matrix.
Control Experiments: To maintain experimental fairness, control experiments involved randomly selecting and extracting common matrices from one, two, three, four, or all five models for comparative analysis.
These GPU specifications ensured efficient training and accurate comparison across different model extractions.

Result
The comparison between LoRA fine-tuning and different CoRA fine-tuning versions at the same rank showed that the FB models slightly outperformed LoRA on two datasets. Some key points from the results include:

In the yahma dataset, three models, including TB_5, demonstrated improvements across all evaluation metrics compared to traditional FineTuning.
The TB_5 model achieved high scores in ROUGE-1 (0.421), ROUGE-2 (0.230), and ROUGE-L (0.396), with additional high METEOR (0.320) and SacreBLEU (17.296) scores and a BERT score of 0.897, outperforming other methods.
FB_5 and TB_5 models showed significant advantages in evaluating the code-to-text dataset.
Utilizing a common basis matrix as an optimized initial state for the B matrix in LoRA proved effective, maintaining performance with reduced computational resources and achieving favorable training outcomes within the same budget.
Training with the common basis matrix along with the A matrix helped retain original model knowledge and adapt to new tasks efficiently.
Across different parameter settings, the FB model demonstrated better linguistic coherence and quality control, especially in ROUGE-L and METEOR scores.
The FB strategy showed better stability and quality alignment in SacreBLEU scores, indicating its effectiveness in ensuring translation quality.
The TB-enhanced strategy outperformed traditional LoRA in various parameter settings, demonstrating effective understanding and reproducing details.
At higher parameter settings, the TB strategy surpassed traditional LoRA in METEOR and SacreBLEU scores, highlighting the FB and TB strategies' potential in enhancing model performance for complex linguistic tasks.
Ablation Study
The researchers conducted ablation studies to assess the impact of different factors on the experimental results and to identify unstable variables. This process ensures the fairness and accuracy of the findings, as portrayed in the Figure: Ablation Study.

Replacing the entire B matrix with zeros and freezing it serves as a baseline to evaluate the necessity of the B matrix in the model's performance.
Removing the influence of the B matrix significantly decreased the model's performance across evaluation metrics, highlighting the crucial role of this matrix in maintaining baseline model performance.
Substituting the entire B matrix with ones and freezing it examines the effect of uniform non-zero values in the matrix, showing that constant values do not enhance model performance effectively.
Randomly initializing and freezing the B matrix aimed to verify if performance enhancements observed with the extracted B matrix could be replicated with random initialization.
Models with randomly initialized B matrices exhibited superior performance compared to those initialized with zeros or ones, particularly in the yahma_48_ran and code_48_ran configurations.
The findings underscore the significant influence of the B matrix in the model and emphasize the importance of appropriately initializing this matrix to boost overall performance. These experiments also highlight that the selection and optimization of the B matrix play a crucial role in achieving optimal model performance.

Analysis
The analysis of the Generation model's performance entails evaluating its outputs based on fluency, relevance, and accuracy, using GPT-4 for scoring alongside the original dataset results. The evaluation criteria aim to ensure semantic fluency, alignment with human interpretative expectations, and relevance to intended outcomes. Key points included in the analysis are:

The assessment focuses on Fluency, Relevance, and Accuracy of the model's outputs.
The evaluation criteria aim to determine if the model's outputs meet expected standards.
Objective evaluation using GPT-4 indicates that all four models achieved the expected scores, fulfilling the experiment's objectives.
Regarding the assessment of the generated code's readability and correctness, the analysis reveals the following:

Manual review and execution of the code from a text-to-code dataset were conducted to assess readability and correctness.
The generated code demonstrates high clarity, adherence to naming conventions, and extensive commenting for easy comprehension.
The accuracy score indicates that the code fulfills its intended functions and closely matches specification requirements during execution.
The stability across various test scenarios, low error rate, and reliable performance indicate that the generated code can effectively complete its designated tasks.
Conclusion
The authors introduce two novel optimization approaches for LoRA strategies, showcasing how computational resources can be saved without compromising the performance of extensive language models. Specifically:

Their method substitutes matrix B to decrease parameters during model training, maintaining fine-tuning effectiveness and sometimes outperforming conventional LoRA parameter setups.
Utilizing a common basis matrix as an optimized initial state for matrix B training has displayed favorable outcomes compared to traditional LoRA methods.
These successes underscore the significance of integrating computational efficiency into model design and optimization, offering practical techniques for training large models in resource-limited settings.
Future research could explore more optimal structures for leveraging the common basis matrix, potentially further reducing parameters while enhancing model transfer and downstream task training capabilities.