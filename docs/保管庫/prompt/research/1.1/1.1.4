4. Data Selection via Optimal Control for Language Models
Yuxian Gu, Li Dong, Hongning Wang, Yaru Hao, Qingxiu Dong, Furu Wei, Minlie Huang

2.0x
The researchers choose pre-training datasets sourced from CommonCrawl for training Language Models (LMs). They conduct pre-training on these selected datasets and compare the Performance Differential Scaling (PDS) with the Redpajama data cleaning pipeline. The comparison involves evaluating the effectiveness of PDS in enhancing pre-training outcomes when compared to the Redpajama pipeline.

The computation curves in this section are derived from training a massive 1.7 billion parameter LM. Key points to note about this training process are:

The LM is large-scale, containing 1.7 billion parameters.
Computation curves are generated to track the progress and performance of the training process.
These curves provide insights into how the model learns over time and how its performance metrics evolve during training.
The training process likely involves significant computational resources due to the scale of the LM and the complexity of the pre-training task.
Introduction
The paper highlights the growing significance of data selection for pre-training language models (LMs) to enhance model learning and downstream performance while addressing increasing data demands and computational costs. Unlike previous methods dependent on manual heuristics, the authors introduce a novel approach connecting data selection with Optimal Control theory. This approach allows in-depth analysis of how data selection impacts LM pre-training and downstream performance. Key points included:

Pre-training data selection optimization is achieved through Pontryagin's Maximum Principle (PMP) which offers a theory-driven alternative to ad-hoc practices.
The authors propose a framework, PMP-based Data Selection (PDS), that efficiently selects high-quality pre-training data by solving PMP conditions on a proxy dataset and utilizing a data scorer to predict quality scores.
PDS leverages the dynamic nature of LM pre-training and operates offline, avoiding additional training-time computation overhead while preserving optimized pre-training pipelines.
Experimental results demonstrate significant speed-up in pre-training, consistent improvement in downstream tasks across various LM sizes, and enhanced data utilization, reducing data demand by 1.8 times in a data-constrained setting.
The study includes analysis and ablation studies to elucidate the key factors of PDS for the benefit of future research in data selection methodologies.
Problem Formulation
The research focuses on a Language Model (LM) parameterized with θ ∈ R^N, pretrained from scratch on a dataset D = {x_n}^|D|_(n=1), over a total of T steps. The goal is to select a subset D' from D to enhance downstream performance by reducing the downstream loss J(θ) through data selection.

The pre-training process links J(θ) to D', characterized by a data quality score vector γ, where higher scores indicate instances more beneficial for reducing J(θ), prompting the LM to learn more from them. The main points include:

The pre-training loss is a weighted sum of per-instance loss defined by γ.
The objective is to identify γ that minimizes J(θ) and then select instances with the highest scores in γ.
The LM is trained using Gradient Descent (GD) or the Adam optimizer.
The optimization problem involves minimizing the Area Under the Curve (AUC) of J(θ_t) over the pre-training process. Key aspects are:

Lower AUC signifies faster loss convergence and improved downstream performance.
AUC captures the overall LM training dynamics compared to evaluating J(θ_t) at individual time steps.
Minimizing the AUC enhances the LM's Scaling Laws constants, leading to significant learning improvements.
Data Selection as Optimal Control
The authors equate the optimization problem to discrete-time optimal control, where the cost function J(•) resembles a control problem with model parameters evolving as state variables and data quality scores as the adjustable control variables. This unique view allows framing data selection as an optimization challenge from a more theoretical angle.

Theoretical Optimal Solution for Data Selection
Optimal control problems utilize Pontryagin's Maximum Principle (PMP), setting necessary conditions for optimal control and state variables.
However, the constraint of time-invariance on control variables in the data selection scenario complicates optimization, shrinking the feasible region and posing a challenge.
The PMP conditions for data selection under this constraint are presented in Theorem 2.1.
Implementation and Insights
Theorems and conditions, derived using PMP and Lagrange multiplier method, illustrate a target vector aligning with optimal data selection.
Illustrations and equations demonstrate how gradient directions and data quality scores are interrelated, showcasing the selection process.
The target vector λ * t guides towards high-quality data points for optimal training dynamics.
The equation system involving model parameters, target vectors, and optimal control variables forms the basis of the data selection framework PDS.
Solving the system of equations simultaneously can unveil the optimal data quality scores critical for effective data selection strategies.
PDS: Data Selection based on PMP
The data selection process based on Positive-Unlabeled Pre-training (PMP) involves three main components:

Uniform Sampling: Initially, a proxy dataset (Dprx) is sampled uniformly from the pre-training corpus (D).

Data Quality Score Computation: Algorithm 1, guided by the PMP conditions, calculates quality scores for each instance in Dprx.

A small Language Model (LM) is fine-tuned to predict quality scores based on instances in Dprx.
The learned data scorer then evaluates quality scores for the entire pre-training corpus D.
High-Quality Corpus Formation: Instances with high quality scores are selected to create a high-quality corpus (D'), which is utilized for pre-training Language Models of any size.

Data Quality Scores from PMP
Algorithm 1 is utilized to solve the PMP conditions for data selection iteratively, ultimately providing the data quality scores represented by γ *.

The algorithm addresses a bi-level optimization problem:
An outer loop computes γ *.
Two inner loops compute θ * t and λ * t based on the current γ *.
Initialization and updates:
γ * is uniformly initialized and then updated for T o epochs based on θ * t and λ * t from the outer iterations.
Inner loop details:
Forward inner loop computes θ * t for t = 0 to T − 1.
Reverse inner loop computes λ * t for t = T − 1 to 0 based on θ * t.
Update of γ *:
Updated based on θ * t and λ * t, avoiding a "hard" update to prevent instability.
Efficient Implementation:
Due to computational intensity, the outer loop is limited to one epoch, employing stochastic gradient descent (SGD) in inner loops.
Training a proxy LM with N prx parameters and T prx steps is used to reduce computation.
To maintain training dynamics, Algorithm 1 is run multiple times using checkpoints from different pre-training stages of the proxy LM.
The approach is further optimized by incorporating practical techniques to reduce computational overhead, enhancing the efficiency of the data quality scoring process.

Data Scorer
The data scorer in this study involves fine-tuning a small Language Model (LM) to match data quality scores on a specific dataset (D prx). Here's how it works:

Each instance in dataset D prx is encoded by averaging the output hidden states of the data scorer.
The representation of each instance then goes through a linear head, which outputs a scalar value.
The linear head and the LM are trained simultaneously to align with the data quality scores on D prx using Mean Square Error loss.
In this setup, x prx n represents the nth instance in D prx, ϕ denotes the data scorer parameters, and h(•, •) ∈ R d is the average output hidden states of an LM with d as the hidden state size.
The parameters of the linear head are denoted as w ∈ R d and b ∈ R.
Once the fine-tuning is complete, the data scorer is used to predict the data quality scores for instances in D, where the quality score for a specific instance x n is determined by γ.
Data Selection
The authors estimate the value of instances in D using output scores from a data scorer to choose the final pre-training corpus D' for a large language model (LM). To ensure data diversity, they employ the Gumbel-Top-K method to add randomness to the selection process, where u1, u2,..., u|D| are sampled independently from Uniform(0, 1), and τ is a hyper-parameter that controls the Gumbel noise strength. The selection of data is influenced by a data selection ratio r, with K = r|D| in their experimental setup.

Key points:

Utilization of output scores from a data scorer to estimate instance values in D
Adoption of the Gumbel-Top-K technique to introduce randomness in data selection
Independent sampling of u values from a uniform distribution and the use of hyper-parameter τ to control Gumbel noise strength
Management of selected data size through a data selection ratio r, where K = r|D| in experiments
Discussion
The researchers introduce the Pre-training Data Selection (PDS) method, which differs from traditional offline data selection techniques by incorporating long-range training dynamics for better instance selection in Language Model (LM) pre-training. Key points include:

Incorporation of Long-Range Training Dynamics: PDS uses a "target vector" to enhance data selection quality, acknowledging the dynamic nature of LM pre-training.

Transferability of Data Quality Information: The study demonstrates that data quality information is transferable across different LM sizes, enabling efficient selection processes.

Efficiency and Flexibility of PDS: PDS stands out for its offline corpus selection, which can be done once and applied to pre-train multiple LMs of varying sizes without added computational burden. Additional highlights include:

Negligible computational demands in the proxy environment compared to large-scale pre-training.
Easy integration into optimized pre-training pipelines by replacing data sources due to its offline nature.
Experimental Setup
The experimental configuration features:

Data: Utilization of the CommonCrawl split from Redpajama as D to prevent domain weight influence. Pre-processing involves merging multiple documents into a single pre-training instance containing 1,024 tokens.

Downstream Loss Computation: The downstream loss J(θ) is computed on the training split of LIMA, a dataset with 1,030 varied instruction-response pairs covering diverse downstream scenarios. Evaluation extends to various datasets beyond LIMA to prevent over-fitting.

Model: Adoption of the same model architecture as Mistral, with pre-training of Language Models (LMs) having 160M, 470M, 1B, and 1.7B parameters. Model configurations are elaborated in a detailed table.

PDS
The researchers compute data quality scores using a 160M proxy LM called D prx, which comprises 160K instances sampled from D uniformly. Here's a breakdown of their process:

The proxy LM is pre-trained on D for 50K steps.
They select checkpoints at various step intervals: [10K, 20K, 30K, 40K, 50K].
Experimental Results:
The results on downstream evaluation datasets in OLMo are presented in a table.
Accuracy scores and average scores across datasets are reported.
The best scores for each model size are highlighted in bold.
PDS demonstrates superior performance compared to the baseline models in most scenarios.
The proxy LM undergoes inner loops with specific parameters:

η = 0.008 over T prx = 100 steps.
A mini-batch size of 256 is used.
γ * is updated for one outer loop epoch with α = 1.
Fine-tuning and Hyperparameters:
They fine-tune a 125M Fairseq-Dense model for the data scorer along with a linear head, using a specific objective function.
Details about the training process for the data scorer can be referred to in Appendix E.2.
For Data Selection, the researchers set certain hyperparameters:

δ = 0.1, r = 0.4.
Further exploration of hyperparameters is detailed in Appendix G.3.
Pre-Training
The pre-training process involves specific steps and evaluations, including:

Pre-training all language models (LMs) for 100k steps with a batch size of 512 and a max input length of 1,024, totaling around 50 billion tokens.
Selecting a 50 billion token dataset from a corpus with 125 billion tokens to explore how different data selection methods enhance LM learning, especially with a large dataset.
Analyzing the effectiveness of Progressive Data Selection (PDS) when the dataset size (D) is limited, discussed in Section 3.3.
Evaluating LMs' performance, including their 0-shot accuracy on downstream test datasets in OLMo and their 0/5-shot accuracy on MMLU.
Assessing the LM's language modeling loss on a subset of DCLM to ensure that models trained on the selected dataset maintain diversity and cover long-tail knowledge.
Comparing PDS with conventional pre-training and three offline data selection methods:
Conventional pre-training involves training an LM on 50 billion tokens uniformly sampled from D.
RHO-Loss method selects data with high reducible losses.
DSIR method selects data based on high n-gram overlap with instances in LIMA.
IF-Score method selects data with high influence scores based on a specific equation.
Main Results
The authors evaluated pre-trained Language Models (LMs) using the OLMo evaluation datasets and Minimum Message Length Unification (MMLU). Key points include:

Test losses on the DCLM subset showed better performance for PDS-trained LMs with 160M, 470M, 1B, and 1.7B parameters compared to conventionally pre-trained LMs.
Evaluation results illustrated consistent performance improvements with PDS across different model sizes.
Extrapolation using the Scaling Law demonstrated the persistent performance gains with PDS, even with recent large LMs like GPT-3 and Llama family.
The DCLM corpus, curated using human heuristics, was found to be diverse and comprehensive, serving as a valuable resource for LM pre-training.
Algorithm 1 provided a structured approach to curate pre-training corpus as an alternative to the complex human-based pipeline.
With the 1.7B model, PDS achieved a 2.0 times acceleration in training-time computation compared to traditional pre-training, as depicted in Figure (a).
Comparable acceleration trends were observed across various model sizes and in DCLM losses, as shown in the figures.
Analysis: Data-Constrained Setting
When the original pre-training data is limited, the LM needs to be trained on the PDS-selected data for multiple epochs to ensure the corpus contains 50B tokens.

Figure shows that selecting 1/4 of the data with PDS and training for 4 epochs achieves the lowest test losses, indicating improved data utilization.
Extrapolating the loss curve reveals that PDS reduces the pre-training data requirement by 1.8 times compared to conventional pre-training.
Efficient Implementation for Solving Data Quality Scores
Efficient implementations are introduced in Section 2.3.1 to solve data quality scores by running Algorithm 1 for a single outer epoch, using a small proxy LM trained for a limited number of steps.

In a simulated setting in Figure, the efficient implementation significantly reduces computational overhead while maintaining performance compared to the exact solution as per Algorithm 1.
Complexity Analysis
The computational complexity comparison between PDS and pre-training in Table indicates that the overhead of running PDS to select data is only about 1/9 of pre-training a 1.7B model.
PDS being an offline method allows the selected corpus to pre-train multiple LMs without extra computational costs.
Its offline nature makes PDS seamlessly integrate into optimized pre-training pipelines, requiring only a data source replacement without changing the pre-training process.
Ablation Studies on PMP-Solver
The researchers compared PDS with other data selection methods, highlighting the importance of incorporating early-stage learning dynamics from language models (LMs) for better performance:

Early training dynamics in LMs are more valuable than single-step gradients, aiding in selecting better local optimums.
PDS with early-stage training data outperformed other strategies like IF-Score, indicating the significance of long-range training dynamics.
The study delved into the impact of proxy models and proxy datasets on data scorer and LM performance:

Increasing the size of the proxy LM enhanced LM performance but decreased the data scorer's performance.
Smaller proxy LMs focus on shallow patterns for data quality estimation, while larger proxy LMs encode more complex learning information, posing challenges for the data scorer.
Related Work
In the realm of data selection for Language Models (LM), various approaches have been explored to enhance LM learning speed or refine downstream performance:

Some methods involve curating data before LM training, categorized as offline methods, including domain-mixing, data pruning, sample-wise data selection, and data programming.
Other techniques dynamically select data during LM training by adjusting domain-mixing weights or employing more refined reweighting strategies.
This study delves into data selection from a fundamental perspective, theoretically deriving optimal selection criteria and devising scalable algorithms to put it into practice.
In the domain of Deep Learning, the application of optimal control principles has proven effective:

In this context, the forward pass of a multi-layer neural network is viewed as a control process, with hidden vectors as state parameters and model parameters as control variables optimized during training.
Several works have leveraged Pontryagin's Maximum Principle to enhance optimization algorithms in deep learning:

These efforts aim to achieve better convergence rates and broader application scenarios, as well as establish theoretical foundations for neural networks to aid in interpretation.
In a departure from existing research, this study introduces a novel approach to Optimal Control by framing the model's learning process as the control process, leveraging an "orthogonal" perspective.

Conclusion
The researchers in this study delve into the selection of pre-training data for Language Models (LMs) from both theoretical and practical standpoints. Here's what they found:

They approach data selection as an Optimal Control problem, using Pontryagin's Maximum Principle (PMP) to establish the theoretical framework for optimal data selection.

Building on the theoretical foundations, the researchers introduce PDS, a practical framework that effectively implements the PMP conditions in real-world scenarios by leveraging long-range LM training dynamics.

Through extensive experiments, they demonstrate that PDS identifies high-quality pre-training corpora that not only expedite LM learning but also enhance LM performance across different model sizes and downstream tasks.

PDS is also shown to enhance data utilization in scenarios with limited data availability, alleviating the issue of pre-training data exhaustion.

A Connection Between AUC and Scaling Law Constants
The relationship between the area under the loss curve (AUC) and scaling law constants in Language Models (LMs) is explored in this section:

The test losses of LMs follow a power-law in relation to training steps after a warmup phase, characterized by scaling law constants C and c, along with irreducible loss L_irre and the warmup end steps T_0.
The irreducible loss L_irre, unaffected by pre-training data selection strategies, emphasizes the importance of reducible loss, influenced by data quality scores γ.
Analyzing the AUC of the reducible loss for large T reveals crucial insights:
For c(γ) < 1, minimizing AUC leads to a decrease in B(γ) and an increase in β(γ), enhancing LM scaling properties.
Conversely, for c(γ) > 1, minimizing AUC results in decreasing C(γ) and increasing c(γ), further optimizing LM scaling.
B Proof of Theorem 2.1
To prove Theorem 2.1, the researchers start by describing the standard discrete-time Pontryagin's Maximum Principle (PMP) in Optimal Control for time-variant control variables, presenting Theorem B.1 (PMP) within the context of an optimization problem in a discrete dynamical system. Here are the key points outlined in the proof of Theorem 2.1:

The optimization problem involves state variable θ t and control variable γ t that are continuous in R N ×D.
The solution to the problem is denoted by γ * t and θ * t for the control and state variables, respectively.
The Hamilton function plays a crucial role in this context.
A critical challenge in proving Theorem 2.1 is ensuring the control variables remain invariant with respect to the training steps, introducing complexity to the optimization problem.
The requirement of invariant data weights leads to a system of T − 1 equations, ultimately equivalent to an optimization problem.
The method of Lagrange multipliers is employed to solve the optimization problem, incorporating Lagrange multipliers (µ n,t ) for minimization considerations.
By applying Theorem B.1 to specific equations and substitutions, the researchers demonstrate the validity of Eq. ( ) and Eq. ( ) in Theorem 2.1.
The time-invariant constraint further refines the solution, leading to a complete equation system with T equations and T unknown variables, ultimately proving Theorem 2.1 by combining key equations and derivations.
C Derivation for Adam
The section presents the formulation and derivation of the Adam optimization algorithm. Here are the key points included:

The parameter update rules for Adam are defined for 0 ≤ t ≤ T − 1, with hyper-parameters β1, β2, ε, η, and the gradient function g_t = ∇L(θ_t, γ_t, D).
Initialization: Adam sets m_0 = 0 and v_0 = 0.
Formulating LM training with Adam as an Optimal Control problem involves treating a vector in R^(3N) as the state variable and defining a state-transition function F from Θ_t to Θ_t+1.
Hamiltonian Function: Similar to Gradient Descent (GD), the Hamiltonian function is defined, leading to the application of Pontryagin's Maximum Principle (PMP).
Theorem: The section introduces Theorem C.1 (PMP Data Selection for Adam) which provides guidance on data selection and optimal solutions in the context of Adam optimization.
Co-state vector: It is mentioned that there exists a co-state vector linked to the state variable corresponding to the optimal solution.
State Transition Control: It is noted that a specific equation controls the state transition, simplifying further derivations by making a reasonable assumption.
D Algorithm 1 as Proximal Gradient Descent
The section provides an alternative perspective on Algorithm 1 by employing the Proximal Gradient Descent method for optimization. Here are the key points included:

The optimization of Equation () is carried out with specific rules related to the cost function denoted by A(γ) in Equation ().
The co-state vector λ t+1 in Algorithm is highlighted to enhance robustness during initialization.
Fitted parameters such as E ′ (N ), B 0 (N ), and β 0 (N ) are used to establish a model size scaling curve.
Constants from Equation () and average constants from Equation () are utilized for the initialization of the LBFGS algorithm.
Parameter initializations (a 0 , b 0 , α 0 , β 0 , e 0 ) for the LFBGS algorithm optimization of Equation () are determined.
Specific settings like δ = 1 × 10 −3 and a learning rate of 0.05 are chosen for running the LFBGS algorithm, allowing the computation of predicted losses.
In the Data-Constrained Setting (Section 3.3), computation of expected token demand for conventional pre-training to match PDS performance is detailed. The obtained token demand (D 4) indicates that the LM can be trained for 4 epochs based on suggested methods.
G.3 Ablation Studies
In this section, the authors focus on ablation studies related to data selection ratio and its impact on the performance of PDS when working with a large original training corpus. Here are key points from this section:

The researchers investigate how varying the data selection ratio influences PDS performance.

Lower data selection ratios are found to enhance the final model performance.

Maintaining a lower data selection ratio requires a larger original corpus to ensure an adequate number of training tokens.

For their main experiments, the authors opt to set the data selection ratio (α) at 0.4 to strike a balance between model effectiveness and data requirements.

Gumbel Noise in Data Selection
The researchers investigate the impact of Gumbel-Top-K with different strengths (τ values) on data selection, as shown in Figure. Here are the key findings:

Effect of τ value:
τ = 0.1 leads to optimal performance by enhancing the diversity of the pre-training corpus.
Excessively large τ values result in Performance Degradation Syndrome (PDS), resembling random selection, which reduces performance.