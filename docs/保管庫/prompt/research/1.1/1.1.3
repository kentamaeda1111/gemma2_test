3. Gradual Learning: Optimizing Fine-Tuning with Partially Mastered Knowledge in Large Language Models
Bozhou Li, Hao Liang, Yanan Li, Fangcheng Fu, Hongzhi Yin, Conghui He, Wentao Zhang 

Introduction
Large language models (LLMs) have shown remarkable performance due to pretraining on extensive text data, enabling them to encode vast knowledge. However, challenges emerge during fine-tuning and inference, leading to overfitting and knowledge gaps. The constraints on LLM design include:

Constraint 1: Knowledge in fine-tuning data should align with pretrained knowledge to reduce overfitting.
Constraint 2: External knowledge provided during inference should relate to learned knowledge.
To address these limitations, the researchers propose a two-stage fine-tuning method leveraging the model's reasoning ability and interconnected knowledge. This approach aims to expand the training data pool and enhance LLM performance by utilizing knowledge reasoning capabilities. The hypothesis suggests that LLMs can utilize partially understood knowledge for improved comprehension during self-supervised fine-tuning.

The study conducted experiments with mainstream LLMs on a closed-book question-answering task, validating the hypothesis. A two-stage fine-tuning framework was designed, incrementally improving knowledge mastery through augmented training data. Experimental results demonstrated substantial accuracy improvements on test data and enhanced knowledge acquisition within the training dataset.

Key contributions of the study include:

The proposal that fine-tuning can enhance LLMs' mastery of previously incompletely understood knowledge.
Utilizing augmented training data to further refine knowledge acquisition through subsequent fine-tuning rounds.
Broadening the data range for LLM fine-tuning, offering a data-driven approach to enhance model performance.
Impact of New Knowledge on Hallucination
Large Language Models (LLMs) gather extensive knowledge in the pretraining stage, turning them into repositories of information for later tasks. However, a common issue known as hallucination arises, where the model generates content that, while grammatically correct, contradicts context or real-world knowledge. The reasons behind hallucinations can be attributed to factors like poor datasets and inefficient data utilization.

Key points included:

LLMs tend to accumulate knowledge primarily in the pretraining phase, with limited capacity to acquire new knowledge during processes like supervised fine-tuning.
Fine-tuning often leads LLMs to grasp the stylistic nuances of the specific dataset rather than factual information, potentially resulting in the generation of inaccurate content.
Unfamiliar queries can trigger hallucinations in LLMs, indicating that training models on diverse data not encountered during pretraining can impact the nature of the hallucinations.
Introducing unfamiliar knowledge as a scaffold in in-context learning has been shown to reduce LLM performance in related tasks.
Integrating new knowledge post-pretraining stages poses a significant hurdle for LLMs, impeding their ability to both absorb and effectively apply this new information during fine-tuning and inference processes.
Quantifying LLM Knowledge Mastery
Assessing the degree of knowledge mastery in Large Language Models (LLMs) is crucial for model improvement and understanding their functioning. Here are the key points discussed in this section:

In scenarios with known ground truth, like closed-book question-answering, accuracy can be measured by generating responses.
For real-world LLM applications without ground truth, alternative evaluation methods are needed.
Confidence scores are commonly used to evaluate a model's certainty in its outputs, indicating knowledge mastery.
Well-calibrated confidence scores accurately reflect the correctness probability of model answers in real-world settings.
LLMs trained with Reinforcement Learning from Human Feedback (RLHF) may confidently provide incorrect information, affecting confidence calibration.
Methods to improve confidence calibration include assessing agreement among outputs, training models to compute confidence based on activation states, and direct queries on confidence.
Kadavath et al. highlighted that accuracy of repeated responses serves as a well-calibrated metric for evaluating model knowledge mastery, a method adopted in this study.
Continue Fine-tuning
The authors address the concept of continual fine-tuning, emphasizing the importance of iterative model updates with new data to enable adaptation over time. This process is vital for models dealing with a continuous influx of data to prevent forgetting previously learned information.

Key points included:
Continual fine-tuning involves iteratively updating a model with fresh data post initial fine-tuning.
Catastrophic forgetting is a major challenge where the model's performance significantly degrades on previously mastered tasks after exposure to new data.
The model inadvertently overwrites prior knowledge while adjusting to new information, leading to decreased accuracy and efficiency in previous tasks.
Methods
The authors conducted experiments in two parts. The first part analyzed changes in knowledge types post fine-tuning. Having confirmed their hypotheses, the second part introduced a two-stage fine-tuning strategy based on these results.

Key points:

The study is divided into two parts:
Part one: Analyzes changes in knowledge types after fine-tuning.
Part two: Proposes a two-stage fine-tuning strategy based on the first part's findings.
Detailed methods are explained in Sections 3.1 and 3.2.
The overall process is visually depicted in a figure for clarity.
Detect Knowledge Type Change
The researchers suggest that models can acquire new knowledge during fine-tuning that was not initially present in the training data:

New knowledge may be inferred from existing information in the training dataset.
Existing knowledge in the training data, when combined with model-acquired information during pretraining, can lead to the inference of previously unknown knowledge.
To test this hypothesis, the authors classified knowledge into four categories ('Highly Known', 'Maybe Known', 'Weakly Known', and 'Unknown'), following the framework by Gekhman et al., with specific criteria detailed in a table.

The model was fine-tuned using data classified as 'Maybe Known'.
Periodically, at the end of each epoch, the model's accuracy on the test set was assessed.
Knowledge types were re-evaluated once the model reached peak accuracy.
Two-stage Fine-tuning
In response to varied knowledge mastery outcomes post fine-tuning, the researchers introduced a two-stage fine-tuning strategy:

First Stage and Observations:

Some knowledge improved mastery, while some declined post fine-tuning in Section 3.1.
These observations led to the proposal of a two-stage fine-tuning approach.
Second Stage Details:

Utilized data classified as 'Maybe Known' including 'Weakly Known' and 'Unknown' post initial training.
Continued fine-tuning with this dataset and applied a replay strategy with a 0.2 replay ratio on 'Highly Known' data.
Training Strategy:

Implemented a strategy where a portion of 'Highly Known' data was sampled at the start of each epoch based on the replay ratio.
Mixed this sampled data with the training set, shuffled, and used for training.
Advantages of the Strategy:

Expanded training data effectively.
Helped in retaining previously mastered knowledge to prevent forgetting.
Decoding and Probabilistic Estimations:

Used a table for knowledge classification based on question-answer pairs.
Utilized greedy decoding with probability estimation for correct answers and performed ten generations with randomly constructed prompts.
Employed non-greedy decoding with a set temperature, sampling size, top-k value, and ten generations for analysis.
Type Definition Explanation
The section explains different types of question-answer pairs based on their difficulty level for correct answering using greedy or non-greedy decoding strategies:

Highly Known:

Definition: P(correct (q, a; T = 0)) = 1
Explanation: Can be correctly answered using a greedy decoding strategy.
Maybe Known:

Definition: P(correct (q, a; T = 0)) ∈ (0, 1)
Explanation: Might be correctly answered using a greedy decoding strategy.
Weakly Known:

Definition: P(correct (q, a; T = 0)) = 0 ∧ P(correct (q, a; T > 0)) > 0
Explanation: Might be correctly answered by a non-greedy decoding strategy, but not by a greedy one.
Unknown:

Definition: P(correct (q, a; T = 0)) = 0 ∧ P(correct (q, a; T > 0)) = 0
Explanation: Cannot be correctly answered even with a non-greedy decoding strategy.
Experiments and Analysis
The researchers conducted experiments to validate the limitations of one-stage fine-tuning and explored the effectiveness of two-stage fine-tuning in improving model performance. Here's a summary of the key findings and methodologies used:

One-stage Fine-Tuning Limitations:
Training only on data initially classified as 'MaybeKnown' outperformed training on all data or other types of data.
Increasing fine-tuning epochs did not enhance model performance, leading to overfitting without early stopping.
Two-stage Fine-Tuning Experiment:
Significant improvement in test accuracy was observed compared to one-stage fine-tuning.
Strategies were designed and tested to evaluate the impact of acquiring new knowledge vs. reducing forgetting through data replay.
Impact of Two-stage Fine-Tuning:
Strategies involved fine-tuning with different data categories to enhance model performance.
Combining approaches of augmenting training data with improved knowledge and replaying fully mastered data proved most effective.
Model Mastery Evaluation:
Tracking knowledge points in the training set revealed that two-stage fine-tuning significantly increased the knowledge classified as 'Highly Known'.
Multiple Rounds of Fine-Tuning:
Iterative rounds of fine-tuning post two-stage process did not further enhance model performance, potentially due to a saturation of knowledge mastery and lack of fresh data.
Efficiency and Convergence:
The method showed rapid convergence with a decreased number of knowledge type changes after the second stage of fine-tuning, highlighting the method's efficiency.
The experiments demonstrated the effectiveness of two-stage fine-tuning in improving model accuracy by balancing the acquisition of new knowledge and reduction of forgetting, presenting a comprehensive approach to enhance model performance in a structured and methodical manner.

Experimental Setup
The experimental configuration features:

Conducting a closed-book question-answering task using the WikiQA dataset, training on question-answer pairs with a single answer.
Testing for questions with multiple answers, considering a response correct if the model generates the first answer accurately.
Randomly selecting four other questions of the same type for each data point in the training set to create prompt templates and assessing accuracy through multiple generations.
Considering an answer correct if the correct response is a substring in the model's output, utilizing VLLM for accelerated inference on knowledge types.
Employing LoRA for fine-tuning with a rank setting of 64, utilizing the AdamW optimizer, a cosine scheduler, and an initial learning rate of 3e-4.
Conducting training on four A100 GPUs and inference on a single A100 GPU, all with 80GB of memory, with a batch size of 32.
Primarily using the Qwen2 model for experiments and also validating conclusions with trials using the LLaMA3 model.
Experiment Results and Analysis
The experiment involved fine-tuning a model on a training dataset with various knowledge types and evaluating its accuracy on a test set. Here are the key findings and analyses from the experimental results:

Changes in Knowledge Classification:

After fine-tuning, many knowledge points initially labeled as 'Weakly Known' or 'Unknown' transitioned to 'Maybe Known' or even 'Highly Known'.
Conversely, some points originally 'Highly Known' were downgraded, indicating possible forgetting during fine-tuning.
Model training was solely on 'Maybe Known' labeled data, influencing the classification changes observed.
Influence of Fine-Tuning:

Extensive attribute changes observed post-fine-tuning, notably affecting the knowledge categories.
Retesting points without fine-tuning revealed stable category change patterns due to concentration inequality.
Graph Construction Analysis:

A graph was constructed to investigate category changes, focusing on 'Maybe Known' and 'Weakly Known' points.
Nodes representing entities from question-answer pairs were connected, categorized as 'Initial', 'Reclassified', or 'Linked Reclassified'.
After training, nodes transitioning to 'Maybe Known' were often closely linked to original 'Maybe Known' nodes, supporting the experiment's hypothesis.
Fine-Tuning Configuration:

During continual fine-tuning, adjustments were made to the learning rate (15e-5), weight decay (0.01), replay ratio (0.2), and LoRA settings (rank of 64).
To ensure accuracy evaluation consistency, a fixed prompt test set was generated with a predefined random seed (42) at the experiment's outset.
Discussion
The authors proposed and experimented with the idea that fine-tuning can impact the classification of knowledge not directly involved in the process, leading to improvements in model accuracy and knowledge mastery. The experiments primarily focused on demonstrating the feasibility of this approach, providing a qualitative understanding of the process.

Key points included:

The refinement of the fine-tuning process based on insights resulted in enhanced model accuracy on the test set and better knowledge acquisition from the fine-tuned dataset.
The WikiQA dataset used in the study is less structured compared to domain-specific datasets, consisting of simple question-answer pairs with a limited range of question formats and lacking strong internal coherence seen in specialized datasets.
Domain-specific datasets may offer tighter connections between knowledge, potentially causing more significant changes in knowledge classification during fine-tuning.
Challenges arise in determining the presence of knowledge outside closed-book question-answering tasks, highlighting broader applications for the proposed approach along with new challenges to address.
Effective management of catastrophic forgetting during continual learning necessitates a balanced integration of new and old data, calling for improved methods in continual fine-tuning to achieve this equilibrium.
The experiments employed basic data replay and moderate learning rate reduction, indicating substantial room for enhancing the fine-tuning process further.