
3. Investigating large language models for their competence in extracting grammatically sound sentences from transcribed noisy utterances
Alina Wr√≥blewska



Introduction
The field of natural language understanding (NLU) aims to replicate human language comprehension through language modeling techniques. Researchers focus on developing large language models (LLMs) crucial for various natural language processing (NLP) tasks, including comprehension and generation of natural language. Despite advancements, LLMs still fall short of achieving the intricate linguistic competence inherent to humans. Training of LLMs involves extensive datasets with diverse textual, code-based, and structured data sources, presenting various linguistic challenges. The study focuses on the linguistic dimension of speech comprehension, separate from phonological difficulties encountered in spoken language decoding. Understanding spoken utterances involves challenges related to syntactic complexity and separating semantically relevant content from noise, a skill inherent to humans. The study aims to investigate if LLMs can effectively tackle similar challenges encountered by humans during utterance comprehension.

Key Points:

NLU strives to mimic human language comprehension through language modeling techniques.
LLMs are crucial for NLP tasks but still lack human-like linguistic competency.
Training of LLMs involves diverse datasets with various linguistic challenges.
The study focuses on the linguistic dimension of speech comprehension.
Human challenges in decoding spoken messages differ from the focus of the study on text processing.
Understanding spoken utterances involves addressing syntactic complexity and semantic noise filtering.
This research explores if LLMs can effectively handle challenges akin to human speech comprehension.
Proposed Approach
Processing spoken data poses unique challenges compared to written texts due to various factors like background noise and informal language use. This study focuses on leveraging advanced Language Model Models (LLMs) to process transcribed spoken data but questions their ability to accurately interpret noisy speech while ignoring non-fluency features. The research aims to:

Investigate if LLMs can extract syntactically and semantically correct sentences from noisy utterances.
Explore the capability of LLMs to filter out speech-specific elements that do not contribute to understanding spoken data.
Utilize prompting methodology to guide LLMs in identifying well-formed sentences within noisy utterances.
Evaluate the performance of LLMs in isolating refined utterances and excluding non-fluency features using a gold-standard dataset.
Examined Speech-Specific Phenomena
Conversations typically involve multiple speakers taking turns to speak. Each uninterrupted speech segment by a speaker, called a "turn," forms the fundamental unit for linguistic study. In analyzing speech, researchers consider various speech-specific phenomena:

Utterance Characteristics: Besides conveying the intended message, spoken utterances often incorporate interruptions or additional elements typical of spoken language.

Elements in Utterances: Spoken language may contain:

Non-linguistic tokens
Disfluencies (such as repetitions, hesitations, or corrections)
Restarts (restarting a sentence or phrase)
These phenomena are essential in understanding the dynamics and characteristics of natural conversations and the nuances of spoken language.

Non-Linguistic Tokens
Non-linguistic tokens in spoken language include silent and non-silent pauses (fillers) where the speaker briefly stops speaking. These pauses are transcribed as '...' for silence and '(yy)' for inarticulate sounds in transcripts. Pauses are marked with the discourse UD dependency type in annotations.

Non-linguistic tokens in spoken language consist of silent pauses for silence and non-silent pauses (fillers) for inarticulate sounds.
Transcripts represent silence as '...' and inarticulate sounds as '(yy)' to capture these non-linguistic components.
Pauses are identified in annotations with the discourse UD dependency type for further analysis.
Disfluencies
Disfluencies in speech are disruptions that interrupt the smooth flow of communication, indicating uncertainty, hesitation, or the need for clarification. These interruptions are often resolved through speech corrections. Common types of disfluencies include:

Repetitions: such as 'two, ei... eight, one, five'
Substitutions: for example, 'I received... we received a message'
Reformulations: like 'We lost eight... seventy pounds'
Key Points:

Disfluencies serve as cues for uncertainty or the need for clarification in speech.
Corrections are annotated with disfluencies, indicating their dependent relationship.
Disfluencies are categorized under the reparandum dependency type.
Restarts
Restarts in language refer to clauses or phrases that are disconnected from the preceding string of words, indicating an interruption in the ongoing utterance where the speaker starts a new phrase or clause afresh. For instance:

Example: 'cause I don't have a..., I don't remember the password' (the phrase "cause I don't have a..." is the restart in this context).
The annotation for restarts in linguistic analysis is typically identified with the parataxis:restart UD type.
In summary, restarts represent a linguistic phenomenon where the speaker breaks off from the current utterance and begins a new one, denoting a lack of syntactic continuity within the spoken language expression.

Prompt-driven Cognisance of Well-Structured Utterances
The researchers employ a prompting technique to direct Large Language Models (LLMs) in solving specific Natural Language Processing (NLP) tasks. Here's a breakdown of this approach:

Prompting Technique:

Instructs LLMs to generate or analyze texts based on predefined verbal instructions.
Tailors LLMs to specific NLP tasks, offering control over their responses.
Few-Shot Paradigm Implementation:

Involves providing input-output examples to guide LLMs.
Employs pairs of noisy input utterances and well-structured output utterances.
Enhances LLM performance by prompting well-structured outputs.
Processing Noisy Utterances:

LLMs filter out speech-specific noise like fillers, repetition strings, and false starts.
Challenges arise in identifying substitutions, reformulations, and false starts, necessitating deep input utterance analysis.
Grammar Coherence and Syntactic-Semantic Rules:

LLMs output tokens composing coherent, grammatically correct utterances after filtering non-fluency features.
Rely on acquired syntactic-semantic rules during training.
Approach to Sentence Recognition:

LLMs identify well-formed sentence tokens within noisy utterances by internalizing Abstract Syntax (AS) representations without needing to predict them.
Apply rules akin to human language processing for sentence identification, not aiming to predict ASs.
Definition of Evaluation Tasks
The section introduces the concept of probing as a valuable methodology for revealing the capabilities and limitations of Natural Language Processing (NLP) models, particularly when solving specialized tasks. This methodology aids in interpreting the information contained in the internal representations of the models.

Key points included:
Purpose of Probing Tasks:

Probing tasks are specifically designed to evaluate the linguistic proficiency of Large Language Models (LLMs) in recognizing speech-specific noise and extracting well-structured and coherent utterances.
The primary goal is to determine whether LLMs have acquired the ability to differentiate semantically relevant content from speech-specific noise while being trained on extensive textual datasets.
Benchmarking Process:

The proposed probing tasks involve benchmarking the output of LLMs against a gold-standard dataset.
Within this dataset, tokens representing well-structured utterances are annotated as positive instances, while speech-specific tokens are identified as negative instances.
By structuring the evaluation tasks in this manner, the researchers aim to gain insights into how effectively LLMs can distinguish and process relevant linguistic information from noise, contributing to a deeper understanding of their language comprehension capabilities.

Well-structured Task
The researchers aim to evaluate if language models (LLMs) preserve all tokens from well-structured utterances in their generated output. Here are the key points included in this section:

Objective: The primary focus is on assessing if tokens extracted by LLMs form coherent and well-formed sentences, which are validated using Universal Dependencies (UD) approximations.

Illustrative Example: The section provides a visual example where a well-structured utterance, following the predicate-argument structure of the verb 'podam' (meaning 'I will give' in English), is compared to the output generated by the model.

Discourse, Reparandum, and Restart
The tasks of discourse, reparandum, and restart aim to evaluate the ability of Language Model Models (LLMs) in correctly filtering out specific types of speech tokens from their generated utterances. These tasks serve the purpose of not only removing these tokens but also identifying the most challenging speech-specific phenomenon for LLMs.

Discourse Task
Aim: Verify if LLMs can identify and eliminate non-linguistic tokens like pauses and inarticulate sounds from the final utterances.
Example: In Figure X, three discourse subtrees (brown boxes) should not be present in the final utterance.
Reparandum Task
Objective: Assess whether LLMs can detect disfluencies such as repetitions, substitutions, and reformulations to accurately exclude them.
Sample Scenario: A reparandum token (marked with a blue box) and its related discourse token ('to...') should be excluded from the generated utterance.
Restart-task
The researchers designed a task, called Restart-task, to evaluate whether the Large Language Model (LLM) can accurately identify all tokens within false start subtrees. In this task:

A specific example was provided where a token is labeled with parataxis:restart.
The head-subtree of this token, highlighted with a green box, symbolizes a false start segment ('To niech pani...') that should be excluded from the final utterance.
This task aims to assess the LLM's ability to recognize and handle false start subtrees, which are crucial for ensuring the accuracy and coherence of generated text.

Tested Models
The study explores several Language Model Models (LLMs) based on the transformer architecture, including:

Generative Pre-trained Transformer (GPT-3.5 and GPT-4):
GPT-3.5 demonstrates exceptional performance in Natural Language Understanding (NLU) tasks.
GPT-4 is pre-trained to predict the next token in a document.
Llama 2 and Mistral 7B:
These publicly available LLMs are evaluated in the study.
Mistral 7B, a multi-modal model, achieves human-level performance on various benchmarks.
Bielik:
A recently released Polish LLM derived from Mistral 7B is examined in the research.
The researchers interact with these models through APIs to extract tokens from noisy input utterances, aiming for deterministic outputs by setting the temperature to 0 and the inference parameter (n) to 1.

Probing Dataset
The DiaBiz dataset is a comprehensive multi-modal dataset containing recorded and transcribed phone conversations in Polish. Here are the key points regarding this probing dataset:

The dataset consists of 101 dialogues, including 3421 turns and 82,806 tokens, manually annotated following the UD guidelines.
Each turn in the dataset has a conventional UD structure assigned to it.
In cases where turns contain multiple sentences, the UD trees of these sentences are interconnected using the parataxis label.
Apart from the standard UD dependency types, the dataset includes discourse, reparandum, and parataxis:restart types in the utterance trees.
The dataset is structured in JSON format, with each turn token labeled as True (suitable for well-structured utterances) or False (unsuitable for inclusion in LLM's output).
It comprises 75,107 True-tokens, with an average of 21.95 tokens per well-structured utterance, while the remaining 7699 False-tokens form subtrees of 5577 speech-specific phenomena.
Language Models (LLMs) are primarily expected to remove speech-specific discourse tokens, with an average removal of two tokens for each reparandum and eight tokens for each restart.
Discourse dependencies are typically associated with individual tokens, whereas reparandum and parataxis:restart heads allow for the removal of nested speech-specific dependencies.
Tokens in the speech-specific subtrees of the JSON structure are annotated as either True (indicating removal in a specific probing task) or False (suggesting preservation in a probing task), with some tokens annotated as True for multiple speech phenomena.
Prompt Engineering
The process of engineering prompts to guide Large Language Models (LLMs) in producing coherent sentences from noisy input involves considering various factors:

Providing illustrative explanations of speech phenomena versus incorporating few-shot explicit input-output examples in prompts was compared, with the latter approach found to be more beneficial.
GPT-4 stands out in reliably processing JSON structures, while GPT-3.5 and other LLMs often struggle with correct JSON generation, requiring instructions to use strings for both input and output.
Language choice (English vs. Polish) slightly impacts outcomes, as observed in testing scenarios with the Polish Bielik LLM using diverse prompts in a small dataset of 50 turns.
The designed prompts are intended to be universally applicable across all LLMs rather than customized for specific models, aiming to help LLMs eliminate speech-specific disruptions and produce well-formed outputs such as phrases, sentences, or sequences.
The prompts not only provide task-specific guidance but also include a range of speech-specific phenomena to consider, along with details on input and output formats demonstrated through examples.
First Experiment
The study evaluates the performance of Language Model Models (LLMs) in extracting well-structured utterances from noisy transcriptions compared to gold standard utterances. Here are the key findings from the first experiment:

Performance Comparison:

GPTs outperform open LLMs, especially in recall values.
GPT-4 and GPT-3.5 demonstrate high efficiency with recall rates of 97% and 94% respectively.
Bielik, Mistral, and Llama perform less effectively, with recall values ranging from 50% to 75%.
Recall Disparity Analysis:

Pearson's correlation is 0.93 and Spearman's correlation is 0.95.
GPT-4's extraction ratio aligns closely with the gold standard, contributing to its higher recall value.
Structure Completeness:

GPT-4 shows the most complete structures, while Bielik, Mistral, and Llama have deficiencies, especially in missing core arguments.
GPT-4 and GPT-3.5 exhibit better performance in extracting core arguments than Bielik, Mistral, and Llama.
Dependency Types Analysis:

Core arguments are crucial for coherent structures; GPTs have lower omissions compared to other LLMs.
Non-core dependents, nominal dependents, and function words also contribute to grammatical disruptions in extraction.
Speech-specific Elements:

Most input tokens are well-structured, potentially masking limitations in identifying speech-specific elements as negative instances.
True negative rates (TNR) indicate the quality of detecting speech-specific segments, with GPTs and Bielik showing higher accuracy than Llama and Mistral.
Out-of-Vocabulary (OOV) Words:

Llama and Mistral struggle with OOV words, generating significantly more compared to Bielik and GPTs.
GPT-3.5 and GPT-4 generate fewer OOV words, indicating better adherence to prompt instructions.
This experiment highlights the strengths and weaknesses of different LLMs in extracting structured utterances from noisy transcriptions, emphasizing the importance of recall, completeness, and accurate identification of speech-specific elements.

Second Experiment
The experiment aims to assess the effectiveness of various LLMs in filtering out different speech-specific phenomena. Here are the key findings and comparisons from the second experiment:

Mistral outperforms other LLMs in filtering discourse, reparandum, and restart segments effectively.
Mistral excels in identifying discourse phenomena, with a filtering ratio of 99%, while other LLMs like GPTs, Llama, and Bielik exhibit lower ratios.
Reparandum segments are filtered out the most by Mistral (almost 90%), followed by Llama (about 75%), and then by GPTs and Bielik (slightly over 50%).
LLMs struggle with identifying restart phenomena, with low recognition rates such as 30% for GPTs, 40-50% for Bielik, and 66% for Llama.
Unfinished statements (false starts) are often incorrectly treated as valid parts of utterances by LLMs, indicating a challenge in recognizing and filtering them out.
The complexity of identifying restarts stems from their substantial subtrees, averaging around 8 nodes, making it hard to differentiate them as semantically irrelevant.
Mistral stands out as the most effective in filtering out false start subtrees, crucial for constructing well-structured and coherent utterances.
Empirical Observations
The researchers found that in the first experiment, large language models (LLMs) like GPTs are proficient at identifying speech-specific noise and extracting sentences that conform to authoritative scripts (ASs). However, upon closer inspection, they discovered:

Bielik and GPTs struggle to filter out specific speech-related phenomena like restarts, repetitions, substitutions, and reformulations, leading to errors in noise removal.
Mistral, on the other hand, efficiently filters speech-specific segments but tends to be overly aggressive, removing not only noise but also elements of the predicate-argument structure.
This aggressive filtering by Mistral results in output utterances with incorrect structures and lacking coherence.
Overall, GPTs prioritize precision and error avoidance, leaving some residual speech noise, while Mistral's strategy leads to significant grammatical errors.
The errors in LLM-generated outputs highlight deficiencies in their language competence, underscoring the unresolved challenge of acquiring deep syntactic and semantic rules in LLM development.
Related Works
Studies extensively probe state-of-the-art Language Models (LMs) to evaluate their syntactic and semantic knowledge through various diagnostic tests:

Some research focuses on creating probing tests to directly analyze internal model structures and identify regions correlated with linguistic information.
Investigations showcase BERT's ability to perform multiple NLP tasks like POS tagging, parsing, and coreference resolution by pinpointing regions embedding linguistic information.
Parallel studies aim to assess models' proficiency and limitations in representing language, especially in terms of syntactic and semantic knowledge.
Research indicates that while LMs can classify sentences based on linguistic constructions, they struggle to extract meanings and use them effectively within contexts.
Recent studies explore probing speech models for syntax, evaluating their ability to encode linguistic information, such as the depth of syntax trees.
Speech processing involves stages like automatic speech recognition (ASR) and Natural Language Understanding (NLU), with efforts made to integrate ASR with disfluency removal to refine transcripts for downstream NLP and NLU tools:

Models developed for joint ASR and disfluency removal enhance transcript quality for subsequent processing.
Evaluation approaches test how well LMs can detect and filter noise, aiming not just to identify noise but to prioritize meaningful parts of utterances and process well-structured sentences effectively.
Conclusions
In this study, the researchers introduced an approach to evaluate Language Model Models (LLMs) in processing transcribed noisy utterances in Polish to determine their linguistic competence in detecting well-structured sentences within noisy speech. Key points included:

The researchers utilized the prompting technique, tasking powerful Generative Pre-trained Transformers (GPTs) like Llama, Mistral, and a Polish LLM called Bielik to identify noise in speech and extract structured utterances.
Evaluation of the models was done rigorously using a probing dataset from the UD-annotated subset of DiaBiz.
Mistral showed proficiency in filtering out speech-specific noise like false starts, although this proficiency stemmed more from aggressive filtering strategies than from true language comprehension, leading to grammatical errors.
While LLMs demonstrated linguistic competence in syntactic and semantic tasks, they struggled to identify complete and coherent sentences in noisy speech, indicating the need for deeper understanding of syntactic and semantic rules.
The study suggests that psycholinguistic factors and limited syntactic-semantic rule application might affect LLMs' overall performance, indicating the value of integrating psycholinguistic research methods in future evaluations.
The ability of LLMs to handle both written text and spontaneous speech transcriptions is deemed crucial for achieving human-like dialogue with machines, highlighting the importance of further research into selective language processing and speech-text integration.
Overall, the proposed evaluation approach sets the stage for future studies on LLMs and multimodal models to enhance their capabilities in handling both text and speech understanding for more seamless human-machine interactions.

Limitations
The researchers focused on a less commonly studied language, Polish, due to specific requirements of their experimental setup and the advantages of using the DiaBiz dataset:

Polish was chosen to avoid data contamination and because the DiaBiz dataset offers precise transcriptions of utterances, including non-linguistic and speech-specific elements.
The choice of Polish presents a challenge for Language Model Models (LLMs) as it involves processing a non-dominant language and text domain, which is not well-covered in existing LLM training data.
By using Polish, the study can provide insights into how LLMs capture universal linguistic properties across different languages and grammatical relations.
The study acknowledges limitations in language scope and generalization but aims to contribute positively to the Natural Language Processing (NLP) community, opening avenues for future research.
The research does not delve into the internal architectures of LLMs to pinpoint regions responsible for specific linguistic features, which is a clear limitation.
Future research will address this by exploring how LLMs recognize speech-specific elements and internalize syntactic-semantic structures at different layers.