

3. Leveraging Web-Crawled Data for High-Quality Fine-Tuning
Jing Zhou, Chenglin Jiang, Wei Shen, Xiao Zhou, Xiaonan He  -





Introduction
Large Language Models (LLMs) have gained significant attention recently, with data quality being crucial for their excellent performance. Two primary methodologies are utilized for data acquisition:

Leveraging LLMs like Alpaca, ORCA, and WizardLM for distillation to enhance smaller models.
Manual data annotation or selection to improve model performance, emphasizing data quality over quantity.
In challenging domains like mathematics, even advanced models like GPT-4 struggle to excel, highlighting the difficulty in obtaining high-quality annotated data quickly and cost-effectively. Web-crawled data, while noisy, offers a large volume that can address data collection challenges in specific domains, such as mathematical reasoning.

Key points included:

The focus on mathematical reasoning's deep understanding and proficient reasoning.
Previous studies showcasing the benefits of enhancing datasets with synthetic data generated using powerful LLMs like GPT-4.
The exploration of acquiring high-quality data without relying on external LLMs like GPT-4, crucial for prompt performance enhancement and independence as the top model.
The advantages of web-crawled data lie in its large volume and sufficient information for problem-solving, despite formatting issues. The proposed method aims to convert low-quality web-crawled data into high-quality data by aligning it with seed data to generate pairs for LLM fine-tuning, leading to a model specialized in data transformation.

The experimental results indicate a significant improvement in data quality and model performance compared to traditional methods, with key contributions being:

Introduction of a simple and effective method to enhance web-crawled data without relying on additional LLMs.
Improved performance of two open-source models, resulting in an average 9.4% enhancement in solving Chinese math problems.
Identification of how formatting errors can impact semantic accuracy and an analysis of the method's effectiveness.
Large Language Models for Mathematical Reasoning
Large language models (LLMs) are being assessed for complex reasoning abilities using mathematical word problems through various benchmarks like GSM8K, SVAMP in English, and Ape210K, CMATH in Chinese. To enhance reasoning capabilities, the Chain of Thought (CoT) model predicts step-by-step reasoning processes, while the majority voting technique improves model performance. Another approach, "Tree of Thoughts" (ToT), enables LLMs to evaluate reasoning paths for better decision-making.

Equipping LLMs with tools like calculators or programs can enhance their problem-solving abilities. The focus of this paper is on enhancing data quality for CoT, crucial for the model's reasoning proficiency.

Is GPT4 Generated Data Enough?
The section discusses the effectiveness of using synthetic data from powerful Language Model Models (LLMs) like GPT3.5 or GPT4 to improve model performance in training. While leveraging data generated by strong LLMs can enhance model performance, there are inherent limitations to consider:

Studies highlight the importance of employing robust LLMs to create diverse and challenging datasets, leading to improved model performance.
Despite the benefits, relying solely on synthetic data from strong LLMs can cap the model's upper performance limit due to certain inherent constraints.
The quality of data generated by LLMs may be subpar in domains where even the best LLMs struggle, thus raising concerns about data reliability.
Consequently, it is crucial to explore alternative methods that reduce the dependence on additional LLMs to address these limitations and drive progress in the field.
Methods for Generating Synthetic Data
The research discusses various methods for generating synthetic data to enhance the performance of Language Learning Models (LLMs) like GPT-3.5:

Use of GPT-3.5: Employing GPT-3.5 to create high-quality synthetic textbook data proves effective for improving coding performance and common sense reasoning.

Cosmopedia Approach: Cosmopedia constructs a synthetic dataset by extracting diverse prompts from curated sources and web data, focusing on direct generation rather than rewriting.

Retrieval-Augmented Generation (RAG): The method used in this research, termed as RAG, combines retrieval and generation during training, potentially leading to higher accuracy by rewriting existing data.

Utilization of Pretraining Datasets: Some studies explore generating formatted data using pretraining datasets. Jiuzhang 3.0 shows that even small language models can synthesize data by distilling knowledge from datasets generated by larger models like GPT-4.

Novel Data Rewriting Approach: The research introduces a novel method that maximizes the use of existing data through a matching algorithm. This approach differs from distillation processes and focuses on optimizing data utilization.

Settings
The authors gathered a carefully annotated dataset from an educational institution, along with a web-crawled set of mathematical problems. These datasets have different sources and are not identically distributed.

Key points included:
The web-crawled dataset was filtered to include only math problems with detailed solution steps.
The manually annotated seed dataset contains 84,095 instances.
The web-crawled dataset consists of 573,960 instances.
A Close Look at Web-Crawled Data
The researchers have improved the quality of web-crawled data through preprocessing; however, significant format errors and non-standard formatting issues still persist. In particular:

Mathematical expressions may be misrepresented, such as 3^2 - 1^2 incorrectly shown as 32−12 = 8 in the crawled data, leading to mathematical inaccuracies.
Errors in mathematical formulas, due to their complex nature, can make expressions seem correct in formatting while distorting their actual meaning.
These errors can mislead models during training, especially in complex scenarios.
Furthermore, the study highlights the most common errors found in web data, as outlined in Table, with corresponding examples provided for reference in another table.

Web-Crawled Data
The section introduces a series of equations and establishes a pattern where each equation follows the form "n^2 - (n-2)^2 = 8n" for n = 2, 3, 4, and so on. The nth equation in the series can be represented in the general form "n^2 - (n-2)^2 = 8n." This set of equations forms a pattern that can be used to express any equation in the series based on this formula.

The equations in the series follow a consistent pattern of arithmetic operations.
Each equation involves subtraction of the square of one number from the square of another number, resulting in a specific multiple of 8.
The general formula for the nth equation in the series is given as "n^2 - (n-2)^2 = 8n."
This formula allows for the representation of any specific equation in the series based on the value of n.
Meta-Data
The section provides insights into the challenges of correcting errors in mathematical question-answer pairs, specifically focusing on rule-based methods and their limitations. It introduces the distinction between local errors (correctable with nearby context) and global errors (requiring the full context for correction).

Correcting Errors in Mathematical Q&A Pairs
Rule-based methods struggle to correct errors in mathematical QA pairs effectively.
Utilizing flawed samples for training can lead to inconsistencies in model outputs and impact its understanding of mathematical concepts.
Discarding samples with errors reduces information in training data, negatively affecting model performance.
Limitations of Rule-Based Methods
Rule-based methods are vital in data preprocessing but have limitations in correcting all types of errors.
Two error types are defined: local errors and global errors.
Local errors can be fixed by looking at nearby words.
Global errors require understanding the entire context to rectify.
Rule-based methods excel at solving local errors but fall short in addressing global errors.
Challenge Example
An example illustrates the issue, where missing information like "3 times" in a question poses a challenge for rule-based correction.
Different interpretations like "6" or "62" for the answer further complicate error correction.
The example highlights the difficulties rule-based methods face in handling global errors effectively.
Fraction Format Errors
The section addresses issues related to how fractions and special characters like superscripts and subscripts are formatted, potentially affecting their readability and accuracy. Here are the key points included:

Fractions are noted to be in formats that are not compatible with LaTeX standards, which could lead to improper display or rendering of mathematical expressions.
Examples of fraction format errors include fractions written as "x\ny" instead of the correct format "x/y" or "xy."
Such formatting discrepancies can introduce ambiguity or errors in mathematical expressions, making it challenging for readers or computational tools to interpret them accurately.
Super/Subscripts Errors
The section also highlights concerns about the preservation of positional information for special characters such as superscripts and subscripts. Here are the main points covered:

There is a risk of losing the precise positioning of superscripts and subscripts within mathematical equations, potentially altering the intended meaning or calculations.
Errors related to super/subscripts may impact the clarity, correctness, and overall comprehension of mathematical notations, especially in complex or technical contexts.
Addressing these errors is crucial to ensure the accurate representation of mathematical content and to avoid misinterpretations or miscalculations in scientific or academic settings.
Missing Line Breaks
The section discusses two formatting issues commonly encountered in data files and documents:

-Missing Line Breaks: It highlights instances where line breaks ("\n") between different lines are absent, which can lead to data or text being displayed in a continuous block, making it harder to read or process.

-Non-standard Formulae: It points out the use of non-standard symbols like "×" being incorrectly represented as "X," which can affect the accuracy or interpretation of data, especially in scientific or mathematical contexts.

Garbled Characters
The section discusses severe formatting disruptions in a small subset of samples caused by OCR errors. These disruptions lead to garbled characters in the text. Here are the key points included:

Table showcasing typical error types found in web-crawled data.
Fraction format errors and superscripts/subscripts errors being the most common in their dataset, classified as global errors.
The specific example of "cm2" often being interpreted as "cm 2" in many cases, considered a "local error" easily fixable with rules.
Drawback of rule-based methods in handling these errors, requiring extensive analysis of cases and boundary situations, leading to increased workload for individuals.
Feasibility of Model-based Methods
The feasibility of model-based methods was investigated by examining web-crawled samples, highlighting the following key points:

The crawled data, despite various formatting issues, contains valuable information.
The study found that different mathematical problems have consistent formatting errors, allowing models to efficiently learn correct patterns with minimal samples.
Language model models (LLMs) find it easier to reformat data than perform complex reasoning tasks, simplifying the process of obtaining training data by modifying question and answer formats.
In comparison to rule-based approaches that consider local factors, LLMs excel at integrating all information in a sample.
The researchers recommend utilizing web-crawled data and capitalizing on neural networks' language processing capabilities to create high-quality training data. This aligns with the concept of Retrieval-Augmented Generation (RAG), which will be discussed in detail in Section 5.

A Simple and Effective Method for Data Cleaning
The authors propose a straightforward yet powerful method to improve web-crawled data quality by leveraging Large Language Models (LLMs). The method involves the following steps:

Constructing format converter training data by pairing web-crawled data with high-quality data using fuzzy matching.
Training an LLM to transform raw web-crawled examples into high-quality ones.
Using the trained LLM to convert web-crawled data into a high-quality format.
Training another LLM to solve mathematical problems using both high-quality data and the converted web-crawled data.
Formally, by matching math questions and answers between high-quality and web-crawled datasets, a matched dataset is derived. This process involves training a re-generation language model to convert web-crawled data into high-quality ones. The final steps include:

Generating formatted outputs for each sample [question, answer].
Extracting questions and answers from the output using predefined rules to create a cleaned dataset of mathematical problem-solving examples.
Fine-tuning an LLM on both high-quality data and the cleaned dataset to enhance model performance in mathematical reasoning.
Test Datasets and Evaluation Method
The researchers evaluated their model's performance on two Chinese math datasets: Ape210K and CMATH, which focus on Chinese elementary school math. Here is a summary of the evaluation process:

Instead of using LLM as the verifier, the researchers developed an automatic evaluation script in Python.
The automatic evaluation script achieved an impressive evaluation accuracy of 95% on the Ape210K dataset.
For the CMATH dataset, the researchers employed the evaluation script provided in the paper for assessment.
More details about the evaluation script can be found in the paper's Appendix A.2.
Models and Experimental Details
The researchers conducted experiments using two widely used Chinese open-source models: ChatGLM and Qwen, specifically focusing on ChatGLM2-6B and Qwen1.5-7B-Chat. Here are the key points regarding the models and experimental setup:

The experimental configuration features:
Supervised fine-tuning (SFT) with full parameterization across all experiments.
The experiments were not subjected to hyperparameter searches due to time constraints.
All experiments were run once using a predetermined, stable set of hyperparameters.
A batch size of 128 was utilized for both ChatGLM and Qwen during training.
ChatGLM used a cosine learning rate schedule with an initial rate of 5e-5, while Qwen used a learning rate of 5e-6.
The use of a cosine learning rate schedule is emphasized for stable training and improved results.
Instead of early stopping, the training was carried out for three epochs to train all data comprehensively.
Matching Algorithm
The matching algorithm described in the paper focuses on identifying completely identical questions by implementing the following steps:

Initiation of the process by removing characters not belonging to Chinese language, digits, or English letters, as these do not impact the question's meaning.
Elimination of English phrases longer than two characters, as they are more likely to be LaTeX identifiers than variables in Chinese Mathematical problems.
Definition of a pair as matching only if the processed questions are exactly the same or if the processed answer span of high-quality data is a subsequence of the web-crawled data.
Flexibility of the algorithm to be adapted to new scenarios due to the non-critical nature of specific details.
Utilization of a rule-based method instead of embedding-based methods for the algorithm, as rule-based approaches provide more precise control over the details. Embedding methods may mistakenly identify variations like "2+3=5" and "3+5=8" as similar when they are not identical.
Primary goal: Identifying question pairs that are entirely identical rather than just similar.
Main Results
The authors present key findings regarding the comparison between traditional rule-based processes and their model-based approach in data cleaning and training. The main results include:

Fine-tuning the model with high-quality and cleaned data significantly enhances performance by an average of 9.4%, showcasing the effectiveness of the model-based method.
Single-stage fine-tuning (both rule-based and model-based) outperforms post-training followed by fine-tuning, highlighting the superior data efficiency of the single-stage fine-tuning.
The proposed model-based method outperforms the refined rule-based method by up to 4 points, attributed to the higher data quality generated.
The model not only improves data accuracy but also unifies different data formats, aiding the model's understanding.
A notable observation is the comparable performance between cleaned (model) data and a combination of cleaned (model) and high-quality data, suggesting distilled knowledge from training data benefits less when combined.
The authors achieved exceptional performance on small models within 10B parameters, surpassing larger models like Yi-Chat, DeepSeek-Chat, and ChatGLM3, and even some closed-source models like Claude-2 with their 7B model.
In summary, the model-based method for data cleaning and training demonstrated superior performance in accuracy and efficiency compared to traditional rule-based approaches, leading to significant improvements in model performance and data utilization.

More Analysis of the Effectiveness
The section delves into a detailed comparison between the model-based approach and the conventional rule-based pipeline across different model and data sizes. Here's a breakdown of the content:

Comparison conducted between model-based method and traditional rule-based pipeline
Varied model and data sizes for comprehensive analysis
Prompts for one-shot generation and SFT detailed in Appendix A.3
Results of the comparison provided in a corresponding figure
Effectiveness of Rewriting Algorithm
The researchers compared the performance of different rewriting algorithms and found the following results:

Their model-based cleaning method consistently outperformed the one-shot and rule-based methods across various models and data volumes.
Chat-GLM2 and Qwen, when used for one-shot generation, showed inferior results compared to the model-based method after Sequential Fine-Tuning (SFT) using a matching algorithm.
The model-based method showed an average improvement of 3.6% over the rule-based method with Chat-GLM2 and 6.7% with Qwen.
A better base model benefitted more from the model-based re-generation strategy.
The researchers also investigated the impact of data volume on model performance and found:

There was a linearly increasing trend in model effectiveness as data volume doubled, indicating a log-linear relationship.
Chat-GLM showed a 5% improvement for every doubling of data volume, while Qwen only improved by 2%.
The discrepancy in improvement rates could be due to the distribution of data encountered during pre-training, especially limited exposure to mathematical-related data leading to more significant performance gains with increased data volume.
Impact on Questions Across Grades
The impact of the cleaning method on questions across various grade levels is investigated in this section. Here are the key points:

The study analyzes the effect of different cleaning methods on questions grouped by grade levels in the CMATH dataset.
Results show that the model-based re-generation strategy enhances question performance across grades compared to the rule-based method.
The greatest performance improvement is observed for fifth-grade questions on ChatGLM and sixth-grade questions on Qwen.
Higher-grade questions, often assessing complex concepts like fractions or geometry, show significant improvement with the model-based approach.
Qwen exhibits marked enhancements in Grade 6, while ChatGLM struggles with rectifying intricate problems.
The study also investigates the robustness of the method concerning the quantity of high-quality data:
84,095 instances of high-quality seed data were utilized, resulting in 24,336 paired instances for training.
Experiments varying the amount of high-quality data show that even with a subset of 10,000 instances, the model outperforms the rule-based method.
The method's robustness with data size is attributed to the manageable challenge LLMs face in addressing consistent formatting errors.
Despite using a smaller dataset for experiments (80,000 samples), the model demonstrates practicality and effectiveness in real-world scenarios.
The Quality of Data Rewriting
The evaluation of data rewriting quality involved comparing different methods using 100 random data entries, with results summarized in a table. Here's what the findings reveal:

Rule-based rewriting method outperforms the baseline by 5 points.
ChatGLM surpasses the baseline by 12 points.
Qwen surpasses the baseline by 17 points.
Performance of the method on Qwen exceeds that of GPT-4, showing its effectiveness.
Key Points:
The method's performance on challenging questions improves as model capabilities enhance (ChatGLM2 -> Qwen1.5 -> GPT4).
Qwen and ChatGLM struggle with difficult word questions, while GPT4 excels in such scenarios.
The approach surpasses GPT4 in rectifying challenging errors in the dataset, like fraction format errors.
Superior performance is demonstrated on fill-in-the-blank and true/false questions, indicating potential enhancement for GPT-4.
A case study using Qwen1.5-7B-Chat showcases accurate conversion of fraction errors and context-based number filling.
Additional cases in Appendix A.6 highlight significant enhancement in various error types using the proposed method.
In summary, the method shows promise in improving data quality across different error types, with the potential to enhance the performance of existing models like GPT-4.

Model-Cleaned: Question Solution
Guangming Chicken Farm raised 2400 chickens this year, reflecting a 15% increase from last year. To determine the number of chickens raised last year:

Calculation: Divide 2400 by (1 + 15%), which simplifies to 2400 ÷ 1.15 = 2400 ÷ 6/5 = 2000 chickens.
Answer: Hence, the farm raised 2000 chickens last year.
Model-Cleaned: Transformed Examples Table
The table in this section presents examples of model-transformed cases from Chinese elementary school math problems:

Dataset: Chinese elementary school math problems.
Translation: English translations provided for better comprehension.
Discussions
The authors discuss the relationship of their method with the widely discussed RAG technology, highlighting how their approach aligns with RAG by embedding references during training to reduce "hallucinations" in answers. Key points included:

RAG Technology: Referencing during inference to improve answer generation and reduce errors.
Model Enhancement: Distilling unknown knowledge into training data to enhance the model's capabilities.
Generalization Impact: Injecting knowledge positively affects the model's generalization across different domains.
The discussion extends to the possible applications of the proposed method in various domains, emphasizing the role of appropriate data formats derived from pretraining datasets for efficient SFT. Key points included:

Application Flexibility: Method can be extended to different scenarios by leveraging appropriate data formats.
Data Sources: Utilizing open-source datasets and web-crawled resources to create paired data for training.
Personal Corpora: Feasibility of using small seed data in unique scenarios to generate high-quality SFT data.
Future Directions
The training data for the transforming method is generated automatically through fuzzy matching, offering advantages and challenges:

Benefits: Enables the generator to provide accurate answers even when the original answers are wrong.
Challenges: May introduce errors when original answers are correct.
To address potential errors arising from correct original answers, the following strategies could be beneficial:

Utilizing additional verifiers to enhance accuracy.
Implementing self-training methods to enhance the model's mathematical proficiency and the quality of the transformed data.
Conclusion
The researchers highlighted the impact of format errors in web-crawled data on both output format and semantic accuracy in mathematical problems. Building on this understanding, they introduced a straightforward and effective method that utilizes web-crawled data and the capabilities of LLMs to enhance data quality without needing additional language models like GPT-4.

Key points included:

The proposed method demonstrated its effectiveness through experiments, showcasing its superiority in transforming web-crawled data into high-quality data.
Future research avenues could explore how this method can be extended to improve data quality in diverse scenarios beyond mathematical problems.
While the method significantly enhanced model performance without specific annotations or extra language models, certain scenarios may still require some annotations for a smoother start.
It was noted that the data cleaning process could potentially introduce new errors, underscoring the importance of exploring additional methods to further improve data quality.
A.1 Datasets
The datasets in this paper consist of web-crawled data that has undergone processing with Optical Character Recognition (OCR) and filtering techniques to extract text from images on webpages and filter out low-quality samples. Here are the key points:

The web-crawled data contains a mix of texts and images.
OCR is utilized to extract text from images, followed by the application of rules to remove low-quality samples.
Despite the initial processing, there are still format errors and instances of non-standard formatting in the data.
After processing, the dataset comprises 84,095 high-quality seed data and 573,960 web-crawled data entries.
A.2 Evaluation Script
The authors created an auto-evaluation script to assess model performance on Ape210K, yielding an impressive 95% accuracy. Here are the key points regarding the evaluation script:

The evaluation involved analyzing 2 random files: one from ChatGLM2 and the other from Qwen, each with 100 examples, resulting in the same 95% accuracy.

Out of 10 samples misjudged by the script, 3 were initially incorrect but marked correct by the script, while 7 were actually correct but wrongly identified as incorrect by the script.

The main cause of these evaluation discrepancies was the diverse range of outputs, leading to mismatches between the expected answers and those generated by the model.

A.3 Prompts
The researchers do not incorporate any prompts for the mathematical model. However, they do use a prompt for the format conversion of their model to enhance generation performance. The specific prompt employed for this purpose is aimed at implementing one-shot learning. The prompt utilized is:

"To strengthen the generation performance of models without SFT, we adopt one-shot learning."
A.4 Format Error Examples of Web-Crawled Data
The section presents examples of format errors commonly encountered in web-crawled data, as outlined in Table. These errors encompass a variety of issues, such as:

Fraction format errors
Superscripts and subscripts errors
Missing line errors
Other non-standard formats
By illustrating these examples, the researchers highlight the challenges associated with the quality and consistency of data obtained through web crawling.

A.5 Rule-based Methods
The researchers applied rule-based methods to refine web-crawled data by implementing specific rules outlined in Section 4.3. Here are the key points:

Rule-based Refinement:

Developed templates to extract detailed answer parts specifically aligned with questions.
Corrected fraction-related errors by standardizing formats (e.g., replacing "NUM1\nNUM2" with "NUM1/NUM2").
Addressed equation-related non-standard expressions (e.g., replacing ",=" with "=" and ",≈" with "≈").
Limitations and Challenges:

Complex Format Errors: Some formatting errors, though straightforward for humans, pose challenges for rule-based systems.
Incomplete Rule Coverage: It's impractical to encompass all possible rules comprehensively.
Global Errors: Certain overarching errors cannot be rectified through rule-based approaches.
Challenges of Rule-based Systems:

Unpredictable Interactions: Cleaning one format could inadvertently introduce errors in another format.
Example: The difficulty in accurately replacing NUM1\nNUM2 with NUM1/NUM2 without affecting other data was illustrated.
Potential for Improvement:

Neural Networks: The section highlights that neural networks demonstrate greater effectiveness in addressing complex replacement challenges compared to rule-based methods.
A.6 Case Study
The case study section demonstrates the effectiveness of the model by showcasing two additional model-transformed cases using Qwen1.5-7B-Chat:

In the first case, the model successfully detects and corrects the erroneously formatted superscript, enhancing the accuracy of the text.

The second case highlights the model's ability to insert missing line breaks between equations, improving readability and reducing confusion.

Both cases emphasize how the model accurately extracts essential elements from the samples, emphasizing quality over mere replication of the original analysis.

