1. Quality Matters: Evaluating Synthetic Data for Tool-Using LLMs
Shadi Iskander, Nachshon Cohen, Zohar Karnin, Ori Shapira, Sofia Tolmach 

This paper investigates the crucial role of data quality in training large language models (LLMs) for tool use. The authors argue that while synthetic data generation has accelerated progress in this field, the lack of systematic quality assessment poses significant challenges. They propose and evaluate methods to assess and improve the quality of such data, demonstrating its impact on model performance.

Here's a detailed explanation:

1. The Problem:

Training LLMs to utilize external tools (APIs) is a rapidly growing area. However, labeled data for this task is scarce and expensive to create. Researchers often rely on synthetic data generated by LLMs, but the quality of this synthetic data is often unchecked. This uncontrolled data quality can lead to inaccurate models. Existing work focuses mainly on improving training or evaluation methods without adequately addressing data quality.

2. The Proposed Solution:

The authors address this problem with a two-pronged approach to evaluate and enhance the quality of synthetic data for tool-using LLMs:

Intrinsic Evaluation: They define six human-interpretable criteria for assessing the quality of individual data instances:

Instruction Properties:

Specificity: Does the instruction provide sufficient detail?

Coherence: Are the requests within the instruction logically connected?

Solvability: Can the requests be fulfilled using the available APIs?

API-Call Sequence Properties:

Parameter Alignment: Do the parameter values in the API calls accurately reflect the instruction?

Sufficiency: Does the API call sequence address all requests in the instruction?

Minimality: Is the API call sequence efficient (no redundant calls)?

Manual Annotation: They manually annotated a subset of data instances from two widely used benchmarks (ToolBench and ToolAlpaca) based on these criteria to establish ground truth.

Automated Metrics: To automate the process, they devised methods to automatically assess each criterion using ChatGPT. This involved transforming some criteria (specificity, coherence, and parameter alignment) into different NLP subtasks better suited for automated assessment by ChatGPT.

In-Context Evaluation (ICE): As an alternative approach, they introduced ICE. This method assesses the educational value of a data instance by evaluating its effectiveness as a one-shot example in in-context learning. They measure the performance of an LLM at solving related test queries after providing the instance as an example. This is fully automated and doesn't rely on manually defined criteria.

Data Filtering: They used both the automated intrinsic metrics and ICE scores to filter low-quality data from the benchmarks' training sets.

Extrinsic Evaluation: Finally, they trained tool-using LLMs (Vicuna-7B and LLaMA-7B) using the filtered and unfiltered datasets and compared their performance on the respective benchmarks' test sets using pass rate as the evaluation metric.

3. Experiments and Results:

The experiments involved:

Data Generation: Utilizing the ToolBench and ToolAlpaca benchmarks, which are constructed with different methods.

Intrinsic Data Quality Assessment: Manual annotation and automated assessment using ChatGPT, yielding agreement between the two methods. The analysis highlighted a significant difference in data quality between ToolBench and ToolAlpaca, with ToolBench having significantly more low-quality instances.

Extrinsic Evaluation: Training LLMs with varying amounts and quality of data. Results showed that models trained on smaller high-quality datasets (filtered based on intrinsic quality and/or ICE) performed comparably to, or better than, models trained on the much larger original (unfiltered) datasets.

4. Key Findings:

Data quality significantly impacts the performance of tool-using LLMs.

High-quality data is far more important than large quantities of low-quality data. A smaller, high-quality dataset can outperform a larger, lower-quality dataset.

Both intrinsic evaluation (based on the six criteria) and ICE effectively measure data quality. The automated methods closely match manual annotations. ICE is more efficient for large datasets, though the intrinsic metrics offer more interpretable results.

5. Significance:

This work emphasizes the critical need for rigorous data quality checks in training tool-using LLMs. The proposed methods offer practical and scalable ways to assess and improve data quality, contributing to the development of more reliable and efficient tool-using LLMs. The "less is more" philosophy is empirically supported in this context; carefully curated, high-quality data is far superior to large quantities of noisy data.