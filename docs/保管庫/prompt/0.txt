
## data is everything
## 1. Training Data Volume and Dialogue Length
## 2. AI-to-AI Dialogue Generation Validity
## 3. Quality Control in AI-Generated Dialogues
## 4. System Prompts in Fine-tuning
## 5. Fine-tuning Evaluation Metrics
## 6. LoRA Hyperparameter Optimization



それぞれについてどこの箇所を抜粋しているかを見せてください


# 2. 

## 1. MIND: Math Informed syNthetic Dialogues (Akter et al., 2023):
Conversation Prompts: Seven different conversational prompts (styles) are designed to elicit various types of interactions:
- Two Students
- Teacher-Student
- Two Professors
- Debater-Debater
- Interviewer-Interviewee
...

The results highlight that carefully designed prompts and the structure of interactions significantly affect the quality of generated data and subsequently the performance of the trained LLMs.

## 2.  Self-Directed Synthetic Dialogues (Lambert et al., 2023):

This technical report introduces Self-Directed Synthetic Dialogues (SDSD), a novel dataset designed to advance the fine-tuning of language models (LLMs) in generating long, multi-turn conversations... SDSD aims to address the limitations of existing datasets by generating synthetic dialogues where the LLM interacts with itself, following a pre-defined conversation plan.

...the research evidence suggests that future iterations of the dataset could focus on improving the generation of conversations that successfully incorporate these more challenging principles.



3. Synthetic Persona-based Conversations (Jandaghi et al., 2023):

Human Evaluation:
- 91% of SPC conversations judged as human-like compared to human-generated conversations.
- Only 8.04% of conversations for new personas in Iteration 3* deemed artificial, indicating SPC's realism.
- Evaluation of conversation faithfulness showed consistency above 75% across iterations with minimal variation.



