



8. ScalingFilter: Assessing Data Quality through Inverse Utilization of Scaling Laws
Ruihang Li, Yixuan Wei, Miaosen Zhang, Nenghai Yu, Han Hu, Houwen Peng





Introduction
The success of large language models (LLMs) depends significantly on the quality and quantity of the pre-training corpus. Researchers have developed data curation pipelines to enhance dataset quality, focusing on raw web crawling, text extraction, repetition, toxic content removal, and quality filtering.

Quality filters aim to extract high-quality data from a noisy raw corpus, improving the language model's performance without increasing training costs.
Existing filters are categorized as reference-dependent and reference-free methods.
Reference-dependent methods use high-quality seed datasets for filtering but may introduce biases, limiting diversity.
Reference-free methods like perplexity gating assess data quality using predefined metrics like perplexity scores but struggle with indirect quality assessment.
Simple and repetitive content favored by indirect methods may hinder learning diversity and complexity.
ScalingFilter: A Novel Quality Filtering Approach
The ScalingFilter introduced in this study leverages recent scaling laws in generative modeling to assess data quality effectively.

ScalingFilter assesses data quality by analyzing perplexity differences between two pre-trained models on the same data.
Documents with higher perplexity differences are indicative of higher quality, addressing bias issues from a single reference model.
This approach enhances data diversity and complexity by avoiding selection bias towards simple and repetitive content.
The method offers a theoretical analysis using perplexity differences as a quality indicator, inversely deriving model scaling laws.
Experimental Evaluation
Experimental results demonstrate the superiority of ScalingFilter over existing methods in improving filtered data quality while preserving diversity.

A pair of meta-models with different sizes is used to assess perplexity differences in the raw dataset.
High-quality documents are selected using a top-k strategy and used to train a 1.3B model from scratch.
Zero-shot performance on downstream tasks and semantic diversity of the filtered dataset are evaluated.
ScalingFilter outperforms the unfiltered baseline and previous state-of-the-art quality filtering methods, showing improvements in downstream accuracy and semantic diversity.
Contributions
The contributions of this work are threefold:

Introduction of the quality factor, a novel metric correlating directly with training data quality through model scaling laws.
Proposal of ScalingFilter, a quality filtering method utilizing the quality factor for unbiased data curation without reference data, enhancing corpus representativeness.
Introduction of semantic diversity as a reliable metric to evaluate data diversity, showing that ScalingFilter preserves raw data richness better compared to conventional methods.
Methodology
The ScalingFilter method aims to estimate data quality without relying on a high-quality reference dataset or perplexity scores, which can introduce biases. The core concept involves applying the scaling law to estimate sample quality by inversely scaling data or model sizes to minimize loss. Here are the key points:

Optimal Scaling Strategy:

Scaling law indicates a power-law decrease in loss with increasing data or model size.
Determines the optimal model/data scaling-up strategy under a fixed computational budget.
Model scaling exponent “a” and data scaling exponent “b” represent the optimal scaling ratios.
Quality Evaluation:

Experiments show that high-quality data increases the model scaling exponent “a”.
Higher “a” values accelerate loss decrease as model parameters increase, benefiting from high-quality data.
Quality Factor Definition:

Quality factor, denoted as “d”, correlates with data quality.
Quality factor calculation involves perplexity scores of text samples evaluated by meta-models.
Relationships and Proofs:

Positive correlation between quality factor, model scaling exponent “a”, and data quality.
Formulas relate model loss to model size and scaling exponents.
The relationship between loss, model scaling exponent, and model size indicates how increasing model size reduces loss.
Selecting High-Quality Data:

Higher perplexity reduction with increasing model parameters indicates high-quality data.
Quality factor directly characterizes data quality to select high-quality data from a mixed dataset using ScalingFilter.
Practical Implementation:

Calculate quality factor for each sample in a dataset.
Select top samples based on quality factor for data filtering, forming a resulting dataset with the desired cleaning rate.
Experiments
The researchers showcase the efficacy of ScalingFilter by conducting thorough experiments. Here's a breakdown of the key points highlighted in this section:

Language models trained on data filtered by ScalingFilter consistently outperformed:

The unfiltered baseline data
Other commonly used quality filtering methods
The superiority of the models demonstrates the higher quality of the filtered data provided by ScalingFilter.

Evaluation across various downstream tasks consistently showed the enhanced performance of models trained on filtered data.

The researchers also examined the semantic diversity of the filtered dataset:

ScalingFilter was successful in maintaining the diverse range of semantics present in the original dataset.
By presenting these findings, the experiments validate the effectiveness of ScalingFilter in enhancing data quality and preserving semantic diversity, crucial for improved performance in language modeling tasks.

Data Quality Evaluation
The section discusses the evaluation of data quality in the research study, covering the following key points:

The researchers started with five CommonCrawl dumps from 2019 to 2023, processed using the CCNet pipeline to obtain 500 GB of text data, randomly selecting 125 billion tokens for quality filtering.
Each experiment involved training a decoder-only model with 1.3B parameters on 25B tokens, stopping when performance leveled off.
Various tasks like sentence completion, coreference resolution, natural language inference, and question answering were performed using different quality filters.
Pre-trained GPT-2 models with 124M and 774M parameters were used as meta-models to evaluate quality factors for each sample.
The lm-evaluation-harness library was utilized to evaluate zero-shot performance across different downstream tasks for each model trained on filtered documents.
Quality filters like ScalingFilter were compared with random selection, binary classification, importance resampling, and perplexity gating methods, showing performance improvements.
Perplexity, computed by the meta-model with higher capacity, was used to evaluate document quality.
The results revealed that ScalingFilter outperformed existing methods, showing improvements in accuracy over binary classification, importance resampling, and perplexity gating methods.
Ablations and comparisons with various reference datasets were conducted, with competitive performance observed across meta-models trained on different datasets.
Overall, the evaluation of data quality in the study demonstrated the effectiveness of the ScalingFilter method in improving downstream task performance compared to traditional filtering approaches.

Ablations on Different Sizes of Meta-Models
The researchers conducted experiments to analyze the effects of using pairs of meta-models with varying sizes. Key points included:

Decreasing the size of the larger model in meta-models from 774M to 335M led to a 0.96% decrease in average performance in downstream tasks.
Conversely, increasing the size of the smaller model in meta-models from 124M to 335M resulted in a 1.28% performance decrease.
Larger parameter differences between meta-models may better amplify variations in how models capture textual data, improving quality factor assessment.
Future work entails further exploration of the impact of size discrepancies between meta-models.
Ablations on Reference Datasets
The study also evaluated the influence of different reference datasets on quality filtering methods relying on a reference. Noteworthy details included:

Binary classification using OpenWebText as the positive class demonstrated the best performance, aligning with previous research highlighting OpenWebText's superior data quality.
Combining datasets like OpenWebText, Wikipedia, and books produced inferior results, possibly due to training recipe variations.
Surprisingly, importance resampling with Wikipedia resulted in accuracy similar to random baseline, indicating domain shift issues.
The choice of reference datasets significantly affects the performance of quality filters dependent on references.
Data Diversity Evaluation
Training large language models necessitates diverse data to avoid reducing knowledge diversity by filtering out documents that differ from the reference dataset. To evaluate data diversity, the researchers introduced a metric based on semantic diversity, as detailed below:

Semantic Diversity Metric:

Semantic diversity is defined as the exponential of the Shannon entropy of the eigenvalues of the semantic similarity matrix, building on previous work by Friedman and Dieng (2022).
By considering a set of text documents x1, x2, ..., xn and a semantic similarity function f, a similarity matrix S is constructed, where each entry si,j = f(xi, xj).
Eigenvalues λ1, λ2, ..., λn of S/n are utilized to define semantic diversity.
Methodology:

The approach involves extracting semantic embeddings for each document using a pre-trained language model, with cosine similarity as the similarity function.
The bge-base-en-v1.5 model was employed in experiments, focusing on the effects of reference data variations for binary classification and importance resampling.
Experimental Findings:

Evaluation results suggest that OpenWebText emerges as the most suitable reference dataset choice for both reference-dependent quality filtering methods.
Quality Filter
The researchers faced computational constraints when calculating semantic diversity for all documents in the dataset. To determine the optimal document count for this metric, experiments were conducted on the unfiltered dataset. Here's how they proceeded:

Random selection of n samples for each experiment
Calculation of semantic diversity scores for these samples
Repeated this process 10 times to average the score and determine standard deviation
Key points included in this section are:

Semantic diversity stabilizes at around 10,000 samples with a standard deviation of 0.12, balancing accuracy and efficiency for subsequent experiments.
The selected metric effectively reflects data semantic diversity, especially with multiple datasets like news articles, movie reviews, forums, Wikipedia, and web pages.
A positive correlation was observed between semantic diversity and the number of datasets used, demonstrating its accuracy in reflecting data diversity.
Quality filters applied to the dataset enhanced diversity compared to the original unfiltered dataset, mainly by removing machine-generated spams, resulting in higher diversity levels.
Related Work
Research on pretraining data for large language models involves filtering low-quality content using linear classifiers or language models to score and filter documents. Commonly used high-quality data proxies include Wikipedia, books, and OpenWebText. Various approaches and studies have been conducted in this area:

Early studies utilized linear binary classifiers, while more recent ones applied hard thresholding using Wikipedia as the sole positive class, potentially compromising corpus diversity and introducing biases.
Some studies categorized documents by perplexity levels, retaining low-perplexity documents for pre-training, while others preserved all data to maintain diverse writing styles.
Filtering data based on perplexity could introduce bias towards easily predictable texts, potentially discarding high-quality but challenging content.
The authors' approach introduces a quality factor derived from two language models to address biases and improve data selection for pretraining.
Scaling Laws
Scaling laws examine the relationship between model size, dataset size, compute budget, and performance during neural network training. Key points in scaling laws research include:

Initial studies identified a power-law relationship between model and data scaling factors, while a unified formula incorporating data-dependent scaling terms has been proposed.
Recent studies highlight variations in scaling laws across multilingual and multimodal settings, emphasizing the impact of data modality on scaling behaviors.
An in-depth analysis on scaling laws revealed varied parameters for uni-modal and mixed-modal contexts, indicating that data quality can significantly affect scaling behaviors.
A recent study confirms that data quality impacts model and data scaling exponents in scaling laws, underscoring the relationship between scaling law parameters and data quality for selecting high-quality samples.
Conclusion
The researchers introduced ScalingFilter for data quality filtering without the need for reference data. They showed that differences in perplexity across models of varying sizes are related to data quality. Key points from the conclusion include:

The perplexity difference among models correlates positively with data quality.
Samples with higher perplexity differences are chosen for the pretraining dataset.
By removing biases associated with reference datasets, ScalingFilter outperforms strong baselines in downstream tasks while maintaining data diversity.
Limitations
The ScalingFilter method, despite its usefulness, has several limitations that the researchers need to consider:

The method relies on perplexity differences between two language models (LLMs), which may overlook nuanced aspects of text quality such as factual accuracy or various biases like race, social class, and gender biases.
Significant computational resources are required to calculate perplexity differences, especially for large datasets, which can be a practical limitation.
The applicability of ScalingFilter to other languages and domains with limited data is uncertain, pointing towards possible challenges in generalization.
To address these limitations and delve deeper into the connection between semantic diversity, model performance, fairness, and bias, future research should focus on improving ScalingFilter and exploring its capabilities in more diverse settings.

A.1 Hyperparameters of Training Language Models
The researchers trained decoder-only transformer models using Megatron-DeepSpeed. The hyperparameters utilized in the training process include:

Batch size: 2048
Learning rate: 0.00008
Warmup steps: 3% of the total number of training steps
Gradient clipping norm: 0.1
Adam epsilon: 1e-6
Dropout: 0.1
Attention dropout: 0.0
Number of layers: 12
Hidden size: 768
Feedforward size: 3072
These hyperparameters were crucial in achieving the desired training outcomes for the language models.

A.2 Derivation of ScalingFilter
The derivation of ScalingFilter involves exploring the relationship between different parameters and loss functions to optimize model and data scaling. Here's a breakdown of the key points:

The parametric loss function introduced by Chinchilla involves a scaling law represented by a power-law form with model and data scaling exponents (a and b).
By defining η as the sum of two scaling exponents (α and β), the parametric loss function can be expressed to show the relationship with model size (N).
The partial derivatives of the loss function with respect to N indicate that expected loss decreases as the model size increases under the same training conditions.
A specific relationship is established between the model parameter (a) and the slope of the tangent to the loss function curve at a specific model size N0, where a steeper tangent signifies a larger 'a' value.
Transitioning from tangent slope to secant slope, the positive correlation between these slopes is highlighted, providing a way to relate the secant line slope to the model parameter 'a'.
In summary, the derivation of ScalingFilter involves a detailed analysis of how model and data scaling exponents impact the loss function, guiding the optimization process based on the slopes of tangent and secant lines.

A.3 Sampling vs. top-k selection
In contrast to previous sampling methods, ScalingFilter introduces a new approach by comparing sampling without replacement and top-k selection strategies. Here are the key findings:

Sampling methods typically balance quality and diversity of data, impacting downstream performance.
The introduction of a temperature term (τ) allows for adjusting sample diversity, where low τ values represent top-k selection and high τ values signify uniform sampling.
Experimental results show that top-k selection is the preferred method for ScalingFilter due to its simplicity, reference-free nature, and avoidance of noisy sampling strategies, ultimately enhancing downstream performance according to experimental results.
A.4 Ablation Study on Hyperparameters
The authors conducted ablation experiments on 1.3B models to assess the robustness of ScalingFilter by varying training hyperparameters. Specifically, they focused on the learning rate (default 2.5 × 10^(-4)) and global batch size (default 256), doubling their default values for the study.

Key points included:

Aimed to validate the robustness of ScalingFilter through ablation experiments.
Examined the impact of variations in learning rate and global batch size on model performance.
Conducted experiments on 1.3B models with hyperparameters adjusted from default values.
Results of the ablation study were obtained and analyzed by the researchers.