2. The Empirical Impact of Data Sanitization on Language Models.pdf


This paper explores how data sanitization, specifically the removal of personally identifiable information (PII), affects the performance of language models (LMs). Here's a breakdown of the key aspects:

1. The Problem: Privacy vs. Performance

Data Privacy is Crucial: LMs are often trained on massive datasets that may contain sensitive PII like names, addresses, and social security numbers. Exposing this information poses significant privacy risks and can have legal ramifications.

Data Sanitization as a Solution: A common practice to address this is data sanitization, where PII is identified and replaced with generic tokens (e.g., "John Doe" becomes "<NAME>").

The Potential Downside: The authors hypothesize that removing this information could harm the performance of LMs, as they rely on contextual clues, including PII, to understand and generate text.

2. Research Goal: Quantifying the Impact

The paper aims to empirically measure the impact of data sanitization on LM performance across various tasks and model sizes.

3. Methodology: A Multi-Faceted Approach

The researchers conduct experiments using two main approaches:

Fine-tuning Small Language Models:

They fine-tune smaller models (BART and GPT-2) on both original (unredacted) and sanitized versions of datasets.

They use datasets from the GLUE and LexGLUE benchmarks, covering tasks like:

Semantic Similarity: (QQP dataset)

Textual Entailment: (MultiNLI dataset)

Reading Comprehension: (Winograd Schema Challenge dataset)

Multi-class and Multi-label Classification

Extractive Question Answering: (SQUADv2.0 dataset)

Sentiment Analysis: (IMDB dataset)

They evaluate performance using different train/test splits: None/None (original/original), Redact/Redact (sanitized/sanitized), and None/Redact (original/sanitized).

Prompting Large Language Models:

They use larger, pre-trained models (Claude 3.5 Sonnet, Mistral 7B, and GPT-4o) and prompt them to perform tasks using a few-shot, chain-of-thought (CoT) approach.

They test on datasets commonly used to benchmark GenAI models:

DROP (reading comprehension)

GSM8K (math word problems)

BBH (Big-Bench Hard - a collection of challenging tasks)

SQuADv2.0 and IMDB are used from earlier study

They compare performance on original vs. redacted datasets, keeping the few-shot examples unredacted.

For high-impact tasks, they experiment with a "weaker" redaction strategy, leaving some task-critical entities unredacted.

They analyze the correlation between the number of redacted entities and performance degradation.

4. Key Findings: A Mixed Bag

Fine-tuning Results:

Minimal Impact on Most NLP Tasks: For most tasks (sentiment analysis, entailment, classification), training on redacted data resulted in only a small drop in performance (less than 2.5%). This suggests these models rely more on overall context than specific PII entities.

Significant Impact on Extractive QA: The SQuADv2.0 dataset, which involves extracting answers from a text, showed a large performance drop when trained on redacted data. This is likely because extractive QA relies heavily on identifying specific entities.

Misalignment Matters: Performance drops were more pronounced when training on unredacted data and testing on redacted data (None/Redact) compared to training and testing on redacted data (Redact/Redact).

Prompting Results:

Varied Impact Across Tasks and Models: The impact of redaction on LLMs varied significantly depending on the task and the model.

High Impact on Reasoning Tasks: Datasets like DROP, GSM8K, and some BBH tasks that require complex reasoning about entities saw substantial performance drops (up to 100% in some cases).

Low Impact on Others: Tasks like sentiment analysis (IMDB) and some BBH tasks were less affected.

Mistral 7B's Hallucinations: Mistral 7B exhibited unusual behavior, sometimes performing better on redacted data due to hallucinating values for the redacted tokens.

Weaker Redaction Helps: For high-impact tasks, not redacting task-critical entities significantly improved performance, suggesting that a more nuanced approach to redaction can be beneficial.

Non-Linear Correlation: The relationship between the number of redacted entities and performance was not always linear. Redacting randomly showed a linear decrease, while content-based redaction showed a less predictable pattern.

5. Proposed Mitigation Strategy: Content-Based Subsampling
Based on the finding that the degree of impact is not uniform across the dataset, they propose that subsampling the dataset by removing the data points with high number of PII entities might be a good strategy to have minimal impact on performance.
The paper shows that this strategy works well for SQUADv2.0 and GSM8k, but less for DROP because of the diverse nature of the PII entities and the length of the text.

6. Conclusions and Future Directions

Data sanitization's impact on LM performance is complex and task-dependent.

Extractive QA and complex reasoning tasks are particularly vulnerable.

Careful selection of entities to redact and content-based subsampling can mitigate negative impacts.

Future research should explore:

More sophisticated redaction techniques (e.g., pseudo-anonymization).

The impact of redaction on specific domains (e.g., medical, legal).

Better PII redaction tools.

In essence, the paper provides a valuable empirical study highlighting the trade-offs between privacy and performance when sanitizing data for language models. It emphasizes the need for a nuanced, task-aware approach to data sanitization to minimize its negative effects.