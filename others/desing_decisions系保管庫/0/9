9. How Much Data is Enough Data? Fine-Tuning Large Language Models for In-House Translation: Performance Evaluation Across Multiple Dataset Sizes



Introduction
Decoder-only large language models (LLMs) have significantly impacted the machine translation field by leveraging vast data to produce high-quality translations. LLMs like Llama 3 8B Instruct excel in adapting to translation tasks, yielding accurate, human-like outputs. However, generic LLMs may lack the needed nuances, tone, and terminology for specialized translations.

Translation memories (TMs) store previously human-translated segments and translations, proving invaluable for language service providers due to their ability to handle repetitive and organization-specific content effectively.
Integrating TMs with LLMs can enhance models to better grasp organizational requirements, leading to improved translation quality and quicker turnaround times, contingent on factors like TM quality, specificity, and quantity used for fine-tuning.
Fine-tuning LLMs with TMs for domain-specific translation has shown significant performance benefits, emphasizing the high quality and domain relevance of TMs.
The study investigates fine-tuning Llama 3 8B Instruct with TMs from a specific organization, exploring the impact of dataset sizes on translation quality to determine the most efficient return on investment.
Experimental evaluations in various translation directions and languages like Brazilian Portuguese, Czech, German, Finnish, and Korean aim to create tailored translation models that meet the diverse needs of companies, contrasting with generic LLMs.
Data
The raw dataset comprises technical manuals (TMs) from a software organization, covering knowledge base, mobile user interface, and reference materials. To prepare the datasets for training, the following steps were taken:

Five target languages (PT-BR, CS, DE, FI, and KO) were filtered to remove duplicates, source-copies, and segments exceeding 150 words.
Cleaning involved removing HTML tags, converting double spaces to single, and excluding rows with only dates, version numbers, or programming language references.
Random shuffling of rows was done to prevent temporal bias, sequence memorization, and avoid evaluation set bias.
An interlingual aligned dataset was created for all five languages by dropping rows without translations for any target language.
Test set filtering removed segments with over 75% similarity to ensure robust testing using Levenshtein distance and 5-gram-based similarity.
Size reduction of the test split from 1837 to 1353 segments was achieved through this filtering.
An additional 'full dataset' was compiled for each target language, including segments not meeting the inter-lingual alignment criteria.
Larger training sets were obtained, ranging from 107k (CS) to 223k (DE) examples, beyond the initial 14.7k aligned segments to explore the impact on results.
Model
The researchers utilize the Llama 3 8B Instruct model along with its associated tokenizer. The selection between the Instruct model and the base model is based on a detailed machine translation evaluation of various Llama 3 models using the Flores-200 dataset. Some key points include:

Despite addressing the opposite language direction (X to English), the comparable results between Instruct and the base model across five languages in the study suggest similar performance levels.
The baseline for evaluation involves test set metric results from the out-of-the-box Llama 3 8B Instruct model.
For efficient fine-tuning with 4-bit quantisation using Hugging Face Transformers, the researchers employ QLoRA and conduct fine-tuning on a high-performance cluster equipped with four A100-SXM4-80GB GPUs.
Leveraging Hugging Face tools, specifically the Supervised Fine-Tuning Training (SFTTrainer), optimized for fine-tuning language models like Llama. Fine-tuning operations on the largest dataset size take around 2.3 hours.
Prompting
The researchers utilized recommended parameters and model documentation during inference to generate translation outputs from both the baseline model and finetuned versions. Here's a summary of the prompting process:

Prompt Format and Special Tokens: Meta's Llama 3 documentation provides a recommended prompt format along with instructions for incorporating special tokens during training and inference.

Inference Process: The model received the prompt and the source segment to produce translations. Notably, this method is considered zero-shot as it did not involve examples in the prompt.

Structured Output: A JSON scheme {"translation": "string"} was included in the prompt to ensure a structured output. Additionally, the specific EOS token (< |end of text| >) was added following Meta's documentation guidelines for consistent formatting (cf. Appendix B).

Translation
To improve efficiency, the researchers converted both baseline and fine-tuned models to the CTrans-late2 format using 8-bit quantisation. Key points included:

Conversion of models to CTrans-late2 format
Utilization of 8-bit quantisation for improved efficiency
Provision of parameters for inference, detailed in Appendix C
Stopping Criteria and Post-processing
The authors observed overgeneration in early experiments, a topic further explored by recent research. To address this, they used "}assistant" as a stop token in their stopping criteria, significantly reducing the need for post-processing to achieve accurate translations. The approach minimizes post-processing efforts by setting a clear stopping criterion.

Key points included:
Post-processing involves extracting translations by eliminating specific prefixes and trailing characters.
Newline characters are replaced with spaces during the post-processing step.
Models trained on smaller datasets (1k and 2k examples) sometimes require additional cleaning due to inadvertent overgeneration of HTML tags like '
' and '
'.

Proper assessment of translation quality hinges on effectively managing such post-processing challenges.
Evaluation
The evaluation of the models in the study involves using various metrics to assess performance and training efficiency. Here are the key points:

Performance Metrics:

Metrics used for evaluation include BLEU, chrF++, TER via sacreBLEU, and COMET.
Multiple metrics are employed to ensure comparability with a diverse range of work and to provide insights into different performance aspects.
Training Efficiency:

The focus is on demonstrating the training efficiency of the PEFT finetuning method.
Emphasis is placed on the method's capability to approximate the model's translation abilities to the training data.
Metric Emphasis:

Special attention is given to automatic metrics like BLEU, chrF++, TER for measuring n-gram differences and edits.
Quality estimation aspect of COMET is considered for comparing inter-source languages and related research.
Comparison and Post-Editing:

Results are compared against a baseline model (Llama 3 8B Instruct model) and GPT 3.5.
Five professional translators are engaged to post-edit 100 translations from the best-performing model, providing valuable feedback through a questionnaire on translation quality.
Results and Discussion
The results presented in the table indicate a significant performance increase across all languages for datasets containing more than 5k segments compared to the baseline. Here are the key findings:

For the fully aligned 14.7k dataset, there was a notable improvement in performance:

BLEU score increased by 4.8 points, a relative increase of 17.42% on average over the baseline.
chrF++ and COMET scores also showed improvements, with increases of 7.1 and 16.9, respectively.
Additionally, TER decreased by 9 points.
The 100k+ datasets also showed consistent performance gains:

Demonstrating an average increase of 13.7 BLEU, 12.7 chrF++, and 25 COMET scores.
TER decreased to 15.5, indicating improved translation quality.
Comparison with GPT-3.5 revealed interesting insights:

While GPT-3.5 performed better in BLEU and chrF++ for DE and FI, the 100k+ datasets often outperformed GPT-3.5 in other languages and metrics.
The study highlights the effectiveness of fine-tuning midsized LLMs with domain-specific data:

Tailored fine-tuning can lead to competitive or superior results compared to using larger, more general-purpose models like GPT-3.5.
Small Dataset Deterioration
The researchers observed a decrease in translation quality when training models on smaller datasets (1k and 2k) compared to the baseline, despite a decrease in training and evaluation loss during training. Key points included in the analysis are:

Limited variety of examples in smaller datasets could lead to overfitting, causing models to perform well on training data but poorly on unseen test data.
Lack of diversity in smaller datasets may hinder the model's ability to capture the full range of linguistic nuances present in test data, affecting generalization beyond specific training examples.
Smaller datasets might make models more prone to noise like translation errors, leading to learning incorrect patterns and degrading performance on test data.
Quality evaluation using COMET-Kiwi showed consistent data quality across all dataset sizes for each language, indicating that the decline in performance is not due to lower data quality in 1k and 2k datasets.
To address this deterioration in small datasets, the researchers suggest:

Employing hyperparameter fine-tuning, such as dropout and regularization techniques, to prevent overfitting on small training sets.
Exploring adjustments to learning rate, batch sizes, and QLoRA hyperparameters to counter the observed deterioration.
Recommending a minimum of 5k examples to improve metrics for specific domains with limited training data.
Noting that larger datasets (above 10k examples) show performance recovery, surpassing the baseline model, and consistently improving across all metrics, with the best results seen on datasets larger than 100k examples.
Resource Level
The improvements in Korean (KO) performance after fine-tuning to 14.7k data make it comparable or even better than other languages, despite starting with lower baseline scores:

KO's COMET score improved from 36.5 to 84.3, aligning it closely with high-resource languages like PT-BR and DE.
Lower-resourced languages, like KO, show the highest relative gains, indicating significant performance enhancements.
The Llama 3 training dataset includes diverse non-English data, but the specific proportion of Korean data is undisclosed.
Despite the lack of detailed data on Korean in the training recipe, fine-tuning greatly benefitted its performance.
Observations on language performance reveal that:

PT-BR demonstrates superior performance with 14.7k and 100k+ datasets, even though it did not gain as significantly from fine-tuning as the lower-resourced language, KO.
Resource level significantly influences the performance of Large Language Models (LLMs) in Machine Translation (MT) tasks.
Human Evaluation
The human evaluation of the translation model highlighted some key points based on the qualitative comments obtained from translators:

Translators noted that the largest model struggled with ambiguity, particularly when segments lacked complete information, leading to the need for significant rework.
An example provided was a segment like "Get basic, step-by-step instructions to learn" lacking a final object, affecting the quality of translation.
In contrast to human translators who can use research or decision-making to resolve ambiguities with limited information, the model processes segments independently. This isolation prevents the model from accessing potentially clarifying context from surrounding segments. This evaluation sheds light on the model's performance challenges in practical translation scenarios.

Conclusions
Fine-tuning on Translation Memories (TMs) boosts Language Model performance in Machine Translation (MT) tasks. The researchers studied the impact of training set sizes on automatic metrics to optimize translation quality:

Fine-tuning on training sets larger than 5k examples improved translation quality significantly in 19 out of 20 language-training set size combinations.
Leveraging TMs helps models recognize and replicate previously translated segments, styles, and terminologies, aiding adaptation to specialized fields.
The study's use of narrower, hyper-specific training data may explain the promising results compared to broader-domain experiments like medicine.
Small business-friendly models, like the 8B parameter mode, allow for multiple adaptations for specific domains and languages with minimal investment.
Leveraging hyper-specific models for individual languages and narrow domains is under-explored but can maximize ROI with TMs and Large Language Models (LLMs) in MT.
Low-resource languages stand to benefit from models like Llama 38B, especially with substantial performance increases for high-resource languages like PT-BR and DE.
Notable performance gains, particularly for Korean (KO), indicate the effectiveness of leveraging TMs and LLMs.
Acknowledgment of dataset leakage and test data scraping challenges underscores the need for transparency and new test sets in AI research.
Encouraging disclosure of test sets by tech companies and recognizing leadership in dataset integrity by the Llama Team is vital for the AI community.
Future Work
The future work in this area suggests several potential avenues for improvement and exploration:

Introducing checkpoints during training to enable intermediate evaluation for visualizing a clearer learning curve and identifying performance dips and points of diminishing returns.
Obtaining a bespoke test set directly from the organization that owns the TMs, consisting of examples designed in-house to ensure originality and alignment with unique requirements and styles.
Using a bespoke and unseen test set to more accurately assess the performance of fine-tuned models in real-world contexts.
Further investigation into training hyperparameters across different dataset sizes, especially focusing on optimizing performance with smaller training sets (under 5k examples).
To enhance model performance on smaller datasets, strategies such as:

Modifying dropout rates to prevent overfitting.
Applying regularization techniques to improve model generalization.
Fine-tuning the learning rate for efficient convergence should be explored.