次は## 6. LoRA Hyperparameter Optimizationに進みましょう。繰り返しになりますが、gemma 2の2bモデルという規模間のモデルで且つソクラテス風の口調のチャットボット、を目指していた、というのがありまして、ソクラテス的な問答という問い力、のような深いレイヤーのものではなく、口調というわりとレイヤーの浅いものを目指していた、という大前提があるのですが、fine tuning時のLogaの調整については@train.py このコードのような方針をとったのですが、添付のコードの方向性を正当化するような論文はありそうですか？