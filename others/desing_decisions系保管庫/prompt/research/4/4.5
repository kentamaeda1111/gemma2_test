This paper introduces PromptIntern, a novel method for fine-tuning large language models (LLMs) that aims to significantly reduce inference costs. The core idea is to internalize the knowledge from repeated prompts (instructions, examples) directly into the model's parameters during training, thereby eliminating or greatly reducing the need for those prompts during inference. This is analogous to how a human intern learns a task â€“ initially needing detailed instructions, but eventually internalizing that knowledge and becoming proficient without them.

Here's a breakdown of the key aspects:

1. Problem: Fine-tuning LLMs often relies on lengthy prompts containing instructions, examples, and the actual query. These prompts inflate inference costs (time and money) due to the increased number of tokens processed. Existing prompt compression methods try to shorten the prompts without internalizing the knowledge into the model itself.

2. PromptIntern's Approach: PromptIntern tackles this by embedding prompt knowledge into the model's parameters during fine-tuning. It does this in a three-stage pipeline:

* **Preprocessing:** The input prompt (consisting of template/instructions, examples, and query) is preprocessed.
    * **Template Compression:** The repetitive parts of the instructions are compressed using a schedule that gradually reduces the amount of template information over training iterations. Two types of compression are used: instruction compression (for static instructions) and document compression (for lengthy technical details).
    * **Example Absorption:**  Relevant few-shot examples are selected (using a similarity metric) and gradually integrated into the model during training using another schedule to decrease the number of examples over time.

* **Progressive Fine-tuning:** The LLM is fine-tuned using the preprocessed prompts. The schedules for template compression and example absorption ensure that the model progressively learns to perform the task with less reliance on explicit prompt information.  The authors employ LoRA (Low-Rank Adaptation) for parameter-efficient fine-tuning.

* **Inference:**  After training, inference is performed using only the query part of the prompt. The model has internalized the knowledge from the template and examples, making the inference process significantly faster and cheaper.
Use code with caution.
3. Key Contributions:

* **Novel Prompt Internalization:**  A new approach to incorporate prompt knowledge into the model's parameters.
* **Progressive Fine-tuning Strategy:** A carefully designed training strategy that gradually removes explicit prompts as the model learns.
* **Empirical Evaluation:** Extensive experiments on NL2Code tasks demonstrate significant improvements:
    * Over 90% reduction in token usage.
    * 4.2x speedup in inference.
    * 88.3% reduction in monetary inference costs.
Use code with caution.
4. Experiments and Results: The paper extensively evaluates PromptIntern on three NL2Code datasets (MBPP, NL2F, NL2Bash) using various LLMs (both closed-source like GPT-3.5 and GPT-4, and open-source like Llama 2 and Mixtral). The results consistently show the superior performance of PromptIntern in terms of efficiency and comparable accuracy to direct fine-tuning methods that use full prompts. Ablation studies confirm the importance of both template compression and example absorption.

5. Limitations: While highly effective, PromptIntern still has some limitations:

It exhibits a slight performance gap compared to the methods using full prompts during both training and inference.

A formal theoretical analysis of the approach is still lacking.

Further evaluation on more diverse and complex NLP tasks is warranted.

In summary, PromptIntern proposes a significant advancement in efficient LLM fine-tuning by internalizing prompt information. It offers a promising solution to reduce the considerable costs associated with inference, particularly for resource-constrained applications. The paper provides a well-supported argument through comprehensive experiments and analysis.