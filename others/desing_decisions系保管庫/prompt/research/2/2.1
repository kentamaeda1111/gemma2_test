1. MIND: Math Informed syNthetic Dialogues for Pretraining LLMs
Syeda Nahida Akter, Shrimai Prabhumoye, John Kamalu, Sanjeev Satheesh, Eric Nyberg, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro  - Show less




This research paper introduces MIND (Math Informed synthetic Dialogue), a novel method for generating synthetic dialogue data to improve the mathematical reasoning capabilities of Large Language Models (LLMs). The core idea is to leverage a pre-trained LLM to transform unstructured mathematical text (from OpenWebMath) into structured, multi-turn conversations that enhance mathematical reasoning.

Here's a detailed breakdown:

1. Problem Addressed:

Current methods for improving LLMs often use synthetic data, but this data frequently lacks the step-by-step reasoning and structured problem-solving crucial for complex mathematical tasks. MIND addresses this gap by creating synthetic conversations that mimic the way humans solve problems collaboratively, focusing on detailed explanations and step-by-step reasoning.

2. MIND's Approach:

Data Source: MIND uses the OpenWebMath (OWM) corpus, a large dataset of mathematical web text. They use both a smaller subset (OWM-4B, 4 billion tokens) and the full dataset (OWM-14B, 14.7 billion tokens) in their experiments.

Conversation Prompts: Seven different conversational prompts (styles) are designed to elicit various types of interactions:

Two Students

Teacher-Student

Two Professors

Debater-Debater

Interviewer-Interviewee

Layman Know-All

Interactive Problem Solving

LLM Generation: A pre-trained LLM (LLaMA-3-70B-Instruct) generates multi-turn conversations based on these prompts and the input OWM text. The LLM is prompted to decompose the problem step-by-step, explore each step in detail, and reflect different levels of understanding (e.g., a layman asking clarifying questions).

Heuristic Filtering: A heuristic filter removes low-quality conversations (those shorter than 50 tokens, suggesting insufficient detail).

Continuous Pretraining: The synthetic conversations are combined with a portion of the original OWM data and used for continuous pretraining of a base 7B parameter LLM. This means the model is further trained, rather than completely retrained from scratch, using the generated data.

3. Experiments and Results:

The authors conduct extensive experiments to evaluate MIND, focusing on several aspects:

Impact of Conversation Styles: All conversation styles showed improvement over baseline models trained only on raw OWM data, particularly in GSM8K (a challenging math reasoning benchmark). The "Longest Conversation" (the longest conversation generated for each context, irrespective of style) consistently produced the best results. However, the "Two Professors" style performed poorly, possibly because the assumed shared expertise between the two participants led to less detailed explanations. This highlights the importance of knowledge gaps in the conversations for effective learning.

Data Scaling: Experiments using both OWM-4B and OWM-14B showed that the benefits of MIND extend to larger datasets. A model trained with conversations from the smaller OWM-4B outperformed one trained on the much larger OWM-14B on some benchmarks, demonstrating the quality of the synthesized data.

Comparison with Rephrasing and Other Methods: MIND outperformed simple rephrasing techniques (which preserve meaning but don't generate conversations) and a comparable method, DEEPSEEKMATH, which used a different approach to create mathematical data.

Impact of Data Integration: The research emphasizes the importance of restructuring raw data for better performance, rather than simply adding synthetic data to the existing raw corpus.

4. Key Conclusions:

MIND generates high-quality synthetic conversations that effectively improve LLMs' mathematical reasoning ability.

Structuring raw text into conversations, especially those that leverage knowledge gaps between participants, is crucial for generating effective training data.

Continuous pretraining with synthetic data generated by MIND outperforms training with raw data alone, even when the raw data is substantial.

MIND is scalable, efficient, and effectively uses readily available LLM technology.

In summary, the paper makes a strong case for the benefits of using synthetic conversational data, especially for tasks that require multi-step reasoning, like mathematics. The results highlight that carefully designed prompts and the structure of interactions significantly affect the quality of generated data and subsequently the performance of the trained LLMs. The method is scalable and uses readily available technology, making it a promising approach for improving LLMs in various domains.