
3. Synthesizing Post-Training Data for LLMs through Multi-Agent Simulation
Shuo Tang, Xinyue Pang, Zhou Yu Liu, Bohan Tang, Rui Ye, Xiaowen Dong, Yanfeng Wang, Siheng Chen

This paper introduces MATRIX-Gen, a novel framework for synthesizing high-quality post-training data for Large Language Models (LLMs). The key innovation is the use of multi-agent simulation to generate diverse and realistic text-based scenarios that reflect real-world human needs. This contrasts with previous methods that relied on pre-defined prompts or seed data, which often lacked the realism and diversity of real-world interactions.

Here's a breakdown of the key components and contributions:

1. Multi-Agent Simulation (MATRIX):

Real-world grounded agents: The system uses 1000 agents, each initialized with real-world human profiles (anonymized to protect privacy) and assigned realistic life goals. This ensures that agent behaviors and interactions mimic real-world scenarios.

Structured communication: To prevent an overwhelming number of interactions, agents are grouped based on profile similarity (homophily). Each group has a "modulator" (an LLM) that manages communication within and between groups, ensuring efficient and realistic interactions.

Scenario generation: Agents interact within this structured environment, pursuing their goals and generating text-based actions. These actions constitute a diverse set of realistic scenarios.

2. Scenario-Driven Instruction Generation (MATRIX-Gen):

User requirements integration: MATRIX-Gen takes the scenarios generated by MATRIX and incorporates specific user requirements to synthesize high-quality instruction-response pairs. It can generate different types of datasets:

MATRIX-Gen-SFT: Supervised fine-tuning data with simple and diverse instructions.

MATRIX-Gen-DPO: Preference tuning data with complex and specialized instructions.

Domain-specific datasets: Data tailored for specific tasks like coding or safety.

Controllability: By adjusting the generation prompts, users can control the type and complexity of the generated data, allowing for fine-grained control over the post-training process.

3. Post-Training:

The synthesized datasets generated by MATRIX-Gen are then used for post-training LLMs. The paper demonstrates this process using Llama-3-8B-Base.

Key Findings and Contributions:

Superior performance: Post-training Llama-3-8B-Base with only 20,000 instruction-response pairs generated by MATRIX-Gen outperformed Meta's Llama-3-8B-Instruct, which was trained on over 10 million pairs, on several benchmarks (AlpacaEval 2 and Arena-Hard). This highlights the high quality of the synthesized data.

Realism and diversity: The multi-agent simulation approach generates diverse and realistic scenarios, leading to more effective post-training data.

Controllability: The framework offers a high degree of control over the types of data generated, allowing for specialized training on specific tasks.

Scalability: The structured communication mechanism enables the simulation of a large number of agents, making the system scalable.

In essence, the paper presents a novel and effective approach to synthesizing post-training data, significantly improving the efficiency and effectiveness of LLM fine-tuning. The use of multi-agent simulation represents a significant advancement in data generation for LLMs, offering a path towards more efficient and effective model training. The appendix contains further details on experimental setups, additional results on different models (Qwen-7B), and examples of generated scenarios and instructions.