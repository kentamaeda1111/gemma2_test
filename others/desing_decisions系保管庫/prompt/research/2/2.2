2. Rethinking Data Synthesis: A Teacher Model Training Recipe with Interpretation
Yifang Chen, David Z. Zhu

Introduction
The authors discuss the shift towards utilizing instruction design, particularly seen in OpenAI's methods, for data curation in large language models post-training. However, challenges such as high labor costs exist with traditional human-generated instruction collection methods. To overcome this, recent strategies have focused on synthetic data generation using advanced teacher language models, starting with a small set of example tasks and iteratively enhancing prompts. Despite these advancements, current approaches often rely on standard supervised fine-tuning models, leading to limitations like prioritizing accuracy over novelty in question generation and producing incomplete questions in chat formats. This prompts the authors to investigate the need for specialized models dedicated to data synthesis.

Key points included:

Shift towards using instruction design for data curation in large language models.
Challenges with traditional human-generated instructions due to labor costs.
Recent focus on synthetic data generation using advanced teacher language models.
Limitations of current approaches in question generation and model design.
Investigation into the necessity of specialized models for data synthesis.
The authors delve into two crucial aspects that differentiate data synthesis from standard language model training: the impact of prompt masking and the optimization of training data. They highlight the significance of learning from prompts for generating better synthetic data and reveal that selecting a smaller subset of training data can often lead to more effective synthetic data. Leveraging these insights, the authors introduce NOMAD (No Masking Data Synthesizer), a novel approach designed to address these challenges, demonstrating superior performance in generating synthetic data compared to baseline methods, even with limited training samples.

Key points included:

Importance of prompt masking in improving response quality and generating synthetic data effectively.
Finding that smaller, carefully selected training sets can enhance synthetic data quality.
Introduction of NOMAD as a solution to optimize data synthesis.
NOMAD's outperformance of baseline methods in generating synthetic data with both small and large training sample sizes.
The authors also propose evaluating synthetic data quality based on "relevance" and "novelty" to offer insights into optimal training strategies, providing a deeper understanding of the effectiveness of their approach.

Key points included:

Proposal to assess synthetic data quality based on relevance and novelty criteria.
Aim to enhance understanding of optimal training strategies for synthetic data generation.
Problem Statement
The researchers aim to generate additional synthetic data, X synthesis, by utilizing a pretrained student model, M s, and a pretrained teacher model, M t, along with an existing high-quality instruction dataset, X train. The main objectives and considerations in this problem statement include:

The goal is to propose new methods to train the teacher model M t using the dataset X train for generating supplementary synthetic data, X synthesis.
The effectiveness of the proposed methods will be evaluated by training the student model M s on a combination of the original X train and the newly generated X synthesis, comparing its performance with a model trained solely on the original X train.
Past research has mainly focused on developing prompting techniques to query a teacher model already fine-tuned with instructions, leveraging the external data used to train the teacher model. In contrast, this work emphasizes working with only the pretrained version of the teacher model to maintain strict control over the instruction data employed.
Our Strategy
The authors' main strategy involves three key stages: M synthesis training, X synthesis generation, and filtering, as outlined below:

M Synthesis Training
No-Prompt-Masked Training: Differs from standard language model training by exposing the model to complete instruction-response pairs rather than just response parts.

Helps the model learn high-quality prompts and ensure prompt-response alignment.
Enables simultaneous generation of prompts and responses, enhancing model training efficiency.
Proper Training Set Size: Balances relevance and novelty by selecting a subset of a large dataset for superior synthetic data, challenging the idea of using maximal data.

X Synthesis Generation
Adopts a prompting strategy where only "User: " is input, allowing autonomous generation of prompts and responses.
Post-processes data to maintain only first-round conversations, ready for integration with prompt-engineering approaches.
Simple Filters
Implements filters for content quality maintenance and coding-type data generation.
Utilizes repeated words removal and coding filters to enhance data quality and performance.
These filters are computationally inexpensive and significantly boost performance without consuming excessive time.
Setup
The experimental setup features:

Teacher and Student Models: The Llama3-8B model is chosen as the backbone for both the teacher model (M synthesis) and the student model (M s).

Training Data: Existing training data, or a subset of it, can be utilized for training the data synthesis model (M synthesis) and the final model (M policy) when combined with previously generated X synthesis.

Data Collection: Two settings are considered in the main results:

A 15k randomly sampled subset
The full 300k dataset from the TULU v2 data collection
All data follow a unified template: "User: [prompt content] Assistant: [response content]"
M Synthesis Training
The researchers explore prompt-masked training and no-prompt-masked training, using 2 epochs consistently for all data sizes to ensure equal exposure of each training data point to the model.

Training Parameters:
2 epochs used consistently
Equal exposure of training data points to the model
X Synthesis Generation
They generated 30,000 raw data entries using the prompt strategy from Section 3, resulting in 25,000 valid chat-formatted entries.

M s Training
The researchers employ prompt-masked training for fine-tuning the final policy model, following a standard Self-Training Framework (SFT) approach. They explore two training strategies: equal epoch and equal computational budget settings, ensuring fair exposure of samples to the learner.

Key points included:

4 epochs are used for 15K X train samples, while 2 epochs are utilized for 300K X train samples.
In cases with low training samples (15K X train), the baseline model is run for 8 epochs to maintain a comparable computational budget.
Baseline and evaluation metrics focus on downstream tasks like TriviaQA for knowledge, TruthfulQA-generation for truthfulness, BBH-NOCOT-FS and BBH-COT-FS for reasoning, GSM8K, and IFEval for instruction-following.
The model trained solely on X train serves as the baseline, with performance measurements conducted under both equal epoch and similar budget settings. This highlights the potential enhancement by incorporating X synthesis data into the training process.
Main Result
The researchers found that their NOMASKEDFILTERED method outperforms the baseline by approximately 1.5% when using just 15K samples for both M synthesis and the student model M s, and supplementing the original training data X train. Key points included:

Notable improvements were seen with over 4% gain in TriviaQA and over 2% in GSM8K.
X synthesis from prompt-masked training, regardless of filtering, led to performance degradation when combined with the original dataset.
The study showed that no-prompt-masked training for M synthesis is crucial for maintaining performance when combined with the original dataset.
Main Result
When considering a larger 300K X train dataset, the findings presented some surprising results. Key points included:

Training M synthesis using all 300k data actually led to a downgrade in performance compared to the baseline.
Data generated from the 15K no-prompt-masked trained M synthesis was the only one that outperformed the baseline in this setting.
Property of the Synthetic Data
The authors introduce a similarity score, NormSim, to measure the relationship between the synthesized data X and the original 300K TULU dataset X TULU using the all-mpnet-base-v2 model for extracting embeddings. Here is a summary of the properties of the synthetic data discussed in this section:

Similarity Score (NormSim):
NormSim is used to assess how close a generated synthetic data point x is to any target sample from the original dataset.
It focuses on the similarity of x to target samples rather than checking for the same coverage as TULU.
Balancing Relevance and Novelty:
The goal is to have more data points concentrated around the median similarity, striking a balance between novelty and relevance of the generated data.
This approach is supported by observations in the figures presented.
Training on Synthesized Data vs. Mix of Synthesized and Original Data:
The study shows that training only on the synthesized data does not yield performance comparable to training on a combination of the synthesized and original data (X synthesis + X train).
It is demonstrated in App.B.2 that incorporating X train as a reference in training is necessary for achieving better performance.
Limitations
The study on data synthesis model training had limitations:

Conducted at small scales with a 7B-parameter teacher model, a 3B-parameter student model, and less than 300K samples in the data pool.
Potential generalization of this method to larger models requires further exploration in future research.
Moreover, the study:

Focused on the general multi-task TULU dataset but excluded coding data due to methodological constraints.
Future research must assess the performance of these methods across diverse data domains.
References provided in the section:

Chen and Nan Duan, 2023: "Agieval: A humancentric benchmark for evaluating foundation models. Preprint, arXiv:2304.06364."
Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou, 2023: "Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911."
Model Training
The model training process in the study involved consistent choices for learning rate and batch size to ensure uniformity and optimize training performance. Specifically:

Learning rate was set at 2e−5 for all model training iterations.
A fixed batch size of 128 was utilized throughout the training process.
These standardized parameters aimed to support effective model convergence and performance enhancement during the training phase.

A.2 Data Generation
For data generation, the researchers employed the prompt strategy outlined in Section 3, using specific parameters:

Used generation temperature set at 1
Selected top_p = 0.9 for X syn train of 15K to avoid generating low-quality data
Experimented with X syn train at 300K, testing both top_p values of 0.9 and 0.7, with results detailed in Appendix C.1
Observed that slight variations in hyperparameters resulted in slightly different performance outcomes but did not conflict with the main conclusions of the study
A.3.1 Generation-free evaluation metrics
The researchers introduce various evaluation metrics to assess model performance without specific question generation. Here are the key points included:

TriviaQA: A dataset with question-answer-evidence triples, providing valuable distant supervision for answering questions.

TruthfulQA_gen: Involves generating short answers for questions and comparing them to true and false reference answers using the RougeL metric.

BBH: Consists of 23 challenging tasks (BIG-Bench Hard) to evaluate a model's reasoning ability, outperforming human-raters on some tasks. Includes chain-of-thought and non-chain-of-thought versions with 3 shot examples.

GSM8k: A benchmark for grade school math problems to evaluate multi-step mathematical reasoning using natural language descriptions and basic arithmetic operations.

IFEval: Focuses on assessing Large Language Models (LLMs) ability to follow natural language instructions in a standardized and reproducible manner.

IFEval Benchmark: Emphasizes "verifiable instructions" for evaluation, such as word count requirements and keyword mentions, reporting prompt-level loose accuracy for evaluation.

A.4 Problem of IFEval
The researchers found that when using X train as 300K TULU, the baseline accuracy was 34.38, lower than when X train was 15K TULU, indicating data ineffectiveness in instruction following. This could potentially impact the methodology study.

Key Points:

Baseline accuracy was 34.38 with X train at 300K TULU, lower than with 15K TULU.
Original data may be less effective for instruction following.
Data ineffectiveness could introduce confusion in the study of methodologies.
A.5 Filters
The researchers utilize rule-based filters in two parts for processing the data, focusing on code removal and the elimination of repeated words. Here are the key details:

The filters are designed based on a rule-based approach outlined in Section 3.
The primary functions of the filters include removing irrelevant code segments from the data.
Additionally, the filters aim to identify and eliminate repeated words within the dataset to enhance data quality.
A.5.1 Coding Samples
The researchers highlight a limitation in their data synthesis methods when it comes to generating high-quality coding samples, despite their effectiveness in general tasks. Specifically, the coding samples often lack the necessary context to address the problem adequately.

Data synthesis methods are effective for general tasks but struggle with coding samples
Coding samples frequently lack the context needed to solve the problem efficiently
Incorrect Outputs Due to Problem Difficulty
The authors highlight a common issue with generating prompts, where incorrect outputs can occur due to the difficulty of the problem. In the provided example, a prompt is generated without any context for the underlying problem, leading to potentially flawed or irrelevant outputs.

Difficulty in generating accurate outputs can arise when prompts lack essential context.
The lack of problem context in prompts can result in misleading or incorrect outputs.
This challenge underscores the importance of providing sufficient information in prompts for reliable results.
User
The "f" variable in the Java code holds significance in modifying the output. It is crucial for understanding how the code manipulates the final result.

The significance of the "f" variable in the Java code lies in its role in modifying the output.
Understanding how "f" is used is essential for comprehending the code's functionality and impact on the output.
When gathering information about a specific group of people by asking about their favorite passion, verifying the accuracy of the responses becomes a key challenge.

Verifying the accuracy of responses to personal questions like "What is your favorite passion?" poses a significant challenge.
Ensuring the reliability of collected information is essential for drawing meaningful conclusions.
Navigating and adjusting one's behavior have profound implications for various aspects of life, including social interactions, industry trends, and user preferences.

The ability to navigate and adapt behavior is crucial for understanding social dynamics, industry shifts, and individual preferences.
This skill is fundamental for interpreting social patterns, identifying trends, and catering to user needs effectively.
Building social devices and systems based on gathered information can enhance collaboration, learning, and audience engagement.

Utilizing information to develop social tools and platforms can improve teamwork, knowledge sharing, and attracting the right audience.
Creating systems that leverage gathered data can facilitate better cooperation, learning processes, and user engagement.
Providing specific examples can aid in comprehension and organization. Demonstrating how to set up a new integration enhances clarity and structure.

Offering concrete examples enhances understanding and organization of concepts.
Presenting a sample integration setup using the "#.6 library # D'Wilvi '1 _2'" can help users stay focused and organized in their development tasks.
Assistant:
The variable "f" in this code calculates the body frequency range by dividing the difference between the minimum and maximum pulse rate by 5. This range is then adjusted by multiplying it with "f" to vary the frequency.

Questioning individuals about their favorite collaborations may yield inaccurate responses, leading to unreliable information due to personal biases or misunderstandings.

A decision support system aids teams in making informed choices by leveraging an adaptable framework for creating and managing analytics and algorithms to assess team progress and enhance performance.

Team members can control and share tasks, data, and decision policies through the system, ensuring informed decision-making aligned with internal requirements.

The system enables accessing insights and recommendations by creating queries associated with decision policies, supporting decision-making with data analysis tools.

Designed to offer a holistic view of team progress and support decision-making, the system centralizes decision support and policies.

The United Nations prioritizes global well-being through peace promotion, law enforcement, aid provision, and international law compliance, supported by the U.S. as a member nation.

Utilizing out-of-distribution (OOD) data for synthesis, even though initially deemed of low quality, can still enhance performance, especially when compared to X train data alone.

Evaluation of the effectiveness of X synthesis should reference X train data, ensuring compatibility with high-quality datasets beyond the original source.

A.5.2 Long Conversations and Repeated Strings
Long conversations are susceptible to a decline in quality, often exhibiting repeated words towards the end. This repetition issue is illustrated in the example provided. The data generated from prompt-masked training can result in the user behaving like an assistant, leading to role switching between the user and the assistant. The assistant may attempt to prolong the conversation or provide feedback on the user's "response."

Key points:

Extended conversations may experience a degradation in quality, characterized by repeated terms.
The text highlights instances where users in the training data take on the assistant role, influencing conversational dynamics.
The assistant role interchange between the user and the assistant can impact the flow and content of the conversation, potentially affecting its quality.
More Results on Multi-Choice Metrics
The authors previously demonstrated the effectiveness of their methods on free-generation metrics in Section 4.2. However, when evaluating multi-choice metrics, they found that the synthetic data generation approach they proposed is not as effective.

Key points:

The methods showed advantages in free-generation metrics but faced challenges in multi-choice metrics.
Synthetic data generation methodology was found to be less effective when assessing multi-choice metrics.
C.1 Details on Evaluation Metrics
The evaluation metrics in this study involve multi-choice tasks where learners select the choice with the maximum digits from a fixed set of candidates (e.g., A, B, C, D). The specific metrics considered are:

MMLU:
This metric assesses language models' instruction-following capabilities and general intelligence.
AGIEval Benchmark:
Designed to evaluate instruction-following abilities and general intelligence in language models.
Includes a wide range of tasks resembling real-world cognitive challenges such as professional certification questions, academic tests, and complex problem-solving scenarios.
Structured to not only test the model's comprehension of instructions but also its ability to apply knowledge contextually.
Results
The authors compared the performance of two synthetic methods against a baseline under specific conditions:

The results, as depicted in a table, indicate that neither synthetic method showed remarkable performance improvements over the baseline.
Notable enhancements were seen in free-generation metrics under a particular setting, but the synthetic methods did not exhibit substantial performance gains