5. Faithful Persona-based Conversational Dataset Generation with Large Language Models
Pegah Jandaghi, XiangHai Sheng, Xinyi Bai, Jay Pujara, Hakim Sidahmed  - Show less

Introduction
The authors highlight the importance of understanding individuals' stories to effectively engage with them, especially in conversational systems. They introduce personas as abstract user representations capturing a person's background and preferences, commonly used for human-centered design in various fields like marketing, system design, and healthcare. The existing Persona-Chat dataset is criticized for limitations like small size, static dialogues, irrelevant utterances, and contradictory persona attributes.

Key Points:
Personas are crucial for creating engaging conversations in various domains.
Current persona-based datasets lack depth and flexibility.
Challenges in creating persona-based datasets include labor-intensiveness, updating for new topics, and quality concerns.
Existing datasets involve manual data collection processes.
Updating datasets with new topics is complex.
Faithfulness is introduced as a criterion for aligning participants' utterances with their personas.
The authors propose a new framework for generating dynamic persona-based conversational datasets that address these limitations. Their framework leverages unsupervised Language Models (LLMs) to reduce human labor, automates persona generation, expansion, and update processes, and enforces quality criteria such as ensuring faithfulness to make dialogues more human-like.

The framework comprises:
User Generation
User Pairing
Conversation Generation
The Conversation Generation process involves generating plausible user profiles, matching users for conversations, and producing realistic conversations between them. Here are the key points included in this section:

The framework created Synthetic-Persona-Chat (SPC) dataset with 5k user personas and 20k dialogues, which can be adapted for specialized datasets like user music profiles.
An unsupervised approach using Language Models (LLMs) is proposed to create and expand specialized personas.
A framework based on LLMs is introduced to evolve datasets with different objectives.
A Generator-Critic architecture is followed, where the Generator outputs conversation candidates and the Critic evaluates them based on predefined policies.
The Generator and Critic, both based on LLMs, iteratively improve conversation quality by selecting and adding top candidates to the dataset.
The conversation evaluation involves a Mixture of Experts (MoE) approach with three types of experts: General Conversation Quality, Faithfulness, and Toxicity.
The General Conversation Quality experts evaluate based on FED metrics, while the Faithfulness expert ensures consistency with user profiles and the Toxicity expert filters harmful conversations.
The generated conversations are refined by filtering out unfaithful and toxic ones, with the best conversations selected through a majority vote among experts.
Overall, the Conversation Generation component utilizes LLMs, policies for data generation, and an iterative process involving the Generator and Critic to enhance conversation quality by incorporating feedback from multiple expert evaluations.

Definitions
In defining persona-based dialogue generation, the researchers formally describe persona attributes, user profiles, and conversations:

Persona Attributes: Described as sentences defining a user, forming the user's persona like "I like ice cream" or "My native language is Tamazight".
Persona Categories: Groups of persona attributes organized by semantic features, each associated with a query like "What is your occupation?" to aid in describing users.
Persona Attribute Structure: Attributes can overlap or form a hierarchy, with some being specific cases of others.
User Profile: Defined as a set of consistent persona attributes describing a user without contradictions.
Persona-based Conversation: Conversations where at least one persona attribute from each user profile can be inferred, supporting persona-based dialogue generation models.
Faithfulness Criteria
Model Criteria: Introduces a faithfulness criterion to align conversation utterances with user profiles.
Constraint: Utterances should not lower the likelihood of a user's persona attributes.
Faithful Conversation: One that doesn't decrease the inference probability of persona attributes in user profiles.
Formalization: A conversation is faithful if, for all persona attributes in the user profiles, the inference probability given the conversation is equal to or higher than without assumptions.
Method
The method presented in the paper focuses on generating persona-based conversations with minimal human intervention. The process involves three main steps:

User Generation:

Augmenting seed persona attributes Π 0 to create an expanded set Π e.
Generating user profiles based on the expanded persona attributes.
User Pairing:

Pairing user profiles to establish conversational partners.
Conversation Generation:

Using an iterative approach to generate quality conversations between paired user profiles.
This method aims to create authentic and engaging conversations by systematically progressing from initial persona attributes to the final conversational output.

User Generation
The User Generation component comprises two sub-components:

Content-Based User Generation:

Utilizes user browsing history and preferences to generate user profiles.
Profiles are created based on content similarity and user interactions.
Collaborative User Generation:

Involves collaborative filtering techniques to recommend items based on user interactions.
Users are grouped based on behavior and preferences, allowing for personalized recommendations.
User Profile Construction
The process of constructing user profiles in this research involves several key steps and considerations:

Persona attributes are initially generated using prompts and then expanded iteratively by selecting random attributes.
A Natural Language Inference (NLI) model is employed to ensure the consistency of the constructed user profiles.
User profiles are incrementally built by sampling attributes and adding eligible ones that meet consistency and non-redundancy criteria.
Consistency and redundancy assessments are done using an NLI model and persona attribute clustering techniques.
The NLI model used in this study is based on T5 and trained on the TRUE dataset.
Each user profile is created by selecting random persona attributes and assessing them for contradictions with existing attributes using the NLI model.
Attributes are evaluated for similarity to existing attributes to avoid redundancy, and only non-contradictory and non-redundant attributes are added to the user profile.
The process continues until the user profile contains 5 persona attributes.
Persona Expansion
The authors introduce an unsupervised method to expand a set of initial persona attributes (Π 0) into a larger super-set (Π e) without human intervention. The process involves two main steps: query induction and persona bootstrapping, leveraging a bootstrapping technique and a prompted Language Model (LLM).

Query Induction:
Persona categories and associated queries are identified in the initial persona attribute set (Π 0).
Queries are then expanded, including those related to unobserved persona categories, leading to a diverse set of personas.
The process relies on the assumption that if two persona attributes belong to the same category, there exists a query that can output both attributes.
Persona Bootstrapping:
Starts with the initial persona attribute set (Π 0) and category-specific query set (Q) to generate new persona attributes iteratively.
Prompts are created by combining persona attributes and queries, guiding the LLM to generate new persona attributes.
New attributes are added to the persona set after ensuring they are not too similar to existing ones to avoid duplicates.
The iterative process continues until a total of 5k persona attributes are generated.
The combination of query induction and persona bootstrapping steps enables the creation of specialized personas across various domains, expanding the initial seed persona attributes into a significantly larger set through an automated and data-driven approach.

User Pairing
In the User Pairing component, the researchers aim to enhance the engagement of conversations by identifying potential user pairs with common personas. Here's a summary of this section:

The researchers assign a similarity score to pairs of user profiles (U1, U2) based on their semantic similarity, utilizing BERT for representing the user profiles.
The similarity between two user profiles (U1 and U2) is determined by the number of shared persona categories in their profiles.
User pairs (U1 and U2) are matched if their similarity score exceeds a predefined threshold of 2.
Evaluation
The evaluation section delves into the assessment of the dataset generation framework and the resultant dataset named Synthetic-Persona-Chat (SPC), developed using a language model fine-tuned with 24 billion parameters. Here are the key points covered:

Comparison with Persona-Chat (PC) Dataset:

SPC is compared against the widely used PC dataset across various criteria.
Persona Quality Evaluation:

The quality of the generated personas is assessed as a starting point for the evaluation.
Evaluation of SPC:

Automatic metrics and human assessment are employed to evaluate the SPC dataset comprehensively.
Additional Analysis:

Toxicity and diversity aspects of SPC are further analyzed in appendices B.1 and B.1 for a more detailed understanding.
Evaluation of the Expanded Personas
The evaluation of the expanded personas was conducted on two seed datasets: Wikipedia and Persona-Chat. Here are the key points from the evaluation:

The personas from Wikipedia were derived by crawling the pages of the 1,000 most active contributors and extracting user boxes.
The expansion of both datasets was performed using the researchers' framework.
The evaluation of the expanded persona attribute sets was done using automatic metrics.
Findings from the Evaluation:

Comparison between original and expanded persona sets:
The expansion increased the number of persona attributes in SPC by 119% while maintaining original persona categories.
Persona attributes were expanded by 71% compared to Persona-Chat.
Length of generated persona attributes:
The new persona attributes in SPC were 107% longer, indicating greater detail and specificity.
Similar results for Wikipedia persona set:
The method increased the number of persona attributes by 108%.
Persona categories saw a 140% increase, and persona attribute lengths grew by 45%.
These findings highlight the effectiveness of the method in expanding and diversifying persona sets, demonstrating significant improvements in attribute numbers, categories, and attribute lengths for both datasets.

Next Utterance Prediction
A persona-based conversation can enhance next utterance prediction models by incorporating information about speaker personas. The study evaluates the impact of including speaker personas on Transformer-based models for next utterance prediction. Here are the key points:

The experiment compares models' performance with and without user personas as input.
The Transformer (Ranker) model shows higher performance in SPC compared to PC, indicating more intricate and coherent conversations in SPC.
Performance improvements vary between IR Baseline and generative models based on dataset complexities.
The presence of personas affects model performance differently in SPC and PC due to explicit vs. implicit persona reflections in conversations.
Bidirectionality is higher in SPC conversations when personas are considered in the models.
Profile extraction task evaluation highlights better performance in SPC compared to PC, especially when speakers and targets are different.
Human Evaluation
The researchers compared the quality of conversations generated by their framework with those in Persona-Chat by conducting various human evaluations:

Human experiment conducted to detect AI-generated content using a Turing test.
Three human evaluators annotated each pair of conversations, with majority voting for the final annotation.
Reduction of losing rate by 48% in SPC from Iteration 1 to Iteration 3, dropping below 10%.
91% of SPC conversations judged as human-like compared to human-generated conversations.
Only 8.04% of conversations for new personas in Iteration 3* deemed artificial, indicating SPC's realism.
Evaluation of conversation faithfulness showed consistency above 75% across iterations with minimal variation.
Impact assessment of Language Model (LLM) size: Variant with 540 billion parameters (LLM2) showed a 5% advantage in the Turing test over a smaller model.
Multi-iteration approach outperformed single-iteration with LLM2, demonstrating cost-effective high-quality conversation generation.
Related Work
Large Language Models (LLMs) play a crucial role in various NLP tasks, including data augmentation, generation, and evaluation:

Early work utilized LLMs to expand small labeled datasets by generating additional text data.
Subsequent studies explored creating datasets solely through LLMs without human data.
Different LLMs have been assessed for their performance in data augmentation tasks.
Research in conversational dataset generation:

Focuses on the structure of conversational data and how LLMs can generate synthetic training data for dialogue models.
Persona-based conversations, exemplified by Persona-Chat, have been significant in NLP research.
Persona-Chat dataset and evaluation metrics are widely used benchmarks in persona-based conversation generation.
Use of PersonaChat dataset by various models like DialoGPT, BlenderBot, and PersonaChatGen:

PersonaChatGen automates creating persona-based conversations from Persona-Chat using LLMs.
Challenges in synthetic dataset generation:

Ensuring conversation quality in terms of data faithfulness, fidelity, diversity, and consistency is essential.
Works have focused on creating high-quality training datasets and filtering components for conversation dataset generation.
Evaluation challenges of conversational datasets:

Interactive evaluation of conversations with LLMs has been introduced to address evaluation complexities.
Conclusion and Future Work
The researchers introduced a new framework that leverages Large Language Models (LLMs) to produce high-quality persona-based dialogues, leading to the formation of a dataset called Synthetic-Persona-Chat with 20,000 conversations. This dataset is intended to facilitate the advancement of persona-aware conversational agents, particularly in crafting domain-specific multisession dialogues for specific task-oriented engagements.

Key points included:

The framework generated 20,000 conversations for Synthetic-Persona-Chat.
The dataset aims to support the development of persona-aware conversational agents.
Future work may involve generating specialized datasets beyond persona-based conversations.
The proposed Generator-Critic approach is adaptable to various contexts, indicating its potential for creating different specialized datasets.
Limitations
The paper outlines several limitations related to the iterative process used over Large Language Models (LLMs) for dataset generation:

Computational Resources Requirement:

The method outlined in the paper requires significant computational resources and access to an LLM for dataset generation.
Quality Bound by LLM:

The dataset quality is constrained by the limitations of the LLM used, as critics evaluating the quality also rely on the same LLM. This leaves the refinement of critics for future work.
Limitation on Realistic Conversations:

Realistic conversations that lack high quality are challenging to generate. Assumptions of fluency in both parties, perfectly consistent conversation flow, and absence of unexpected events (like interruptions or connection losses) restrict the realism of the conversations.
Incorporating Less Tangible Persona Traits:

Difficulty arises in integrating less concrete persona traits, such as humor, or user attributes that necessitate multiple conversation sessions to manifest in the dataset.
Ethics Statement
The researchers acknowledge the ethical implications of creating datasets with specific objectives, noting the potential for misuse in training biased or hateful models. However, these datasets and models can serve as valuable filters in practical applications.

Key points included:

The authors conducted human experiments using Amazon Mechanical Turk, adhering to platform guidelines to safeguard the rights of human raters.
Participation in the study was voluntary, with raters being informed of their rights at the outset.
Amazon Mechanical Turk implemented security measures to protect raters and prevent the disclosure of their Personal Identifiable Information (PII).
The researchers ensured fair compensation above standard wages to prevent exploitative practices.
Tools were employed to filter out toxic conversations from the final dataset to maintain ethical standards, with details and sample removals provided in Appendix B.1.
A Dataset Generation Framework
The researchers detail their synthetic dataset generation framework called Synthetic-Persona-Chat, which utilizes a Large Language Model (LLM) with 24 billion parameters. For the dataset generation, they employ top-k sampling with k = 40 for decoding and consistently set the temperature value to 0.7 across all components.

Key Points:

The dataset generation framework is named Synthetic-Persona-Chat and relies on a Large Language Model with 24 billion parameters.
Top-k sampling with k = 40 is used for decoding during the dataset generation process.
A fixed temperature value of 0.7 is maintained throughout different components of the framework.
The user and conversation generation components are elaborated on in subsequent subsections for a more in-depth understanding of the framework.
A.1 User Generation
The user generation component in the framework involves expanding the persona attribute set and creating realistic user profiles through the following steps:

Persona Expansion:
Initially, persona categories are identified in the persona attribute set Π 0.
Queries associated with these categories are generated, and bootstrapping is done to create a query set Q.
A bottom-up agglomerative clustering approach employing Scikit-learn is used to identify persona categories, with each persona represented using a BERT-based representation.
Clusters are formed by combining two clusters if their similarity exceeds a predetermined threshold of 0.1 based on inter-cluster average cosine similarity.
The clustering process continues until no pair of clusters is more similar than the specified threshold.
3 instances of persona attributes are sampled from each cluster to construct an initial query set Q 0, which is then expanded using bootstrapping.
LLM Prompting:
The Language Model (LLM) is prompted using templates provided in section 3 and Table, where 5 instances from available queries are sampled at each step.
This prompting process is repeated for 100 steps to generate realistic user profiles.
Templates:
Prompt templates used in this component can be referenced in the provided Table.
User Profile Generation
The process of generating a user profile involves several key steps as outlined in the paper:

At each iteration of the process, a random persona attribute, denoted as π ′, is selected.
To determine redundancy, the BERT representation of persona attributes is utilized.
The similarity between the new attribute, π ′, and each attribute in the user profile is computed.
If the similarity exceeds a predefined threshold (e.g., 0.9), π ′ is considered redundant and is not added to the user profile. This similarity comparison is based on cosine similarities of BERT representations.
To assess consistency, an NLI model is employed.
For each persona attribute in the user profile, a negated version, denoted as ¬π, is generated.
The NLI model is then used to check if the negated attribute is inferred by the new candidate attribute, π ′, or vice versa.
If either inference is true, the selected attribute is deemed inconsistent with the user profile and is not included in the profile.
A.2 Conversation Generation
In the framework discussed, a critic is implemented using prompting an LLM, adopting a mixture of experts approach where each expert prompts the LLM to evaluate specific policies in candidate conversations.

Key Points:
The framework incorporates a group of experts to regulate conversation quality, with their performance assessed using a baseline dataset.
The baseline dataset, FED, comprises 125 human-annotated instances assessed at the conversation level, with experts evaluated based on correctly ranked conversation pairs.
Results indicate experts are over 80% accurate in discerning better conversations within pairs, enhancing conversation quality control.
Additionally, a toxicity expert and a persona faithfulness expert are included in the critic, each utilizing specific prompt templates, enhancing diverse evaluation criteria.
The persona faithfulness expert leverages LLMs' in-context learning, incorporating human-curated examples of faithful and unfaithful conversations in the instructional prompts.
For specific examples of faithful and unfaithful conversations used in the prompts, refer to the provided Table.

Synthetic-Persona-Chat
The Synthetic-Persona-Chat dataset comprises 20,000 conversations, with an average of 11.8 turns per user. The researchers provide an example conversation from this dataset in a table for reference. A comparison is drawn between Synthetic-Persona-Chat and Persona-Chat across various aspects.

Characteristics Assessment:

The authors analyze Synthetic-Persona-Chat (SPC) using automatic evaluators that eliminate the need for human intervention.
Different dimensions of SPC are evaluated using automated methods.
Human Evaluation Experiment:

A subset of the Synthetic-Persona-Chat dataset undergoes a human evaluation experiment.
Human assessors participate in evaluating the quality and aspects of conversations within this subset.
B.1 Automatic Evaluation
The section begins with a thorough analysis and comparison of SPC and PC across various aspects, focusing on toxicity and diversity by leveraging existing tools. The researchers then delve into experiments that gauge the effectiveness of SPC in tasks like predicting the next utterance and extracting profiles.

Key points:

Comprehensive analysis and comparison of SPC and PC.
Assessment of toxicity and diversity using off-the-shelf tools.
Experiments to evaluate SPC's performance in next utterance prediction and profile extraction tasks.
Evaluation of SPC conversations quality through LLM-based methods.
Toxicity Analysis
The researchers conducted an analysis on the toxicity of the conversations generated in their study using an online tool called Perspective. Here are the key points from the toxicity analysis section:

The toxicity analysis was performed at the final iteration of Single-Player Conversations (SPC) using the Perspective online tool.
A notable decrease in strongly toxic or profane conversations was observed throughout the iterations of generating Single-Player Conversations (SPC), thanks to the built-in toxicity filter of the employed Large Language Model (LLM).
Queries for personas were generated by the LLM: personas with "LLM" as the source were bootstrapped, while those with "human" as the source were created by sampling persona categories and prompting the LLM.
Conversations identified as strongly toxic were significantly reduced in SPC, with at most three toxic or profane conversations in each set, compared to more than 50 samples found in the original dataset.
The fraction of conversations with medium profanity and toxicity in SPC was four times lower than similar conversations in the original dataset across all iterations.
The released dataset excluded any conversation marked as strongly toxic by the Perspective tool.
Sample toxic conversations are provided in a table for reference.
Diversity Analysis
Hierarchical topic modeling is employed to evaluate the topic diversity in Single Persona Conversations (SPC) in comparison to Personalized Conversations (PC). Here's a breakdown of the diversity analysis section:

The analysis involves comparing conversations in SPC with equivalent personas in PC to ensure a fair assessment.
The number of topics at different levels of the topic tree is presented in a table, where the first level represents the broadest topic categories.
At the initial level, both SPC and PC exhibit similar levels of topic diversity.
Deeper within the topic tree, a slightly lower diversity is observed in SPC conversations compared to PC.
LLM-based Quality Evaluation
The researchers utilized LLM-based conversation quality evaluators to compare the quality of SPC and PC by using human-curated prompt templates for metrics such as consistency and fluency. Here are the key points regarding the LLM-based quality evaluators used in the study:

LLM-Eval Prompt:

This prompt provided a unified evaluation schema, assessing conversation quality across various dimensions like fluency in a single model call.
Minimal changes were made to the original prompt templates when using this evaluator.
GPT-Score:

Leveraged the capabilities of LLMs, such as zero-shot instructions, to score texts based on human descriptions of quality criteria and valid score ranges.
Included prompt templates populated with descriptions of criteria and their respective valid score ranges.
G-Eval Framework:

Introduced a framework utilizing LLMs with a chain-of-thought approach to evaluate the quality of generated natural language outputs.
Prompted LLMs with criterion descriptions to generate evaluation steps and score the output based on those steps.
Evaluation Process:

Probability of each permissible score was considered as the model output, reflecting the probability distribution of scores assigned by the LLM.
The reported output was the expected value of the score distribution by the LLM.
The main findings from the quality evaluation using LLM-based evaluators showed that SPC consistently outperformed PC across various dimensions evaluated in the study. Notably, when using GPT-Score, each assessed criterion exhibited an average improvement of at least 23 points when comparing SPC and PC.

B.2 Human Evaluation
The researchers conducted a human evaluation of their method using a crowdsourcing platform, which involved two key assessments:

Turing Test:
Random selection of 200 pairs of users from a platform.
Annotators were shown conversations from the platform (original) and the synthesized conversations (generated by the method) and asked to identify the synthetic conversation.
Results of the Turing test were analyzed, indicating the losing rate of the synthetic conversations and the inter-rater agreement assessed through Fleiss' Kappa, falling into fair to moderate agreement.
Faithfulness Study:
Annotators were presented with conversations and a list of persona attributes.
Annotators had to select the persona attributes they would infer from the conversation.
Distractor persona attributes were included to increase the task difficulty.
Distractor persona attributes were created through negated personas (e.g., negating persona attributes like "I like vegetables" to "I don't like vegetables") and random personas selected from user profiles in other conversations.
LLM-Eval All
The section outlines the requirements for formatting output as a JSON instance conforming to a specified schema, emphasizing the need for well-formatted instances. The JSON schema includes properties like content, grammar, relevance, and appropriateness, each with their respective score ranges. The output should be scored on a continuous scale between specified minimum and maximum scores.

Output should adhere to a specific JSON schema with properties such as content, grammar, relevance, and appropriateness, each with defined score ranges.
Well-formatted JSON instances are crucial, as demonstrated by the example provided.
Scores for the generated content must fall within a continuous scale ranging from a minimum to a maximum score.
The section also addresses the evaluation of dialogue content for consistency in responses between two users based on a conversation. Users are prompted to assess the consistency of information provided by the users throughout the dialogue.

Users are asked to determine the consistency of responses provided by users in a conversation based on the information shared.
The evaluation task involves assessing whether the responses align in content throughout the conversation.
G-Eval Coherence
The G-Eval Coherence section outlines the evaluation task involving user personas and conversations. Here's a breakdown of the key points:

Coherence Rating Task Instructions:

An essential task is to rate the coherence of conversations between given pairs of user personas.
Participants need to assess the coherence of the conversation based on a predefined metric.
The quality of utterances collectively determines the coherence rating, aligned with the structure and coherence standards from the Document Understanding Conference (DUC).
Guidelines for Coherence Assessment:

Emphasis is placed on well-structured and well-organized conversations that progress logically from one utterance to the next.
The conversation should not merely present a collection of related information but should evolve into a coherent discussion on a specific topic.
Evaluation Process:

Participants are required to comprehend and analyze the provided conversation between the pair of user personas in the dataset.
Special focus is given to generating persona attributes that contradict the users' original personas to assess how well the conversation adapts to differing attributes.
Each evaluation entry includes 8 user persona attributes as options, which consist of 4 real persona attributes and 4 distractors.
Human annotators' precision in discerning these attributes is evaluated and serves as a measure of conversation faithfulness.
By following these detailed instructions and evaluation criteria, participants can effectively assess the coherence and quality of conversations between user personas, contributing to a better understanding of conversational dynamics and adaptability.

C Ablation Studies
The authors conducted ablation studies to assess the significance of specific components in their framework. They first examined the persona expansion module's influence and then evaluated the contribution of each expert in the Critic mixture.

The importance of individual components in the framework was assessed through ablation studies.
Initially, the impact of the persona expansion module was analyzed.
The study then focused on evaluating the contribution of each expert integrated into the Critic mixture.
C.1 Persona Expansion
The researchers evaluate the query-based persona expansion module's significance introduced earlier. In this assessment, experiments are conducted on two datasets: Wikipedia and PC, showing results in Table C.1.

Key points included:
Persona expansions without the query set are labeled as 'Wikipedia-0' and 'PC-0', both undergoing 100 iterations each.
PC-0 generates 4,477 new persona attributes, 20% less than PC, while Wikipedia-0 produces 4,742 attributes, 50% less than Wikipedia+.
Differences in newly generated persona attributes are notable, indicating the query-based expansion's efficacy in maintaining persona set diversity.
PC-0 and Wikipedia-0 have fewer persona clusters by 6% and 49% respectively, highlighting the impact of query-based expansion on cluster diversity.
The average persona attribute length in PC-0 is 11.38 tokens, 28% less than SPC, implying less detailed attributes. In contrast, 'Wikipedia-0' shows similar attribute lengths to 'Wikipedia+'.
C.2 Conversation Quality
The section explores the impact of experts within the Critic by removing each one to generate datasets using one iteration of the framework. A comparison is then made between these datasets and the output of the initial iteration of SPC. The evaluation metrics from section B.1 are utilized to assess the conversation quality.

Key points included:

Experts within the Critic were individually removed to observe the effect on performance.
Datasets generated without the experts were compared to the output of the first iteration of SPC.
Evaluation metrics like LLM-Eval, GPT-Score, and G-Eval were used to assess the impact.
Exclusion of the experts led to poorer performance based on most criteria:
LLM-Eval: 3 out of 4 criteria showed worse performance.
GPT-Score: 4 out of 6 criteria displayed decreased performance.
G-Eval: 3 out of 5 criteria indicated a drop in performance.
C.3 Faithfulness
The researchers conduct an experiment on faithfulness by removing the faithfulness critic and generating a dataset for comparison with SPC. The comparison involves human annotators through a Turing Test and an LLM-Evaluator. Key points included:

The faithfulness critic is removed to assess its impact on creating conversations.
A dataset is created for comparison with the SPC dataset.
The comparison involves human annotators, a Turing Test, and an LLM-Evaluator.
An experiment comparing conversations created with and without the faithfulness expert is conducted for 200 conversations.
Without the faithfulness critic, precision decreases from 78.0% to 66.0%, indicating its role in eliminating conversations with contradictory information about user personas.
The recall also decreases from 36.0% to 23.0%, suggesting a higher reflection of personas in conversations when the faithfulness expert is present.
LLM-Evaluator
The authors utilize an LLM for evaluating the entire dataset, expanding the comparison beyond a specific subset. The faithfulness of conversations generated initially without the specialized expert is shown in Table.

Key points included:

An LLM serves as the annotator for the entire dataset evaluation.
The faithfulness evaluation of the first iteration conversations is illustrated.
Templates for the LLM-based annotators are detailed in Table, particularly under "LLM-Faithfulness" as the evaluator.
The LLM used for annotation differs from the one employed in dataset creation, being gpt-3.5-turbo.
C.4 Next Utterance Prediction
The researchers conducted experiments comparing next utterance prediction models trained on the SPC dataset against models trained on datasets created without certain experts. Here are the key findings from the analysis:

IR Baseline Performance:
The IR Baseline method achieves its highest performance (39% hit@1) when the FED critic is absent during dataset creation, emphasizing conversation quality over persona-related aspects.
Transformer Ranker Performance:
The Transformer Ranker model, known for understanding intricate concepts, reaches its peak performance (13.9% hit@1) when none of the experts are absent, highlighting the importance of including both FED and the Faithfulness expert in the model architecture.
Impact of FED in Generative Models:
For generative models, the absence of FED has the most significant impact, leading to performance declines (e.g., -12% hit@1, -9% BLEU, -10% ROUGE), showcasing FED's critical role in enhancing the model's generative capabilities.