

1. LoRA Done RITE: Robust Invariant Transformation Equilibration for LoRA Optimization
J. Yen, Si Si, Meng Zhao, Felix Yu, Sai Surya Duvvuri, Inderjit S. Dhillon, Cho‐Jui Hsieh, Sanjiv Kumar








INTRODUCTION
Low-Rank Adaptation (LoRA) is a method for fine-tuning Large Language Models (LLMs) that reduces memory requirements and helps prevent overfitting in scenarios with limited data by incorporating trainable low-rank matrices alongside frozen pretrained weights.

LoRA introduces a low-rank matrix Z (represented by the product of two rank-r matrices A and B) added to the frozen weight matrix W of an LLM.
Recent research has explored various adaptations and enhancements to the original LoRA algorithm to improve its performance.
Standard optimizers applied to LoRA can produce non-transformation-invariant updates, leading to inefficiencies during training.
The imbalance in optimizer updates between the two LoRA factors (A and B) is a common issue, even with techniques like varying learning rates.
To tackle the challenge of ensuring transformation invariance in LoRA optimization, a new method called LoRA-RITE (Robust Invariant Transformation Equilibration) is proposed.
LoRA-RITE uses a transformation-invariant preconditioner on the low-rank side to enforce transformation invariance without significant computational overhead.
The proposed optimizer maintains this property when incorporating first and second moments, which is vital for the practical effectiveness of adaptive optimization methods.
The contributions of the paper include:

Introduction of LoRA-RITE, the first adaptive matrix preconditioning optimizer for LoRA that ensures transformation invariance.
The optimizer achieves minimal overhead in memory and time compared to first-order optimizers, especially with small LoRA ranks compared to matrix dimensions.
Empirical evaluations across different datasets and models demonstrate the superior performance of LoRA-RITE, such as achieving a 55.50% accuracy rate on the GSM8K dataset with a Gemma 7B IT model, outperforming Adam by 7.13% and Lamb by 4.86%.
Transformation Invariance for LoRA Optimization
The section introduces the concept of transformation invariance in LoRA training. It highlights that many current optimizers, when used with LoRA, do not exhibit this crucial property, resulting in inefficient learning outcomes.

Key points:

Transformation invariance is crucial for optimizing LoRA training effectively.
Existing optimizers applied to LoRA often lack transformation invariance.
The absence of transformation invariance can compromise the efficiency of learning processes in LoRA.
Definition of Transformation Invariance
The authors introduce LoRA, a method involving a low-rank matrix addition to weight matrices for fine-tuning, achieved through learning LoRA factors A and B. However, different parameterizations during optimization can result in suboptimal updates to the weight matrix Z. To address this, the concept of transformation invariance is proposed:

Transformation Invariance requires optimizers to produce the same updates to fine-tuned weights for equivalent LoRA factorizations.
This ensures that regardless of the LoRA factor scaling, the updates generated by the optimizer should remain consistent.
A special case, known as scalar scale invariance, is a weaker version that demands updates to remain equivalent even when the LoRA factors are scaled up or down by a constant factor.
Despite its importance, it is revealed that most commonly used optimizers fail to satisfy even the scalar scale invariance requirement for LoRA.
EXISTING OPTIMIZERS ARE NOT SCALAR SCALE INVARIANT
The researchers demonstrate that widely used optimizers like gradient descent and Adam lack scalar scale invariance when applied to LoRA optimization. Here are the key points included:

Gradient descent and Adam are not scalar scale invariant, leading to issues when dealing with extreme scaling factors.
The analysis involves rewriting gradients using chain rule and examining discrepancies in scale terms.
In the case of gradient descent, a scale difference of 1/s^2 exists between different terms, showing lack of scale invariance.
For Adam, similar scale discrepancies are observed, indicating that adaptive updates do not resolve the scalar scale invariance issue.
Optimizers such as Adagrad, RMSProp, and Shampoo also exhibit a lack of scale or transformation invariance, highlighting a common limitation across existing optimizers.
BENEFITS OF TRANSFORMATION INVARIANCE
Transformation invariance is crucial as it leads to more efficient feature learning, offering several advantages and outcomes:

Efficient feature learning, essential for LoRA as the network width increases, ensures that the updates to the matrix Z = AB ⊤ can be decomposed into three parts, where one part is typically negligible due to its dependence on the square of the learning rate.
For efficient feature learning to occur, both δAB ⊤ x and AδB ⊤ x must be of magnitude O(n 0) = O(1) concerning the network width n, where x represents the input embedding. This ensures that the scale neither explodes nor diminishes as the network width grows.
Conventional optimizers often fail to satisfy efficient feature learning, as demonstrated by how the weight norm for factor B fluctuates significantly while the weight norm for factor A remains relatively stable, as depicted in a figure.
A transformation-invariant optimizer, utilizing the same update rule for both A and B, guarantees efficient feature learning under mild assumptions, as proven in Theorem 1 in the Appendix.
In practical scenarios, when training LoRA with standard optimizers, it's common to observe that one of the LoRA factors updates effectively while the other remains nearly unchanged. This imbalance is due to the lack of scalar scale invariance, where the initial scales for the two factors may vary significantly (one initialized from 0 while the other from Gaussian random).
Our Proposed Optimizer
The authors introduce a new algorithm that prioritizes transformation invariance and outperforms previous LoRA optimizers in various practical tasks. The key points of the proposed optimizer are:

Emphasizes transformation invariance
Demonstrates substantial enhancements over prior LoRA optimizers in empirical tasks
DIAGONAL PRECONDITIONING IS NOT ENOUGH FOR TRANSFORMATION INVARIANCE
In this section, the researchers demonstrate that diagonal preconditioning alone is insufficient for achieving transformation invariance in optimization algorithms. Here are the key points included:

The LoRA factors A ∈ R^m×r and B ∈ R^n×r are updated using vec(•) to list matrix elements in column-major order and a symmetric positive definite preconditioning matrix P ∈ R^mr×mr.

Methods like Adam and Adagrad employ diagonal matrices for preconditioning (P), while Shampoo and CASPR use non-diagonal matrices to precondition the update.

Considering LoRA pairs (A1, B1) and (A2, B2) with an invertible matrix R involved in a matrix generalization scenario.

Gradient derivation for LoRA factors using a specific equation (7) is discussed, particularly focusing on the case when n = m = 1.

In the special case of n = m = 1, the impact of equivalent LoRA pairs (A1, B1) and (A2, B2) along with their preconditioners P1, P2 in satisfying certain conditions is examined.

If transformation invariance is upheld, finding diagonal preconditioners P1, P2 satisfying conditions can be challenging due to the choice of an adversarial R.

Matrix preconditioning is deemed essential for achieving transformation invariance, as diagonal preconditioning could fall short in certain scenarios.

Achieving Transformation Invariance
To achieve transformation invariance, the LoRA factors A and B are decomposed into their orthogonal bases and magnitudes, denoted as U A and U B obtained through polar decomposition. The gradients of A and B, ∇A and ∇B, depend on both basis and magnitude, leading to the introduction of "unmagnified gradients" solely reliant on column spaces of A and B to ensure invariance to transformations. This forms the core of the algorithm's capability for transformation invariance.

The proposed method leverages adaptive preconditioning techniques like Adam, showcasing superiority over non-adaptive methods.
The method utilizes unmagnified gradients for adaptive matrix preconditioning, focusing on one-sided preconditioning for efficiency.
The update rule for LoRA-RITE is symmetric for A and B; the update rule for A involves an adaptive preconditioning mechanism similar to Adagrad, ensuring consistency across equivalent LoRA pairs.
Incorporating second-moment adjustments involves accumulating second moments based on varying bases at each step to prevent loss of information, ensuring monotonically increasing preconditioner values. The final update rule accounts for both first and second moments, maintaining transformation invariance across all equivalent LoRA pairs.

The unmagnified preconditioned step, when including the second moment, can be adjusted into Exponential Moving Average (EMA) form.
The first moment is also adjusted using a projection matrix to account for basis changes. The final update rule maintains transformation invariance across all LoRA pairs.
The time and space complexity of the proposed algorithm, when r is significantly smaller than m, n, is comparable to first-order methods like Adam:

Computational costs per iteration include QR-decomposition for matrices of certain dimensions, matrix inverses, and roots, with an overall complexity of O(mr² + nr² + r³).
The method's memory cost matches that of Adam, showing a slight overhead compared to Adam, which becomes negligible due to the small r value.
THEORETICAL ANALYSIS
The theoretical analysis of the algorithm within the online optimization framework involves several key points:

In online convex optimization, decisions are made iteratively within a convex decision set, with potentially adversarially chosen convex loss functions revealed after each decision.
The regret of the algorithm up to a certain step is defined, and the analysis bounds the first-order term and connects it to the loss function's convexity.
Due to the structure of LoRA, the loss functions are not convex, leading to a direct bound on the first-order term.
Assumptions are made on the convex decision set for fine-tuned weights of each layer, including constraints on the norm and gradient.
Theoretical convergence analysis considers a simplified scenario similar to Adagrad and presents a theorem showing convergence to a stable solution or movement in non-value-changing directions.
An additional assumption on smoothness of a preconditioner leads to a stronger convergence result.
By aligning with one-sided matrix Adagrad, the analysis introduces variables to replace quantities with their unmagnified counterparts.
The regret bound of LoRA-Rite is compared to that of one-sided matrix Adagrad, highlighting advantages in the face of imbalanced magnitudes of LoRA factors.
Imbalanced factors in LoRA are common and can impact performance, with the analysis providing insights into the method's empirical success.
RELATED WORK
The authors discuss various optimizers and their limitations in achieving transformation invariance when applied to LoRA:

Adaptive Optimizers:

Optimizers like Adagrad, Adam, and RMSProp utilize accumulated second moments to scale updates but lack transformation invariance when applied to LoRA.
Higher-Order Preconditioners:

Shampoo approximates the full second moment matrix using a Kronecker product, incurring high memory overhead and computational cost.
These methods lack transformation invariance and complexity exceeds the proposed method.
Layer-wise Adaptive Optimization:

Methods like LARS and Lamb dynamically adjust update norms based on weight matrix norms but still lack transformation invariance.
Variants of LoRA:

Parameter-efficient fine-tuning (PEFT) methods like DyLoRA, DoRA, and DeepLoRA adapt LoRA for more efficient training, which is orthogonal to the proposed work.
Recent Approaches:

LoRA+ introduces two different learning rates for LoRA weights, adding an extra hyperparameter.
Riemannian gradient descent is proposed for LoRA optimization, offering transformation invariance but lacks momentum and adaptivity, later combined with element-wise Adam to create ScaledAdam.
Experimental Results
The authors evaluated the LoRA optimizer against other optimizers using various datasets, including the Super-Natural Instructions dataset and four standard LLM benchmarking datasets. Here are the key points:

Optimizers Compared:

Adam: Widely used default optimizer for LoRA fine-tuning.
LoRA+: Adam with different learning rates for A and B, setting B's learning rate 4 times larger than A's.
ScaledAdam: Variant of Adam designed for LoRA optimization.
Shampoo: Adaptive matrix preconditioning method.
Lamb: Variant of Adam normalizing updates for each layer based on parameter norms.
LoRA-RITE: Proposed transformer invariant optimizer.
Experiment Details:

Searched for the best learning rate from 2 * 10^(-6) to 2 * 10^(-2).
Used rank r = 16 for LoRA in most experiments.
Conducted experiments on Gemma 2B, 7B, and mT5-XXL using TPUs.
Results on Super-Natural Instruction Dataset:

Evaluated on a variety of NLP tasks.
Demonstrated superior performance across classification and generation tasks compared to Adam.
Exact match accuracy for classification and ROUGE-L score for generation tasks used for evaluation.
Results on Other LLM Benchmarking Datasets:

Achieved best performance with LoRA-RITE on all datasets, with Lamb as the second best.
Evaluated on datasets like HellaSwag, ArcChallenge, GSM8K, and OpenBookQA.
Ablation Study:

Tested different LoRA ranks and model architectures, showing consistent performance with rank 16.
Successfully applied LoRA-RITE to various architectures, including mT5-XXL.
Training Speed Comparison:

LoRA-RITE showed comparable training speed to Adam, being only 8% slower on Gemma 2B.
Showed better performance than Shampoo despite a lower preconditioner update frequency due to LoRA-specific preconditioning approach.
CONCLUSIONS
The authors highlight the limitations of current LoRA optimization techniques, noting their lack of transformation invariance, which can lead to inefficient feature learning and suboptimal solutions. To address this, the researchers introduce a novel, transformation-invariant optimization algorithm that rivals Adam in terms of time and memory usage. Key points included:

The proposed algorithm consistently outperforms existing LoRA optimizers across various datasets and models, demonstrating higher accuracy.
Despite the improvements, it is essential to recognize the inherent limitations of LoRA itself, such as reduced representational power compared to full fine-tuning, which might slightly impact performance.
Determining the optimal rank selection for balancing efficiency and accuracy remains a non-trivial challenge in practical applications.
Furthermore, the paper references works by various researchers such as Rowan Zellers, Ari Holtzman, Fangzhao Zhang, and others, reflecting the broader context and related research in the field of optimization algorithms for fine-tuning foundation models.