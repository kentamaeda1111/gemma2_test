4. A Comprehensive Analysis of the Effectiveness of Large Language Models as Automatic Dialogue Evaluators
Chen Zhang, Luis Fernando D'Haro, Yiming Chen, Malu Zhang, Haizhou Li  


Introduction
Evaluation in dialogue system research presents challenges, with human evaluation considered the gold standard but costly and not always reproducible. To address this, researchers have proposed automatic evaluation measures, classified into reference-based and reference-free metrics. While reference-free evaluators show improved correlations with human judgment compared to reference-based metrics, they still lack perfect alignment and struggle with generalization to new dialogue data.

Recent advances in large language models (LLMs), especially instruction-tuned models like OpenAI ChatGPT/GPT-4, show promise as automatic dialogue evaluators due to their strong language understanding capabilities. However, their evaluation scope is limited to proprietary models, raising the need to explore open-source foundation models for evaluation effectiveness. The paper aims to comprehensively assess 30 recent LLMs across various dimensions and introduce annotations to existing meta-evaluation datasets for broader evaluation.

Key points included:

Evaluation challenges in dialogue systems push for automatic evaluation measures.
Reference-based and reference-free metrics are used for automatic evaluation.
LLMs offer potential as automatic dialogue evaluators but have limitations in scope.
The study evaluates 30 LLMs across multiple dimensions and introduces annotations to meta-evaluation datasets.
Adversarial perturbation strategies are introduced to probe LLM robustness.
Different ensemble strategies' impact on dialogue evaluation performance is studied.
Preliminaries Meta-Evaluation
The researchers utilize 12 meta-evaluation datasets, divided into 6 at the turn level and 6 at the dialogue level. Each level assesses five quality aspects/dimensions, including context relevance, understandability, specificity, interestingness, coherence, engagingness, and more. They evaluate the overall response and dialogue quality based on these dimensions.

Datasets: 12 meta-evaluation datasets used, split into turn and dialogue levels, each evaluating various quality aspects.
Annotation Process: Instead of crowdworkers, GPT-4 is employed to fill up missing annotations, aiming to save costs and accelerate the process.
Supplementary Tool: GPT-4 complements human evaluation, offering scalability and cost efficiency, albeit potential biases, suggesting the need to address these biases in future research.
Annotation Task: GPT-4 performs annotations with specific settings (temperature and top p values) repeated five times to simulate multiple annotations per data instance.
Inter-Annotator Agreement: GPT-4's inter-annotator agreement is calculated using pairwise Pearson correlations, aiming for a high agreement rate.
Instruction-Tuned Models: Derivatives of vanilla foundation models (Alpaca, Vicuna, Chimera) are fine-tuned to mirror advanced LLMs like ChatGPT and GPT-4.
Alignment Techniques: Techniques like instruction-based supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) are used to align models with human task-solving abilities. Readers are referred to technical reports for detailed LLM information.
Dialogue Evaluation with LLMs
The authors used an explicit scoring procedure to evaluate dialogues or responses from proprietary models like ChatGPT, Palm-2 Bison, and GPT-4. Here's a summary of how this evaluation was carried out:

Explicit Scoring Procedure:
Proprietary models were prompted to provide multi-dimensional ratings of dialogues or responses.
The same instruction template was used for all proprietary models.
The scoring process was repeated five times for each model, and the average score was used.
For open-source LLMs, a different approach was taken due to their weaker instruction-following abilities:

Implicit Scoring Procedure:
Focus was on output probabilities related to the label words "Yes" and "No" generated by the LLMs.
Probability of "Yes" was normalized to obtain the corresponding score.
Greedy decoding was applied to ensure a deterministic generation process for the LLMs.
To evaluate response relevance for open-source LLMs, a specific instruction prompt input was used, and the instruction template was adapted for different LLMs to match their tuning process.

Multi-Dimensional Correlation Analysis
The researchers conducted a comprehensive analysis of Pearson correlation scores across different dimensions at both turn and dialogue levels, highlighting the impact of instruction-tuned models compared to vanilla models:

Instruction-Tuned vs Vanilla Models:
Instruction-tuned models generally outperform vanilla models.
Alpaca-7B outperforms LLaMA-7B significantly, showcasing improvements of 25.1% at the turn level and 36.4% at the dialogue level.
Tulu-13B surpasses LLaMA-13B by 9.1% and 18.2% on average, indicating the effectiveness of alignment techniques like instruction-based finetuning.
LLaMA vs Other Open-Source Families:
LLaMA models exhibit strong performance, with LLaMA-2-7B and XGen-8K-7B leading in different dimensions.
Performance variations are observed among different vanilla models due to variations in model sizes, pretraining corpora, and training strategies.
Impact of Instruction Data:
Models leveraging diverse instruction data, like Tulu-13B and Chimera-inst-chat-13B, display superior performance.
The quality and diversity of instruction data significantly influence model performance, emphasizing the importance of using varied datasets for finetuning LLM-based evaluators.
Performance Across Dimensions:
Models generally excel in dimensions like relevance, coherence, engagement, and overall quality.
However, performance declines slightly in dimensions such as interestingness and understandability at the turn level, and diversity at the dialogue level.
The findings suggest that leveraging LLMs for automatic dialogue evaluation could benefit from focusing on specific dimensions like interestingness, understandability, and diversity in future research and model development.

Performance of GPT-4
The performance of GPT-4 was evaluated in terms of turn-level and dialogue-level abilities compared to other top-5 Language Models (LLMs). Here are the key findings:

Results were presented and compared in a table with other LLMs, showcasing GPT-4's strengths and weaknesses.
When evaluated in various dimensions, GPT-4 excelled in relevance, coherence, and overall quality at both turn and dialogue levels.
In comparison to the second-best LLM (ChatGPT), GPT-4 outperformed by a significant margin of 0.085 at the turn level and 0.042 at the dialogue level.
It was noted that while GPT-4 performed well, it still fell short of perfect correlations (>0.8) on average, indicating that there is room for improvement in automatic dialogue evaluation.
The study suggests that future research should focus on enhancing the conversation understanding capabilities of language models like GPT-4 to address the challenges in dialogue evaluation.
Deviation Between GPT-4 and Human Preferences
The researchers assess the differences in evaluating Language Model Models (LLMs) using GPT-4 annotations compared to human annotations. Here's a breakdown of the analysis:

Comparison of ranking results for 32 LLMs by GPT-4 annotations versus human annotations
Quantification of deviation through Spearman correlation between the two ranking lists
Greater Spearman correlation indicates closer alignment in model rankings
Analysis conducted on the FED dataset, which includes human annotations for all dimensions
Results exhibit minimal deviation between GPT-4's evaluation and human evaluation (Spearman correlation >0.85 agreements) in most dimensions, except for response specificity.
Ensemble Analysis
The section explores two ensemble strategies to improve correlation with human evaluations in language models (LLMs):

Dimension-Wise Ensemble:

Averaging scores from LLMs assigned to different dimensions.
This approach, especially in Chimera-inst-chat-13B and Baize-v2-13B at the dialogue level, shows over 10% improvement compared to direct prompting.
LLMs like Tulu-13B, ChatGPT, and Palm-2 Bison have similar dimension scores, while Chimera-inst-chat-13B and Baize-v2-13B show diverse scores.
Ensemble of Chimera-inst-chat-13B and Baize-v2-13B dimension scores leads to significant improvements.
Model-Wise Ensemble:

Averaging scores of top 3 open-source models for each dimension.
Performance of this ensemble matches ChatGPT and surpasses Palm-2 Bison at the dialogue level.
Performance at the turn level is lower than ChatGPT and Palm-2 Bison for dimensions other than relevance and interestingness.
The results suggest the potential benefits of combining multiple models in ensembles, highlighting opportunities for future research to optimize ensembling methods and model selection.

Robustness of the LLM Evaluators
The researchers explore the robustness of Language Model Evaluators (LLMs) by focusing on negative adversarial perturbations that reduce the quality of original responses or dialogues. Here's a breakdown of the key points:

Metric Robustness Probing:

Prior works have used perturbation strategies to assess metric robustness.
The researchers specifically target negative perturbations that lower the quality of responses/dialogues.
Scoring and Robustness Calculation:

LLMs assign scores (q_dim) for high-quality dialogues/responses in specific dimensions.
Scores (p_dim) for responses/dialogues after negative perturbations indicate LLMs' robustness against those perturbations.
Robustness (R) is computed using the formula R = 1/N ∑ y, where N is the number of instances with perturbations and y depends on a threshold value (θ) that determines quality reduction extent.
Interpreting Robustness:

Different perturbation strategies vary in the degree of quality degradation they cause.
A higher R value indicates greater robustness of LLMs against the perturbations.
Limitations and Focus:

Robustness analysis is most relevant for strong automatic metrics.
The evaluation is limited to specific LLMs like ChatGPT, Palm-2 Bison, Tulu-13B, etc., highlighting the importance of applying this analysis to robust evaluation metrics.
Impact of Different Strategies
The impact of different strategies on the performance of various Large Language Models (LLMs) was evaluated using adversarial perturbations at both the turn and dialogue levels. Here are the key points from this section:

Random Response Perturbation:

Palm-2 Bison outperforms other LLMs by maintaining the original response over 0.3 higher than perturbed response in 62.3% of cases.
Baize-v2-13B and WizardLM-13B-V1.2 struggle to recognize this perturbation.
Challenging Perturbations:

"Replace pronoun" and "replace named entity" perturbations cause all LLMs to drop robustness ratios below 40%, indicating the need for a deeper semantic understanding.
Contradiction Perturbation:

Palm-2 Bison and other LLMs achieve robustness ratios over 50% on this perturbation, significantly outperforming ChatGPT.
Weak Performance:

LLMs generally perform poorly on "repetition", "unnatural paraphrase", and "dullness" perturbations.
Proprietary vs. Open-Source LLMs:

Proprietary models excel in handling dialogue-level coherence perturbations, while ChatGPT performs well in "unnatural paraphrase" but struggles with others.
LLMs' Performance:

LLaMA-2-Chat-13B stands out in handling dialogue-level engagingness and informativeness perturbations compared to other LLMs.
Robustness and Adaptability:

None of the LLMs are universally robust against all adversarial perturbations, highlighting the complexity of evaluating dialogues automatically.
Future Research:

Future work should focus on improving LLMs' robustness and adaptability against diverse perturbations, building on the insights gained from this study.
By analyzing the performance of various LLMs under different perturbation strategies, this study provides valuable insights into their strengths, weaknesses, and the challenges involved in automatic dialogue evaluation.

Conclusion
The researchers conducted a detailed analysis of 30 recent Large Language Models (LLMs), focusing on their evaluation capabilities in various dimensions at dialogue and turn levels. Here are the key insights from their study:

Enriched Meta-Evaluation Datasets:

Added new annotations to 12 existing meta-evaluation datasets for benchmarking new metrics.
Adversarial Testing and Model Ensemble:

Introduced adversarial strategies to test LLM robustness, a novel perspective in this research domain.
Examined the impact of dimension-wise and model-wise ensembles on dialogue evaluation.
Model Evaluation Abilities:

Instruction-tuned models show better alignment with human evaluations compared to vanilla foundation models.
Proprietary models, especially GPT-4, demonstrate superior evaluation abilities when compared to open-source LLMs.
Model size and instruction data play a crucial role in evaluation.
Performance Insights:

Only the ensemble of strong open-source models performs comparably to ChatGPT and PaLM-2 Bison.
LLMs excel more in evaluating coherence, relevance, and overall quality than specificity and diversity.
Robustness and Evaluation:

Ensemble of dimension-specific scores aligns better with overall human evaluations than direct overall quality assessment.
No LLMs are robust against all adversarial perturbations.
Robustness Results:

Google's PaLM-2 Bison is the most robust at the turn level, while Meta's LLaMA-2-Chat-13B excels at the dialogue level.
LLMs show promise in automatic dialogue evaluation, with GPT-4 not excelling in all dimensions.
The findings showcase the diverse strengths and limitations of LLMs in automatic dialogue evaluation, highlighting the significance of factors like model type, size, and robustness in their performance.