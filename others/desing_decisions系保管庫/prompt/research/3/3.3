3. A Comprehensive Analysis of the Effectiveness of Large Language Models as Automatic Dialogue Evaluators
Chen Zhang, Luis Fernando Dâ€™Haro, Yiming Chen  +2 more
23 Mar 2024-Proceedings of the ... AAAI Conference on Artificial Intelligence






Introduction
Evaluating dialogue systems poses challenges, with human evaluation being the gold standard despite its cost and reproducibility issues. Automatic measures, categorized into reference-based and reference-free metrics, supplement human assessment due to these limitations.

Reference-based metrics like BLEU show poor alignment with human judgment, leading to a focus on neural-based reference-free evaluators.
Though reference-free methods show better correlations with human evaluation, they struggle with generalization to new dialogue data.
Large language models (LLMs), especially instruction-tuned ones, show promise as dialogue evaluators due to their strong language understanding abilities.
While existing studies mainly feature proprietary LLMs like OpenAI ChatGPT/GPT-4, the rise of open-source foundation models necessitates an assessment of their effectiveness as dialogue evaluators.
The study conducts a thorough evaluation of 30 recent LLMs across various dimensions like coherence, engagingness, and diversity at both turn and dialogue levels.
By providing annotations for 12 meta-evaluation datasets, the study enhances the availability of resources for evaluating dialogue systems.
The work contributes by making datasets and annotations publicly accessible for benchmarking new evaluation metrics, introducing adversarial perturbations to test LLM robustness, and exploring different ensemble strategies' impact on dialogue evaluation performance.
Preliminaries Meta-Evaluation
The researchers utilize 12 meta-evaluation datasets in their study, divided into 6 datasets at the turn level and 6 at the dialogue level. These datasets are evaluated based on five quality aspects for each level.

Turn-Level Assessment:

Quality aspects considered: context relevance, understandability, specificity, interestingness, and overall response quality.
Dialogue-Level Assessment:

Quality aspects evaluated: coherence, engagingness, informativeness, diversity, and overall dialogue quality.
The team employs GPT-4 for annotations instead of crowdworkers to expedite the process and reduce costs, leveraging its human-level judgment capabilities while being more scalable and cost-effective.

GPT-4 annotations, although efficient, may introduce biases, prompting future research into ways to mitigate these biases through automatic and manual interventions.

Annotation process: GPT-4 is prompted with specific settings (temperature and top p) and the process is iterated five times to simulate multiple annotations per data instance, ensuring robustness.

An inter-annotator agreement metric is used, calculated by averaging pairwise Pearson correlations between annotation scores in different rounds, aiming for high agreement levels like those achieved by BERT, RoBERTa, and BLOOM models.

Special models like Alpaca, Vicuna, and Chimera are developed from foundational models like ChatGPT and GPT-4, tailored for specific tasks through techniques like instruction-based supervised finetuning (SFT) and reinforcement learning from human feedback (RLHF) to align these models with human capabilities in task-solving. For detailed model information, readers are directed to the respective technical reports.

Dialogue Evaluation with LLMs
The researchers in this study evaluate dialogues using Large Language Models (LLMs) like ChatGPT, Palm-2 Bison, and GPT-4. Here's an overview of how the evaluation process is conducted:

Scoring Procedure for Proprietary Models:

Output probabilities of proprietary models are not available, so an explicit scoring procedure is adopted.
LLMs are prompted to provide multi-dimensional ratings of dialogues or responses.
A consistent instruction template is used for ChatGPT, Palm-2 Bison, and GPT-4.
Scores are extracted using matching heuristics due to the models' strong instruction-following abilities.
In rare erroneous cases, manual corrections are made.
Each proprietary model's scoring process is repeated five times, and the average score is considered.
Scoring Procedure for Open-Source LLMs:

Open-source LLMs have weaker instruction-following abilities compared to proprietary models.
An implicit scoring procedure is applied for these models.
The focus is on the output probabilities related to the label words "Yes" and "No" generated by the LLM.
Normalization of the probability of "Yes" is done to determine the corresponding score.
Greedy decoding ensures deterministic generation by the LLMs.
Evaluation Process for Open-Source LLMs:

To evaluate response relevance for open-source LLMs, an example prompt is used.
The probability of "Yes" is normalized to get the corresponding score.
Instructions are adapted for different LLMs to align with their tuning processes.
Multi-Dimensional Correlation Analysis
The researchers present dimension-wise Pearson correlation scores for turn-level and dialogue-level datasets, highlighting the effectiveness of instruction-tuned models over vanilla models:

Instruction-tuned variants consistently outperform vanilla backbone models.
Alpaca-7B outperforms LLaMA-7B significantly at both turn and dialogue levels.
Alignment techniques like supervised finetuning enhance LLMs for better dialogue understanding.
Comparisons between LLaMA and other open-source families reveal performance variations and strengths at different levels:

LLaMA-2-7B and XGen-8K-7B lead in the 7B category at turn and dialogue levels respectively.
Large performance gaps exist among vanilla models due to differences in model sizes and training strategies.
Top performers in the instruction-tuned category include Alpaca-7B and Chimera-inst-chat-7B.
Impact of instruction data on model performance is evident:

Models with diverse instruction sources like Tulu-13B and Chimera-inst-chat-13B outperform others.
Diverse instruction datasets significantly influence model performance, emphasizing the importance of diverse training data.
Performance analysis across dimensions indicates strengths in some areas but slight weaknesses in others:

Models excel in relevance, coherence, engagement, and overall quality but slightly lag in interestingness, understandability (turn level), and diversity (dialogue level).
Future research can focus on designing objectives to enhance performance in specific dimensions for automatic dialogue evaluation leveraging LLMs.
Performance of GPT-4
The performance of GPT-4 was evaluated in comparison to the top-5 language models (LLMs) in turn-level and dialogue-level assessments. Here are the key findings and insights from the evaluation:

GPT-4 excelled in relevance, coherence, and overall quality evaluations at both turn and dialogue levels.
It falls short in specificity at the turn level and diversity at the dialogue level compared to other LLMs.
GPT-4 outperformed the second-best LLM (ChatGPT) significantly, with an absolute margin of 0.085 at the turn level and 0.042 at the dialogue level.
The study used human-annotated data specifically for correlation computation, leading to differences from previous analyses that included both human and GPT-4 annotations.
Even though GPT-4 shows strong performance, it still falls short of reaching perfect correlations (> 0.8) on average, highlighting that automatic dialogue evaluation remains a challenging area.
The results support the use of GPT-4 for completing missing annotations in datasets, but there is room for further research to enhance conversational understanding capabilities of language models.
Deviation Between GPT-4 and Human Preferences
The section discusses the comparison between GPT-4 and human annotations in evaluating Language Model (LLM) rankings. Here's a summary of the content:

Comparison of the rankings of 32 LLMs assessed by GPT-4 annotations with those assessed by human annotations.

Deviation quantified through Spearman correlation between the two ranking lists, where a higher correlation indicates closer agreement in model rankings.

Analysis conducted on the FED dataset (Mehri and Eskenazi 2020a), which provides human annotations for all dimensions.

The results show minimal deviation between GPT-4's evaluation and human evaluation, with a strong agreement (>0.85) in all dimensions except response specificity.

Ensemble Analysis
The section explores two ensemble strategies to enhance correlations with overall human evaluations:

Dimension-Wise Ensemble:

Involves averaging scores from each Language Model (LLM) assigned to different dimensions.
Evaluates if this average correlates better with human evaluations than having the LLM assess overall quality directly.
Averaging Scores from Multiple LLMs:

Combines scores from several LLMs for a specific dimension.
Aims to assess whether this approach can improve correlation with human evaluations.
Dimension-Wise Ensemble
The researchers focused on Tulu-13B, Chimera-inst-chat-13B, Baize-v2-13B, WizardLM-13B-V1.2, Palm-2 Bison, and ChatGPT due to their strong multi-dimensional evaluation performance. Here are the key points from their analysis:

Comparisons between dimension-wise ensemble and direct prompting approaches showed that the ensemble method displayed strong correlations with overall human judgments.
Chimera-inst-chat-13B and Baize-v2-13B saw gains of over 10% in dialogue quality with the ensemble approach compared to direct prompting.
Tulu-13B, ChatGPT, and Palm-2 Bison showed highly similar scores across different dimensions, while Chimera-inst-chat-13B and Baize-v2-13B exhibited more diverse scores.
The ensemble of Chimera-inst-chat-13B and Baize-v2-13B specific dimension scores led to more significant improvements compared to other Language Model Models (LLMs).
For model-wise ensemble, averaging the top 3 open-source models' scores for each dimension showed comparable performance to ChatGPT and outperformed Palm-2 Bison at the dialogue level.
Performance of the ensemble at the turn level was lower than ChatGPT and Palm-2 Bison for dimensions other than relevance and interestingness, highlighting the potential benefits of combining models for enhanced evaluation.
Future research avenues could explore optimal ensembling strategies including how to combine models effectively, which models to include, and how to weigh individual model outputs.
Robustness of the LLM Evaluators
The section discusses the robustness of Language Model Evaluators (LLMs) against negative adversarial perturbations that decrease the quality of responses or dialogues. Here are the key points:

LLMs assign scores (qdim) to high-quality responses/dialogues for a specific dimension.
When negative perturbations are applied targeting that dimension, LLMs give scores (pdim).
The robustness of LLMs against such perturbations is determined by R = 1/N âˆ‘ y, where N is the number of data instances with the perturbation, and y is calculated using a positive threshold value Î¸ indicating the quality reduction.
Different perturbation strategies result in varying degrees of quality degradation.
A higher R value indicates greater robustness of LLMs against perturbations.
The robustness analysis is conducted using strong automatic metrics like ChatGPT, Palm-2 Bison, Tulu-13B, Chimerainst-chat-13B, Baize-v2-13B, WizardLM-13B-V1.2, and LLaMA-2-Chat-13B.
Impact of Different Strategies
The impact of different strategies on various Language Model Models (LLMs) is analyzed, revealing interesting findings:

For the "random response" perturbation, Palm-2 Bison, a top LLM, outperforms others by scoring the original response significantly higher compared to perturbed responses in 62.3% of cases.
Baize-v2-13B and WizardLM-13B-V1.2 often struggle with recognizing the "random response" perturbation.
The strategies of "replace pronoun" and "replace named entity" prove more challenging, causing the robustness ratio of all LLMs to drop below 40%.
LLMs like Palm-2 Bison excel in handling the "contradiction" perturbation, achieving robustness ratios of over 50%, far surpassing ChatGPT.
Performance across LLMs is generally weaker in handling "repetition", "unnatural paraphrase", and "dullness" perturbations.
Proprietary LLMs show better performance in maintaining dialogue-level coherence than open-source models.
Key findings include:

ChatGPT excels in handling "unnatural paraphrase" perturbations.
LLaMA-2-Chat-13B performs well in managing perturbations targeting dialogue-level engagingness and informativeness.
None of the LLMs show robustness against all adversarial perturbations, indicating the complexity of evaluating dialogues effectively.
The study emphasizes the need to enhance LLMs' adaptability and robustness across diverse perturbations and suggests further research directions to address these challenges comprehensively.

Conclusion
In the paper's conclusion, the researchers present key insights and findings from their analysis of 30 recent Large Language Models (LLMs) regarding their evaluation abilities at dialogue and turn levels. They also introduce new annotations to enhance existing meta-evaluation datasets for benchmarking new metrics. Here are the summarized key points:

Model Evaluation Insights:

Instruction-tuned models exhibit better alignment with human evaluations than basic foundation models.
Proprietary models, particularly GPT-4, demonstrate superior evaluation capabilities compared to open-source LLMs.
Evaluation is influenced significantly by model size and instruction data availability.
Model Performance and Robustness:

Only ensembles of strong open-source models perform comparably to ChatGPT and PaLM-2 Bison.
LLMs excel more in evaluating coherence, relevance, and overall quality rather than specificity and diversity.
None of the LLMs exhibit robustness against all adversarial perturbations.
Robustness and Specialized Performance:

Google's Palm-2 Bison shows the best robustness at turn levels, while Meta's LLaMA-2-Chat-13B excels at the dialogue level.
Ensembling dimension-specific scores better correlates with overall human evaluations than direct quality assessments.
LLMs demonstrate potential in automatic dialogue evaluation, though achieving high performance across all dimensions remains a challenge, even for advanced models like GPT-4.